{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0640f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install PyQt6 opencv-python mediapipe numpy playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65bb9630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in d:\\venv3114\\lib\\site-packages (4.12.0.88)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Using cached numpy-2.2.6-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.19.1 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install  opencv-python   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e19855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in d:\\venv3114\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: absl-py in d:\\venv3114\\lib\\site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\venv3114\\lib\\site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in d:\\venv3114\\lib\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in d:\\venv3114\\lib\\site-packages (from mediapipe) (0.7.0)\n",
      "Requirement already satisfied: jaxlib in d:\\venv3114\\lib\\site-packages (from mediapipe) (0.7.0)\n",
      "Requirement already satisfied: matplotlib in d:\\venv3114\\lib\\site-packages (from mediapipe) (3.10.5)\n",
      "Collecting numpy<2 (from mediapipe)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Requirement already satisfied: opencv-contrib-python in d:\\venv3114\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in d:\\venv3114\\lib\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in d:\\venv3114\\lib\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in d:\\venv3114\\lib\\site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in d:\\venv3114\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in d:\\venv3114\\lib\\site-packages (from jax->mediapipe) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in d:\\venv3114\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in d:\\venv3114\\lib\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\venv3114\\lib\\site-packages (from matplotlib->mediapipe) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\venv3114\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\venv3114\\lib\\site-packages (from matplotlib->mediapipe) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\venv3114\\lib\\site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\venv3114\\lib\\site-packages (from matplotlib->mediapipe) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\venv3114\\lib\\site-packages (from matplotlib->mediapipe) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\venv3114\\lib\\site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\venv3114\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in d:\\venv3114\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in d:\\venv3114\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8de0e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\venv3114\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cba592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting playsound\n",
      "  Using cached playsound-1.3.0.tar.gz (7.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: playsound\n",
      "  Building wheel for playsound (setup.py): started\n",
      "  Building wheel for playsound (setup.py): finished with status 'done'\n",
      "  Created wheel for playsound: filename=playsound-1.3.0-py3-none-any.whl size=7123 sha256=83b883aad75449c7c205c8a44082fb07df6916fe2440bcf9621009e4d2992665\n",
      "  Stored in directory: c:\\users\\krunal\\appdata\\local\\pip\\cache\\wheels\\50\\98\\42\\62753a9e1fb97579a0ce2f84f7db4c21c09d03bb2091e6cef4\n",
      "Successfully built playsound\n",
      "Installing collected packages: playsound\n",
      "Successfully installed playsound-1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install  playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4a04b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyQt6 in d:\\venv3114\\lib\\site-packages (6.10.0)\n",
      "Requirement already satisfied: PyQt6-sip<14,>=13.8 in d:\\venv3114\\lib\\site-packages (from PyQt6) (13.10.2)\n",
      "Requirement already satisfied: PyQt6-Qt6<6.11.0,>=6.10.0 in d:\\venv3114\\lib\\site-packages (from PyQt6) (6.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install PyQt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2fde59",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'inattentive_duration' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 534\u001b[39m, in \u001b[36mVideoProcessorThread.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    531\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Process frame with detector\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m# Update alert system\u001b[39;00m\n\u001b[32m    537\u001b[39m alert_info = \u001b[38;5;28mself\u001b[39m.alert_manager.update(\n\u001b[32m    538\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    539\u001b[39m     results.get(\u001b[33m'\u001b[39m\u001b[33mdrowsiness_duration\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \n\u001b[32m    540\u001b[39m     results.get(\u001b[33m'\u001b[39m\u001b[33minattentiveness_duration\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    541\u001b[39m     results.get(\u001b[33m'\u001b[39m\u001b[33mabsence_duration\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m)\n\u001b[32m    542\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 375\u001b[39m, in \u001b[36mGuardDetector.process_frame\u001b[39m\u001b[34m(self, frame)\u001b[39m\n\u001b[32m    373\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mDROWSY\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    374\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mdrowsiness_duration\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mmax\u001b[39m(head_down_duration, eyes_closed_duration)\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43minattentive_duration\u001b[49m > Config.INATTENTIVENESS_THRESHOLD:\n\u001b[32m    376\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mINATTENTIVE\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'inattentive_duration' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'inattentive_duration' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 534\u001b[39m, in \u001b[36mVideoProcessorThread.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    531\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Process frame with detector\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m# Update alert system\u001b[39;00m\n\u001b[32m    537\u001b[39m alert_info = \u001b[38;5;28mself\u001b[39m.alert_manager.update(\n\u001b[32m    538\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    539\u001b[39m     results.get(\u001b[33m'\u001b[39m\u001b[33mdrowsiness_duration\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \n\u001b[32m    540\u001b[39m     results.get(\u001b[33m'\u001b[39m\u001b[33minattentiveness_duration\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    541\u001b[39m     results.get(\u001b[33m'\u001b[39m\u001b[33mabsence_duration\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m)\n\u001b[32m    542\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 375\u001b[39m, in \u001b[36mGuardDetector.process_frame\u001b[39m\u001b[34m(self, frame)\u001b[39m\n\u001b[32m    373\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mDROWSY\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    374\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mdrowsiness_duration\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mmax\u001b[39m(head_down_duration, eyes_closed_duration)\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43minattentive_duration\u001b[49m > Config.INATTENTIVENESS_THRESHOLD:\n\u001b[32m    376\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mINATTENTIVE\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'inattentive_duration' where it is not associated with a value"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'inattentive_duration' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 534\u001b[39m, in \u001b[36mVideoProcessorThread.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    531\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;66;03m# Process frame with detector\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m# Update alert system\u001b[39;00m\n\u001b[32m    537\u001b[39m alert_info = \u001b[38;5;28mself\u001b[39m.alert_manager.update(\n\u001b[32m    538\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    539\u001b[39m     results.get(\u001b[33m'\u001b[39m\u001b[33mdrowsiness_duration\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \n\u001b[32m    540\u001b[39m     results.get(\u001b[33m'\u001b[39m\u001b[33minattentiveness_duration\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    541\u001b[39m     results.get(\u001b[33m'\u001b[39m\u001b[33mabsence_duration\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m)\n\u001b[32m    542\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 375\u001b[39m, in \u001b[36mGuardDetector.process_frame\u001b[39m\u001b[34m(self, frame)\u001b[39m\n\u001b[32m    373\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mDROWSY\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    374\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mdrowsiness_duration\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mmax\u001b[39m(head_down_duration, eyes_closed_duration)\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43minattentive_duration\u001b[49m > Config.INATTENTIVENESS_THRESHOLD:\n\u001b[32m    376\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mINATTENTIVE\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'inattentive_duration' where it is not associated with a value"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venv3114\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Smart ATM Guardian - Advanced Security Guard Monitoring System\n",
    "================================================================\n",
    "A real-time computer vision application that monitors security personnel\n",
    "for drowsiness, inattentiveness, and absence using MediaPipe and OpenCV.\n",
    "\n",
    "Requirements:\n",
    "pip install PyQt6 opencv-python mediapipe numpy playsound\n",
    "\n",
    "Usage:\n",
    "python smart_atm_guardian.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import time\n",
    "from PyQt6.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, \n",
    "                              QHBoxLayout, QPushButton, QLabel, QTextEdit, QFrame)\n",
    "from PyQt6.QtCore import QThread, pyqtSignal, Qt, QTimer\n",
    "from PyQt6.QtGui import QImage, QPixmap, QFont\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Try to import playsound, but make it optional for testing\n",
    "try:\n",
    "    from playsound import playsound\n",
    "    SOUND_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: playsound not available. Audio alerts disabled.\")\n",
    "    SOUND_AVAILABLE = False\n",
    "\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for the Smart ATM Guardian system\"\"\"\n",
    "    \n",
    "    # Video source (0 for webcam, or RTSP URL like \"rtsp://camera_ip:554/stream\")\n",
    "    VIDEO_SOURCE = 0\n",
    "    \n",
    "    # Detection thresholds (in seconds)\n",
    "    DROWSINESS_HEAD_DOWN_THRESHOLD = 10.0\n",
    "    DROWSINESS_EYES_CLOSED_THRESHOLD = 3.0\n",
    "    INATTENTIVENESS_THRESHOLD = 60.0\n",
    "    ABSENCE_THRESHOLD = 120.0\n",
    "    \n",
    "    # Alert escalation timing (in seconds)\n",
    "    TIER1_TO_TIER2_DELAY = 15.0\n",
    "    TIER2_TO_TIER3_DELAY = 30.0\n",
    "    \n",
    "    # Eye Aspect Ratio threshold for detecting closed eyes\n",
    "    EAR_THRESHOLD = 0.2\n",
    "    \n",
    "    # Head pose angle thresholds (in degrees)\n",
    "    HEAD_DOWN_ANGLE = 25.0  # Pitch angle indicating head down\n",
    "    HEAD_TURNED_ANGLE = 45.0  # Yaw angle indicating head turned away\n",
    "    \n",
    "    # ROI for guard presence detection (as fraction of frame: x, y, w, h)\n",
    "    GUARD_ROI = (0.3, 0.2, 0.4, 0.6)  # Center region of frame\n",
    "    \n",
    "    # Sound files (placeholders - user should provide actual files)\n",
    "    SOUND_WARNING = \"warning.wav\"  # Gentle beep or cough\n",
    "    SOUND_ALARM = \"wake_up_alarm.wav\"  # Loud wake-up sound\n",
    "    \n",
    "\n",
    "# ==================== DETECTION ENGINE ====================\n",
    "class GuardDetector:\n",
    "    \"\"\"\n",
    "    Core detection engine using MediaPipe for pose and face analysis.\n",
    "    Implements multi-modal detection: drowsiness, inattentiveness, absence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize MediaPipe solutions\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        \n",
    "        # Create pose and face mesh detectors\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        # State tracking\n",
    "        self.head_down_start_time = None\n",
    "        self.eyes_closed_start_time = None\n",
    "        self.head_turned_start_time = None\n",
    "        self.absence_start_time = None\n",
    "        \n",
    "        # Eye landmarks for EAR calculation (left and right eye)\n",
    "        self.LEFT_EYE_INDICES = [33, 160, 158, 133, 153, 144]\n",
    "        self.RIGHT_EYE_INDICES = [362, 385, 387, 263, 373, 380]\n",
    "        \n",
    "    def calculate_ear(self, eye_landmarks) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Eye Aspect Ratio (EAR) to detect eye closure.\n",
    "        \n",
    "        EAR = (||p2-p6|| + ||p3-p5||) / (2 * ||p1-p4||)\n",
    "        where p1-p6 are the eye landmark points.\n",
    "        \n",
    "        Lower EAR values indicate closed eyes.\n",
    "        \"\"\"\n",
    "        # Vertical distances\n",
    "        v1 = np.linalg.norm(eye_landmarks[1] - eye_landmarks[5])\n",
    "        v2 = np.linalg.norm(eye_landmarks[2] - eye_landmarks[4])\n",
    "        \n",
    "        # Horizontal distance\n",
    "        h = np.linalg.norm(eye_landmarks[0] - eye_landmarks[3])\n",
    "        \n",
    "        # EAR calculation\n",
    "        ear = (v1 + v2) / (2.0 * h + 1e-6)  # Add epsilon to avoid division by zero\n",
    "        return ear\n",
    "    \n",
    "    def get_head_pose_angles(self, face_landmarks, image_shape) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Calculate head pose angles (pitch, yaw, roll) from face landmarks.\n",
    "        \n",
    "        Pitch: Head up/down (positive = looking up, negative = looking down)\n",
    "        Yaw: Head left/right (positive = turned right, negative = turned left)\n",
    "        Roll: Head tilt\n",
    "        \"\"\"\n",
    "        h, w = image_shape[:2]\n",
    "        \n",
    "        # Key facial landmarks for pose estimation\n",
    "        # Nose tip, chin, left eye corner, right eye corner, left mouth corner, right mouth corner\n",
    "        key_indices = [1, 152, 33, 263, 61, 291]\n",
    "        \n",
    "        if not face_landmarks:\n",
    "            return 0.0, 0.0, 0.0\n",
    "        \n",
    "        # Extract 2D landmarks\n",
    "        landmarks_2d = []\n",
    "        for idx in key_indices:\n",
    "            lm = face_landmarks.landmark[idx]\n",
    "            landmarks_2d.append([lm.x * w, lm.y * h])\n",
    "        landmarks_2d = np.array(landmarks_2d, dtype=np.float64)\n",
    "        \n",
    "        # 3D model points (approximate facial structure)\n",
    "        model_points = np.array([\n",
    "            (0.0, 0.0, 0.0),           # Nose tip\n",
    "            (0.0, -330.0, -65.0),      # Chin\n",
    "            (-225.0, 170.0, -135.0),   # Left eye corner\n",
    "            (225.0, 170.0, -135.0),    # Right eye corner\n",
    "            (-150.0, -150.0, -125.0),  # Left mouth corner\n",
    "            (150.0, -150.0, -125.0)    # Right mouth corner\n",
    "        ])\n",
    "        \n",
    "        # Camera internals (approximate)\n",
    "        focal_length = w\n",
    "        center = (w / 2, h / 2)\n",
    "        camera_matrix = np.array([\n",
    "            [focal_length, 0, center[0]],\n",
    "            [0, focal_length, center[1]],\n",
    "            [0, 0, 1]\n",
    "        ], dtype=np.float64)\n",
    "        \n",
    "        dist_coeffs = np.zeros((4, 1))  # Assuming no lens distortion\n",
    "        \n",
    "        # Solve PnP to get rotation vector\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            model_points, landmarks_2d, camera_matrix, dist_coeffs\n",
    "        )\n",
    "        \n",
    "        if not success:\n",
    "            return 0.0, 0.0, 0.0\n",
    "        \n",
    "        # Convert rotation vector to rotation matrix\n",
    "        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "        \n",
    "        # Calculate Euler angles\n",
    "        sy = np.sqrt(rotation_matrix[0, 0] ** 2 + rotation_matrix[1, 0] ** 2)\n",
    "        singular = sy < 1e-6\n",
    "        \n",
    "        if not singular:\n",
    "            pitch = np.arctan2(rotation_matrix[2, 1], rotation_matrix[2, 2])\n",
    "            yaw = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "            roll = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
    "        else:\n",
    "            pitch = np.arctan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])\n",
    "            yaw = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "            roll = 0\n",
    "        \n",
    "        # Convert to degrees\n",
    "        pitch = np.degrees(pitch)\n",
    "        yaw = np.degrees(yaw)\n",
    "        roll = np.degrees(roll)\n",
    "        \n",
    "        return pitch, yaw, roll\n",
    "    \n",
    "    def detect_person_in_roi(self, pose_landmarks, image_shape) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a person (guard) is present in the designated ROI.\n",
    "        \"\"\"\n",
    "        if not pose_landmarks:\n",
    "            return False\n",
    "        \n",
    "        h, w = image_shape[:2]\n",
    "        roi_x, roi_y, roi_w, roi_h = Config.GUARD_ROI\n",
    "        \n",
    "        # Check if key body landmarks (shoulders, nose) are in ROI\n",
    "        key_landmarks = [\n",
    "            pose_landmarks.landmark[self.mp_pose.PoseLandmark.NOSE],\n",
    "            pose_landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_SHOULDER],\n",
    "            pose_landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        ]\n",
    "        \n",
    "        in_roi_count = 0\n",
    "        for lm in key_landmarks:\n",
    "            x, y = lm.x, lm.y\n",
    "            if (roi_x <= x <= roi_x + roi_w) and (roi_y <= y <= roi_y + roi_h):\n",
    "                in_roi_count += 1\n",
    "        \n",
    "        # At least 2 out of 3 key landmarks should be in ROI\n",
    "        return in_roi_count >= 2\n",
    "    \n",
    "    def process_frame(self, frame: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Process a single frame and detect guard states.\n",
    "        \n",
    "        Returns:\n",
    "            dict with detection results including state, timings, and annotated frame\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        h, w = frame.shape[:2]\n",
    "        \n",
    "        # Convert BGR to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process with pose and face mesh\n",
    "        pose_results = self.pose.process(rgb_frame)\n",
    "        face_results = self.face_mesh.process(rgb_frame)\n",
    "        \n",
    "        # Initialize detection results\n",
    "        results = {\n",
    "            'person_detected': False,\n",
    "            'eyes_closed': False,\n",
    "            'head_down': False,\n",
    "            'head_turned': False,\n",
    "            'drowsiness_duration': 0.0,\n",
    "            'inattentiveness_duration': 0.0,\n",
    "            'absence_duration': 0.0,\n",
    "            'state': 'UNKNOWN',\n",
    "            'pitch': 0.0,\n",
    "            'yaw': 0.0,\n",
    "            'ear': 0.0\n",
    "        }\n",
    "        \n",
    "        # Draw ROI on frame\n",
    "        roi_x, roi_y, roi_w, roi_h = Config.GUARD_ROI\n",
    "        cv2.rectangle(frame, \n",
    "                     (int(roi_x * w), int(roi_y * h)),\n",
    "                     (int((roi_x + roi_w) * w), int((roi_y + roi_h) * h)),\n",
    "                     (0, 255, 0), 2)\n",
    "        cv2.putText(frame, \"Guard Post\", (int(roi_x * w), int(roi_y * h) - 10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        \n",
    "        # Check for person presence\n",
    "        if pose_results.pose_landmarks:\n",
    "            results['person_detected'] = self.detect_person_in_roi(\n",
    "                pose_results.pose_landmarks, frame.shape\n",
    "            )\n",
    "            \n",
    "            # Draw pose landmarks\n",
    "            self.mp_drawing.draw_landmarks(\n",
    "                frame, pose_results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS\n",
    "            )\n",
    "        \n",
    "        # Absence detection\n",
    "        if not results['person_detected']:\n",
    "            if self.absence_start_time is None:\n",
    "                self.absence_start_time = current_time\n",
    "            absence_duration = current_time - self.absence_start_time\n",
    "            results['absence_duration'] = absence_duration\n",
    "            \n",
    "            if absence_duration > Config.ABSENCE_THRESHOLD:\n",
    "                results['state'] = 'ABSENT'\n",
    "            else:\n",
    "                results['state'] = 'PERSON_NOT_IN_POST'\n",
    "            \n",
    "            # Reset other timers\n",
    "            self.head_down_start_time = None\n",
    "            self.eyes_closed_start_time = None\n",
    "            self.head_turned_start_time = None\n",
    "            \n",
    "        else:\n",
    "            # Person is present, reset absence timer\n",
    "            self.absence_start_time = None\n",
    "            \n",
    "            # Analyze face for drowsiness and attention\n",
    "            if face_results.multi_face_landmarks:\n",
    "                face_landmarks = face_results.multi_face_landmarks[0]\n",
    "                \n",
    "                # Draw face mesh\n",
    "                self.mp_drawing.draw_landmarks(\n",
    "                    frame, face_landmarks, self.mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=self.mp_drawing.DrawingSpec(\n",
    "                        color=(0, 255, 0), thickness=1\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Calculate EAR for both eyes\n",
    "                left_eye_points = np.array([\n",
    "                    [face_landmarks.landmark[i].x * w, face_landmarks.landmark[i].y * h]\n",
    "                    for i in self.LEFT_EYE_INDICES\n",
    "                ])\n",
    "                right_eye_points = np.array([\n",
    "                    [face_landmarks.landmark[i].x * w, face_landmarks.landmark[i].y * h]\n",
    "                    for i in self.RIGHT_EYE_INDICES\n",
    "                ])\n",
    "                \n",
    "                left_ear = self.calculate_ear(left_eye_points)\n",
    "                right_ear = self.calculate_ear(right_eye_points)\n",
    "                avg_ear = (left_ear + right_ear) / 2.0\n",
    "                results['ear'] = avg_ear\n",
    "                \n",
    "                # Check for closed eyes\n",
    "                if avg_ear < Config.EAR_THRESHOLD:\n",
    "                    results['eyes_closed'] = True\n",
    "                    if self.eyes_closed_start_time is None:\n",
    "                        self.eyes_closed_start_time = current_time\n",
    "                else:\n",
    "                    self.eyes_closed_start_time = None\n",
    "                \n",
    "                # Get head pose\n",
    "                pitch, yaw, roll = self.get_head_pose_angles(face_landmarks, frame.shape)\n",
    "                results['pitch'] = pitch\n",
    "                results['yaw'] = yaw\n",
    "                \n",
    "                # Check for head down (drowsiness indicator)\n",
    "                if pitch < -Config.HEAD_DOWN_ANGLE:  # Negative pitch = looking down\n",
    "                    results['head_down'] = True\n",
    "                    if self.head_down_start_time is None:\n",
    "                        self.head_down_start_time = current_time\n",
    "                else:\n",
    "                    self.head_down_start_time = None\n",
    "                \n",
    "                # Check for head turned away (inattentiveness)\n",
    "                if abs(yaw) > Config.HEAD_TURNED_ANGLE:\n",
    "                    results['head_turned'] = True\n",
    "                    if self.head_turned_start_time is None:\n",
    "                        self.head_turned_start_time = current_time\n",
    "                else:\n",
    "                    self.head_turned_start_time = None\n",
    "                \n",
    "                # Calculate durations\n",
    "                if self.eyes_closed_start_time:\n",
    "                    eyes_closed_duration = current_time - self.eyes_closed_start_time\n",
    "                else:\n",
    "                    eyes_closed_duration = 0.0\n",
    "                \n",
    "                if self.head_down_start_time:\n",
    "                    head_down_duration = current_time - self.head_down_start_time\n",
    "                else:\n",
    "                    head_down_duration = 0.0\n",
    "                \n",
    "                if self.head_turned_start_time:\n",
    "                    inattentive_duration = current_time - self.head_turned_start_time\n",
    "                    results['inattentiveness_duration'] = inattentive_duration\n",
    "                \n",
    "                # Determine state based on combined indicators\n",
    "                # Drowsiness = (head down > 10s) OR (eyes closed > 3s)\n",
    "                if (head_down_duration > Config.DROWSINESS_HEAD_DOWN_THRESHOLD or\n",
    "                    eyes_closed_duration > Config.DROWSINESS_EYES_CLOSED_THRESHOLD):\n",
    "                    results['state'] = 'DROWSY'\n",
    "                    results['drowsiness_duration'] = max(head_down_duration, eyes_closed_duration)\n",
    "                elif inattentive_duration > Config.INATTENTIVENESS_THRESHOLD:\n",
    "                    results['state'] = 'INATTENTIVE'\n",
    "                else:\n",
    "                    results['state'] = 'ATTENTIVE'\n",
    "                \n",
    "                # Display metrics on frame\n",
    "                cv2.putText(frame, f\"EAR: {avg_ear:.2f}\", (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                cv2.putText(frame, f\"Pitch: {pitch:.1f} Yaw: {yaw:.1f}\", (10, 60),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            else:\n",
    "                # Face not detected but person present\n",
    "                results['state'] = 'FACE_NOT_DETECTED'\n",
    "        \n",
    "        results['annotated_frame'] = frame\n",
    "        return results\n",
    "\n",
    "\n",
    "# ==================== ALERT MANAGER ====================\n",
    "class AlertManager:\n",
    "    \"\"\"\n",
    "    Manages the tiered alert system with escalation logic.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_alert_tier = 0\n",
    "        self.tier1_start_time = None\n",
    "        self.tier2_start_time = None\n",
    "        self.last_sound_time = 0\n",
    "        self.sound_cooldown = 2.0  # Minimum seconds between sounds\n",
    "        \n",
    "    def play_sound(self, sound_file: str):\n",
    "        \"\"\"Play sound file with cooldown to avoid spam\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_sound_time >= self.sound_cooldown:\n",
    "            if SOUND_AVAILABLE:\n",
    "                try:\n",
    "                    playsound(sound_file, block=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error playing sound: {e}\")\n",
    "            else:\n",
    "                print(f\"[SOUND] Would play: {sound_file}\")\n",
    "            self.last_sound_time = current_time\n",
    "    \n",
    "    def update(self, state: str, duration: float) -> dict:\n",
    "        \"\"\"\n",
    "        Update alert state based on detected condition.\n",
    "        \n",
    "        Returns dict with alert_tier, message, and color\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Reset if state is normal\n",
    "        if state in ['ATTENTIVE', 'PERSON_NOT_IN_POST']:\n",
    "            self.current_alert_tier = 0\n",
    "            self.tier1_start_time = None\n",
    "            self.tier2_start_time = None\n",
    "            return {\n",
    "                'tier': 0,\n",
    "                'message': f'Status: {state}',\n",
    "                'color': 'green'\n",
    "            }\n",
    "        \n",
    "        # Handle problematic states\n",
    "        if state in ['DROWSY', 'INATTENTIVE']:\n",
    "            # Tier 1: Initial warning\n",
    "            if self.current_alert_tier == 0:\n",
    "                self.current_alert_tier = 1\n",
    "                self.tier1_start_time = current_time\n",
    "                self.play_sound(Config.SOUND_WARNING)\n",
    "                return {\n",
    "                    'tier': 1,\n",
    "                    'message': f'WARNING: {state} detected!',\n",
    "                    'color': 'yellow'\n",
    "                }\n",
    "            \n",
    "            # Tier 2: Escalation after delay\n",
    "            elif self.current_alert_tier == 1:\n",
    "                tier1_duration = current_time - self.tier1_start_time\n",
    "                if tier1_duration > Config.TIER1_TO_TIER2_DELAY:\n",
    "                    self.current_alert_tier = 2\n",
    "                    self.tier2_start_time = current_time\n",
    "                    self.play_sound(Config.SOUND_ALARM)\n",
    "                    return {\n",
    "                        'tier': 2,\n",
    "                        'message': f'ALARM: {state} persisting! WAKE UP!',\n",
    "                        'color': 'red'\n",
    "                    }\n",
    "                return {\n",
    "                    'tier': 1,\n",
    "                    'message': f'WARNING: {state} ({tier1_duration:.0f}s)',\n",
    "                    'color': 'yellow'\n",
    "                }\n",
    "            \n",
    "            # Tier 3: Critical escalation\n",
    "            elif self.current_alert_tier == 2:\n",
    "                tier2_duration = current_time - self.tier2_start_time\n",
    "                if tier2_duration > Config.TIER2_TO_TIER3_DELAY:\n",
    "                    self.current_alert_tier = 3\n",
    "                    return {\n",
    "                        'tier': 3,\n",
    "                        'message': f'CRITICAL: {state} - Supervisor notified!',\n",
    "                        'color': 'red'\n",
    "                    }\n",
    "                return {\n",
    "                    'tier': 2,\n",
    "                    'message': f'ALARM: {state} ({tier2_duration:.0f}s)',\n",
    "                    'color': 'red'\n",
    "                }\n",
    "        \n",
    "        # Handle absence\n",
    "        if state == 'ABSENT':\n",
    "            self.current_alert_tier = 3\n",
    "            return {\n",
    "                'tier': 3,\n",
    "                'message': 'CRITICAL: Guard absent from post!',\n",
    "                'color': 'red'\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'tier': 0,\n",
    "            'message': f'Status: {state}',\n",
    "            'color': 'gray'\n",
    "        }\n",
    "\n",
    "\n",
    "# ==================== VIDEO PROCESSING THREAD ====================\n",
    "class VideoProcessorThread(QThread):\n",
    "    \"\"\"\n",
    "    Worker thread for video capture and processing.\n",
    "    Runs detection engine and emits results to GUI.\n",
    "    \"\"\"\n",
    "    \n",
    "    frame_processed = pyqtSignal(object)  # Emits detection results dict\n",
    "    error_occurred = pyqtSignal(str)\n",
    "    \n",
    "    def __init__(self, video_source):\n",
    "        super().__init__()\n",
    "        self.video_source = video_source\n",
    "        self.running = False\n",
    "        self.detector = GuardDetector()\n",
    "        self.alert_manager = AlertManager()\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Main processing loop\"\"\"\n",
    "        self.running = True\n",
    "        cap = cv2.VideoCapture(self.video_source)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            self.error_occurred.emit(f\"Failed to open video source: {self.video_source}\")\n",
    "            return\n",
    "        \n",
    "        while self.running:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                self.error_occurred.emit(\"Failed to read frame from video source\")\n",
    "                break\n",
    "            \n",
    "            # Process frame with detector\n",
    "            results = self.detector.process_frame(frame)\n",
    "            \n",
    "            # Update alert system\n",
    "            alert_info = self.alert_manager.update(\n",
    "                results['state'],\n",
    "                results.get('drowsiness_duration', 0.0) or \n",
    "                results.get('inattentiveness_duration', 0.0) or\n",
    "                results.get('absence_duration', 0.0)\n",
    "            )\n",
    "            \n",
    "            # Combine results\n",
    "            results['alert'] = alert_info\n",
    "            \n",
    "            # Emit to GUI\n",
    "            self.frame_processed.emit(results)\n",
    "        \n",
    "        cap.release()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the processing thread\"\"\"\n",
    "        self.running = False\n",
    "\n",
    "\n",
    "# ==================== MAIN GUI APPLICATION ====================\n",
    "class SmartATMGuardian(QMainWindow):\n",
    "    \"\"\"\n",
    "    Main application window for the Smart ATM Guardian system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.video_thread = None\n",
    "        self.init_ui()\n",
    "        \n",
    "    def init_ui(self):\n",
    "        \"\"\"Initialize the user interface\"\"\"\n",
    "        self.setWindowTitle(\"Smart ATM Guardian - Security Monitoring System\")\n",
    "        self.setGeometry(100, 100, 1200, 800)\n",
    "        \n",
    "        # Central widget\n",
    "        central_widget = QWidget()\n",
    "        self.setCentralWidget(central_widget)\n",
    "        main_layout = QVBoxLayout(central_widget)\n",
    "        \n",
    "        # Title\n",
    "        title_label = QLabel(\"üõ°Ô∏è Smart ATM Guardian\")\n",
    "        title_font = QFont(\"Arial\", 20, QFont.Weight.Bold)\n",
    "        title_label.setFont(title_font)\n",
    "        title_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        main_layout.addWidget(title_label)\n",
    "        \n",
    "        # Video and info layout\n",
    "        content_layout = QHBoxLayout()\n",
    "        \n",
    "        # Left: Video display\n",
    "        video_layout = QVBoxLayout()\n",
    "        self.video_label = QLabel()\n",
    "        self.video_label.setMinimumSize(640, 480)\n",
    "        self.video_label.setStyleSheet(\"border: 2px solid #333; background-color: black;\")\n",
    "        self.video_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        video_layout.addWidget(self.video_label)\n",
    "        \n",
    "        # Control buttons\n",
    "        button_layout = QHBoxLayout()\n",
    "        self.start_button = QPushButton(\"‚ñ∂ Start Monitoring\")\n",
    "        self.start_button.setStyleSheet(\"\"\"\n",
    "            QPushButton {\n",
    "                background-color: #4CAF50;\n",
    "                color: white;\n",
    "                font-size: 16px;\n",
    "                padding: 10px;\n",
    "                border-radius: 5px;\n",
    "            }\n",
    "            QPushButton:hover {\n",
    "                background-color: #45a049;\n",
    "            }\n",
    "        \"\"\")\n",
    "        self.start_button.clicked.connect(self.start_monitoring)\n",
    "        \n",
    "        self.stop_button = QPushButton(\"‚èπ Stop Monitoring\")\n",
    "        self.stop_button.setStyleSheet(\"\"\"\n",
    "            QPushButton {\n",
    "                background-color: #f44336;\n",
    "                color: white;\n",
    "                font-size: 16px;\n",
    "                padding: 10px;\n",
    "                border-radius: 5px;\n",
    "            }\n",
    "            QPushButton:hover {\n",
    "                background-color: #da190b;\n",
    "            }\n",
    "            QPushButton:disabled {\n",
    "                background-color: #cccccc;\n",
    "            }\n",
    "        \"\"\")\n",
    "        self.stop_button.clicked.connect(self.stop_monitoring)\n",
    "        self.stop_button.setEnabled(False)\n",
    "        \n",
    "        button_layout.addWidget(self.start_button)\n",
    "        button_layout.addWidget(self.stop_button)\n",
    "        video_layout.addLayout(button_layout)\n",
    "        \n",
    "        content_layout.addLayout(video_layout, 2)\n",
    "        \n",
    "        # Right: Status and event log\n",
    "        info_layout = QVBoxLayout()\n",
    "        \n",
    "        # Status panel\n",
    "        status_frame = QFrame()\n",
    "        status_frame.setStyleSheet(\"border: 2px solid #333; border-radius: 5px; padding: 10px;\")\n",
    "        status_layout = QVBoxLayout(status_frame)\n",
    "        \n",
    "        status_title = QLabel(\"System Status\")\n",
    "        status_title.setFont(QFont(\"Arial\", 14, QFont.Weight.Bold))\n",
    "        status_layout.addWidget(status_title)\n",
    "        \n",
    "        self.status_label = QLabel(\"Status: Not Monitoring\")\n",
    "        self.status_label.setFont(QFont(\"Arial\", 12))\n",
    "        self.status_label.setStyleSheet(\"color: gray; padding: 10px;\")\n",
    "        self.status_label.setWordWrap(True)\n",
    "        status_layout.addWidget(self.status_label)\n",
    "        \n",
    "        self.metrics_label = QLabel(\"Metrics: --\")\n",
    "        self.metrics_label.setFont(QFont(\"Arial\", 10))\n",
    "        self.metrics_label.setWordWrap(True)\n",
    "        status_layout.addWidget(self.metrics_label)\n",
    "        \n",
    "        info_layout.addWidget(status_frame)\n",
    "        \n",
    "        # Event log\n",
    "        log_title = QLabel(\"Event Log\")\n",
    "        log_title.setFont(QFont(\"Arial\", 14, QFont.Weight.Bold))\n",
    "        info_layout.addWidget(log_title)\n",
    "        \n",
    "        self.event_log = QTextEdit()\n",
    "        self.event_log.setReadOnly(True)\n",
    "        self.event_log.setStyleSheet(\"border: 2px solid #333; border-radius: 5px;\")\n",
    "        info_layout.addWidget(self.event_log)\n",
    "        \n",
    "        content_layout.addLayout(info_layout, 1)\n",
    "        \n",
    "        main_layout.addLayout(content_layout)\n",
    "        \n",
    "        # Flash timer for alert\n",
    "        self.flash_timer = QTimer()\n",
    "        self.flash_timer.timeout.connect(self.toggle_flash)\n",
    "        self.flash_state = False\n",
    "        \n",
    "        # Initial log message\n",
    "        self.log_event(\"System initialized. Ready to start monitoring.\")\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start video monitoring\"\"\"\n",
    "        self.start_button.setEnabled(False)\n",
    "        self.stop_button.setEnabled(True)\n",
    "        \n",
    "        # Create and start video processing thread\n",
    "        self.video_thread = VideoProcessorThread(Config.VIDEO_SOURCE)\n",
    "        self.video_thread.frame_processed.connect(self.update_display)\n",
    "        self.video_thread.error_occurred.connect(self.handle_error)\n",
    "        self.video_thread.start()\n",
    "        \n",
    "        self.log_event(\"Monitoring started.\")\n",
    "        self.status_label.setText(\"Status: Monitoring Active\")\n",
    "        self.status_label.setStyleSheet(\"color: green; padding: 10px;\")\n",
    "        \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop video monitoring\"\"\"\n",
    "        if self.video_thread:\n",
    "            self.video_thread.stop()\n",
    "            self.video_thread.wait()\n",
    "            self.video_thread = None\n",
    "        \n",
    "        self.start_button.setEnabled(True)\n",
    "        self.stop_button.setEnabled(False)\n",
    "        self.flash_timer.stop()\n",
    "        \n",
    "        self.log_event(\"Monitoring stopped.\")\n",
    "        self.status_label.setText(\"Status: Not Monitoring\")\n",
    "        self.status_label.setStyleSheet(\"color: gray; padding: 10px;\")\n",
    "        self.video_label.clear()\n",
    "        self.video_label.setText(\"Video feed stopped\")\n",
    "        \n",
    "    def update_display(self, results: dict):\n",
    "        \"\"\"Update GUI with detection results\"\"\"\n",
    "        # Update video display\n",
    "        frame = results['annotated_frame']\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = rgb_frame.shape\n",
    "        bytes_per_line = ch * w\n",
    "        qt_image = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)\n",
    "        pixmap = QPixmap.fromImage(qt_image)\n",
    "        \n",
    "        # Scale to fit label while maintaining aspect ratio\n",
    "        scaled_pixmap = pixmap.scaled(\n",
    "            self.video_label.size(),\n",
    "            Qt.AspectRatioMode.KeepAspectRatio,\n",
    "            Qt.TransformationMode.SmoothTransformation\n",
    "        )\n",
    "        self.video_label.setPixmap(scaled_pixmap)\n",
    "        \n",
    "        # Update status\n",
    "        alert_info = results['alert']\n",
    "        self.status_label.setText(alert_info['message'])\n",
    "        \n",
    "        # Set color and flash for critical alerts\n",
    "        color = alert_info['color']\n",
    "        if alert_info['tier'] >= 2:\n",
    "            if not self.flash_timer.isActive():\n",
    "                self.flash_timer.start(500)  # Flash every 500ms\n",
    "        else:\n",
    "            self.flash_timer.stop()\n",
    "            self.status_label.setStyleSheet(f\"color: {color}; padding: 10px; font-weight: bold; font-size: 14px;\")\n",
    "        \n",
    "        # Update metrics\n",
    "        metrics_text = f\"\"\"\n",
    "        Person Detected: {'‚úì' if results['person_detected'] else '‚úó'}\n",
    "        State: {results['state']}\n",
    "        Eye Aspect Ratio: {results['ear']:.3f}\n",
    "        Head Pitch: {results['pitch']:.1f}¬∞\n",
    "        Head Yaw: {results['yaw']:.1f}¬∞\n",
    "        Alert Tier: {alert_info['tier']}\n",
    "        \"\"\"\n",
    "        self.metrics_label.setText(metrics_text)\n",
    "        \n",
    "        # Log significant events\n",
    "        if alert_info['tier'] == 1 and not hasattr(self, '_last_tier1_log'):\n",
    "            self.log_event(f\"‚ö†Ô∏è TIER 1 WARNING: {results['state']} detected\")\n",
    "            self._last_tier1_log = True\n",
    "        elif alert_info['tier'] == 2 and not hasattr(self, '_last_tier2_log'):\n",
    "            self.log_event(f\"üö® TIER 2 ALARM: {results['state']} persisting!\")\n",
    "            self._last_tier2_log = True\n",
    "            delattr(self, '_last_tier1_log')\n",
    "        elif alert_info['tier'] == 3 and not hasattr(self, '_last_tier3_log'):\n",
    "            self.log_event(f\"üî¥ TIER 3 CRITICAL: {results['state']} - Escalation required!\")\n",
    "            self._last_tier3_log = True\n",
    "            delattr(self, '_last_tier2_log')\n",
    "        elif alert_info['tier'] == 0:\n",
    "            # Reset logging flags when back to normal\n",
    "            if hasattr(self, '_last_tier1_log'):\n",
    "                delattr(self, '_last_tier1_log')\n",
    "            if hasattr(self, '_last_tier2_log'):\n",
    "                delattr(self, '_last_tier2_log')\n",
    "            if hasattr(self, '_last_tier3_log'):\n",
    "                self.log_event(f\"‚úì Guard returned to attentive state\")\n",
    "                delattr(self, '_last_tier3_log')\n",
    "    \n",
    "    def toggle_flash(self):\n",
    "        \"\"\"Toggle flash effect for critical alerts\"\"\"\n",
    "        self.flash_state = not self.flash_state\n",
    "        if self.flash_state:\n",
    "            self.status_label.setStyleSheet(\"color: white; background-color: red; padding: 10px; font-weight: bold; font-size: 14px;\")\n",
    "        else:\n",
    "            self.status_label.setStyleSheet(\"color: red; padding: 10px; font-weight: bold; font-size: 14px;\")\n",
    "    \n",
    "    def handle_error(self, error_message: str):\n",
    "        \"\"\"Handle errors from video processing thread\"\"\"\n",
    "        self.log_event(f\"‚ùå ERROR: {error_message}\")\n",
    "        self.stop_monitoring()\n",
    "    \n",
    "    def log_event(self, message: str):\n",
    "        \"\"\"Add an event to the log with timestamp\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {message}\"\n",
    "        self.event_log.append(log_entry)\n",
    "        # Auto-scroll to bottom\n",
    "        self.event_log.verticalScrollBar().setValue(\n",
    "            self.event_log.verticalScrollBar().maximum()\n",
    "        )\n",
    "    \n",
    "    def closeEvent(self, event):\n",
    "        \"\"\"Clean up when closing the application\"\"\"\n",
    "        if self.video_thread:\n",
    "            self.video_thread.stop()\n",
    "            self.video_thread.wait()\n",
    "        event.accept()\n",
    "\n",
    "\n",
    "# ==================== MAIN ENTRY POINT ====================\n",
    "def main():\n",
    "    \"\"\"Main entry point for the application\"\"\"\n",
    "    app = QApplication(sys.argv)\n",
    "    \n",
    "    # Set application style\n",
    "    app.setStyle('Fusion')\n",
    "    \n",
    "    # Create and show main window\n",
    "    window = SmartATMGuardian()\n",
    "    window.show()\n",
    "    \n",
    "    sys.exit(app.exec())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422318b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3114",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
