{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0640f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyQt6 opencv-python mediapipe numpy playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb9630",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  opencv-python   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyQt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2fde59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Smart ATM Guardian - Advanced Security Guard Monitoring System\n",
    "================================================================\n",
    "A real-time computer vision application that monitors security personnel\n",
    "for drowsiness, inattentiveness, and absence using MediaPipe and OpenCV.\n",
    "\n",
    "Requirements:\n",
    "pip install PyQt6 opencv-python mediapipe numpy playsound\n",
    "\n",
    "Usage:\n",
    "python smart_atm_guardian.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import time\n",
    "from PyQt6.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, \n",
    "                              QHBoxLayout, QPushButton, QLabel, QTextEdit, QFrame)\n",
    "from PyQt6.QtCore import QThread, pyqtSignal, Qt, QTimer\n",
    "from PyQt6.QtGui import QImage, QPixmap, QFont\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Try to import playsound, but make it optional for testing\n",
    "try:\n",
    "    from playsound import playsound\n",
    "    SOUND_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: playsound not available. Audio alerts disabled.\")\n",
    "    SOUND_AVAILABLE = False\n",
    "\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for the Smart ATM Guardian system\"\"\"\n",
    "    \n",
    "    # Video source (0 for webcam, or RTSP URL like \"rtsp://camera_ip:554/stream\")\n",
    "    VIDEO_SOURCE = 0\n",
    "    \n",
    "    # Detection thresholds (in seconds)\n",
    "    DROWSINESS_HEAD_DOWN_THRESHOLD = 10.0\n",
    "    DROWSINESS_EYES_CLOSED_THRESHOLD = 3.0\n",
    "    INATTENTIVENESS_THRESHOLD = 60.0\n",
    "    ABSENCE_THRESHOLD = 120.0\n",
    "    \n",
    "    # Alert escalation timing (in seconds)\n",
    "    TIER1_TO_TIER2_DELAY = 15.0\n",
    "    TIER2_TO_TIER3_DELAY = 30.0\n",
    "    \n",
    "    # Eye Aspect Ratio threshold for detecting closed eyes\n",
    "    EAR_THRESHOLD = 0.2\n",
    "    \n",
    "    # Head pose angle thresholds (in degrees)\n",
    "    HEAD_DOWN_ANGLE = 25.0  # Pitch angle indicating head down\n",
    "    HEAD_TURNED_ANGLE = 45.0  # Yaw angle indicating head turned away\n",
    "    \n",
    "    # ROI for guard presence detection (as fraction of frame: x, y, w, h)\n",
    "    GUARD_ROI = (0.3, 0.2, 0.4, 0.6)  # Center region of frame\n",
    "    \n",
    "    # Sound files (placeholders - user should provide actual files)\n",
    "    SOUND_WARNING = \"warning.wav\"  # Gentle beep or cough\n",
    "    SOUND_ALARM = \"wake_up_alarm.wav\"  # Loud wake-up sound\n",
    "    \n",
    "\n",
    "# ==================== DETECTION ENGINE ====================\n",
    "class GuardDetector:\n",
    "    \"\"\"\n",
    "    Core detection engine using MediaPipe for pose and face analysis.\n",
    "    Implements multi-modal detection: drowsiness, inattentiveness, absence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize MediaPipe solutions\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        \n",
    "        # Create pose and face mesh detectors\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        # State tracking\n",
    "        self.head_down_start_time = None\n",
    "        self.eyes_closed_start_time = None\n",
    "        self.head_turned_start_time = None\n",
    "        self.absence_start_time = None\n",
    "        \n",
    "        # Eye landmarks for EAR calculation (left and right eye)\n",
    "        self.LEFT_EYE_INDICES = [33, 160, 158, 133, 153, 144]\n",
    "        self.RIGHT_EYE_INDICES = [362, 385, 387, 263, 373, 380]\n",
    "        \n",
    "    def calculate_ear(self, eye_landmarks) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Eye Aspect Ratio (EAR) to detect eye closure.\n",
    "        \n",
    "        EAR = (||p2-p6|| + ||p3-p5||) / (2 * ||p1-p4||)\n",
    "        where p1-p6 are the eye landmark points.\n",
    "        \n",
    "        Lower EAR values indicate closed eyes.\n",
    "        \"\"\"\n",
    "        # Vertical distances\n",
    "        v1 = np.linalg.norm(eye_landmarks[1] - eye_landmarks[5])\n",
    "        v2 = np.linalg.norm(eye_landmarks[2] - eye_landmarks[4])\n",
    "        \n",
    "        # Horizontal distance\n",
    "        h = np.linalg.norm(eye_landmarks[0] - eye_landmarks[3])\n",
    "        \n",
    "        # EAR calculation\n",
    "        ear = (v1 + v2) / (2.0 * h + 1e-6)  # Add epsilon to avoid division by zero\n",
    "        return ear\n",
    "    \n",
    "    def get_head_pose_angles(self, face_landmarks, image_shape) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Calculate head pose angles (pitch, yaw, roll) from face landmarks.\n",
    "        \n",
    "        Pitch: Head up/down (positive = looking up, negative = looking down)\n",
    "        Yaw: Head left/right (positive = turned right, negative = turned left)\n",
    "        Roll: Head tilt\n",
    "        \"\"\"\n",
    "        h, w = image_shape[:2]\n",
    "        \n",
    "        # Key facial landmarks for pose estimation\n",
    "        # Nose tip, chin, left eye corner, right eye corner, left mouth corner, right mouth corner\n",
    "        key_indices = [1, 152, 33, 263, 61, 291]\n",
    "        \n",
    "        if not face_landmarks:\n",
    "            return 0.0, 0.0, 0.0\n",
    "        \n",
    "        # Extract 2D landmarks\n",
    "        landmarks_2d = []\n",
    "        for idx in key_indices:\n",
    "            lm = face_landmarks.landmark[idx]\n",
    "            landmarks_2d.append([lm.x * w, lm.y * h])\n",
    "        landmarks_2d = np.array(landmarks_2d, dtype=np.float64)\n",
    "        \n",
    "        # 3D model points (approximate facial structure)\n",
    "        model_points = np.array([\n",
    "            (0.0, 0.0, 0.0),           # Nose tip\n",
    "            (0.0, -330.0, -65.0),      # Chin\n",
    "            (-225.0, 170.0, -135.0),   # Left eye corner\n",
    "            (225.0, 170.0, -135.0),    # Right eye corner\n",
    "            (-150.0, -150.0, -125.0),  # Left mouth corner\n",
    "            (150.0, -150.0, -125.0)    # Right mouth corner\n",
    "        ])\n",
    "        \n",
    "        # Camera internals (approximate)\n",
    "        focal_length = w\n",
    "        center = (w / 2, h / 2)\n",
    "        camera_matrix = np.array([\n",
    "            [focal_length, 0, center[0]],\n",
    "            [0, focal_length, center[1]],\n",
    "            [0, 0, 1]\n",
    "        ], dtype=np.float64)\n",
    "        \n",
    "        dist_coeffs = np.zeros((4, 1))  # Assuming no lens distortion\n",
    "        \n",
    "        # Solve PnP to get rotation vector\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            model_points, landmarks_2d, camera_matrix, dist_coeffs\n",
    "        )\n",
    "        \n",
    "        if not success:\n",
    "            return 0.0, 0.0, 0.0\n",
    "        \n",
    "        # Convert rotation vector to rotation matrix\n",
    "        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "        \n",
    "        # Calculate Euler angles\n",
    "        sy = np.sqrt(rotation_matrix[0, 0] ** 2 + rotation_matrix[1, 0] ** 2)\n",
    "        singular = sy < 1e-6\n",
    "        \n",
    "        if not singular:\n",
    "            pitch = np.arctan2(rotation_matrix[2, 1], rotation_matrix[2, 2])\n",
    "            yaw = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "            roll = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
    "        else:\n",
    "            pitch = np.arctan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])\n",
    "            yaw = np.arctan2(-rotation_matrix[2, 0], sy)\n",
    "            roll = 0\n",
    "        \n",
    "        # Convert to degrees\n",
    "        pitch = np.degrees(pitch)\n",
    "        yaw = np.degrees(yaw)\n",
    "        roll = np.degrees(roll)\n",
    "        \n",
    "        return pitch, yaw, roll\n",
    "    \n",
    "    def detect_person_in_roi(self, pose_landmarks, image_shape) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a person (guard) is present in the designated ROI.\n",
    "        \"\"\"\n",
    "        if not pose_landmarks:\n",
    "            return False\n",
    "        \n",
    "        h, w = image_shape[:2]\n",
    "        roi_x, roi_y, roi_w, roi_h = Config.GUARD_ROI\n",
    "        \n",
    "        # Check if key body landmarks (shoulders, nose) are in ROI\n",
    "        key_landmarks = [\n",
    "            pose_landmarks.landmark[self.mp_pose.PoseLandmark.NOSE],\n",
    "            pose_landmarks.landmark[self.mp_pose.PoseLandmark.LEFT_SHOULDER],\n",
    "            pose_landmarks.landmark[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "        ]\n",
    "        \n",
    "        in_roi_count = 0\n",
    "        for lm in key_landmarks:\n",
    "            x, y = lm.x, lm.y\n",
    "            if (roi_x <= x <= roi_x + roi_w) and (roi_y <= y <= roi_y + roi_h):\n",
    "                in_roi_count += 1\n",
    "        \n",
    "        # At least 2 out of 3 key landmarks should be in ROI\n",
    "        return in_roi_count >= 2\n",
    "    \n",
    "    def process_frame(self, frame: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Process a single frame and detect guard states.\n",
    "        \n",
    "        Returns:\n",
    "            dict with detection results including state, timings, and annotated frame\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        h, w = frame.shape[:2]\n",
    "        \n",
    "        # Convert BGR to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process with pose and face mesh\n",
    "        pose_results = self.pose.process(rgb_frame)\n",
    "        face_results = self.face_mesh.process(rgb_frame)\n",
    "        \n",
    "        # Initialize detection results\n",
    "        results = {\n",
    "            'person_detected': False,\n",
    "            'eyes_closed': False,\n",
    "            'head_down': False,\n",
    "            'head_turned': False,\n",
    "            'drowsiness_duration': 0.0,\n",
    "            'inattentiveness_duration': 0.0,\n",
    "            'absence_duration': 0.0,\n",
    "            'state': 'UNKNOWN',\n",
    "            'pitch': 0.0,\n",
    "            'yaw': 0.0,\n",
    "            'ear': 0.0\n",
    "        }\n",
    "        \n",
    "        # Draw ROI on frame\n",
    "        roi_x, roi_y, roi_w, roi_h = Config.GUARD_ROI\n",
    "        cv2.rectangle(frame, \n",
    "                     (int(roi_x * w), int(roi_y * h)),\n",
    "                     (int((roi_x + roi_w) * w), int((roi_y + roi_h) * h)),\n",
    "                     (0, 255, 0), 2)\n",
    "        cv2.putText(frame, \"Guard Post\", (int(roi_x * w), int(roi_y * h) - 10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        \n",
    "        # Check for person presence\n",
    "        if pose_results.pose_landmarks:\n",
    "            results['person_detected'] = self.detect_person_in_roi(\n",
    "                pose_results.pose_landmarks, frame.shape\n",
    "            )\n",
    "            \n",
    "            # Draw pose landmarks\n",
    "            self.mp_drawing.draw_landmarks(\n",
    "                frame, pose_results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS\n",
    "            )\n",
    "        \n",
    "        # Absence detection\n",
    "        if not results['person_detected']:\n",
    "            if self.absence_start_time is None:\n",
    "                self.absence_start_time = current_time\n",
    "            absence_duration = current_time - self.absence_start_time\n",
    "            results['absence_duration'] = absence_duration\n",
    "            \n",
    "            if absence_duration > Config.ABSENCE_THRESHOLD:\n",
    "                results['state'] = 'ABSENT'\n",
    "            else:\n",
    "                results['state'] = 'PERSON_NOT_IN_POST'\n",
    "            \n",
    "            # Reset other timers\n",
    "            self.head_down_start_time = None\n",
    "            self.eyes_closed_start_time = None\n",
    "            self.head_turned_start_time = None\n",
    "            \n",
    "        else:\n",
    "            # Person is present, reset absence timer\n",
    "            self.absence_start_time = None\n",
    "            \n",
    "            # Analyze face for drowsiness and attention\n",
    "            if face_results.multi_face_landmarks:\n",
    "                face_landmarks = face_results.multi_face_landmarks[0]\n",
    "                \n",
    "                # Draw face mesh\n",
    "                self.mp_drawing.draw_landmarks(\n",
    "                    frame, face_landmarks, self.mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=self.mp_drawing.DrawingSpec(\n",
    "                        color=(0, 255, 0), thickness=1\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Calculate EAR for both eyes\n",
    "                left_eye_points = np.array([\n",
    "                    [face_landmarks.landmark[i].x * w, face_landmarks.landmark[i].y * h]\n",
    "                    for i in self.LEFT_EYE_INDICES\n",
    "                ])\n",
    "                right_eye_points = np.array([\n",
    "                    [face_landmarks.landmark[i].x * w, face_landmarks.landmark[i].y * h]\n",
    "                    for i in self.RIGHT_EYE_INDICES\n",
    "                ])\n",
    "                \n",
    "                left_ear = self.calculate_ear(left_eye_points)\n",
    "                right_ear = self.calculate_ear(right_eye_points)\n",
    "                avg_ear = (left_ear + right_ear) / 2.0\n",
    "                results['ear'] = avg_ear\n",
    "                \n",
    "                # Check for closed eyes\n",
    "                if avg_ear < Config.EAR_THRESHOLD:\n",
    "                    results['eyes_closed'] = True\n",
    "                    if self.eyes_closed_start_time is None:\n",
    "                        self.eyes_closed_start_time = current_time\n",
    "                else:\n",
    "                    self.eyes_closed_start_time = None\n",
    "                \n",
    "                # Get head pose\n",
    "                pitch, yaw, roll = self.get_head_pose_angles(face_landmarks, frame.shape)\n",
    "                results['pitch'] = pitch\n",
    "                results['yaw'] = yaw\n",
    "                \n",
    "                # Check for head down (drowsiness indicator)\n",
    "                if pitch < -Config.HEAD_DOWN_ANGLE:  # Negative pitch = looking down\n",
    "                    results['head_down'] = True\n",
    "                    if self.head_down_start_time is None:\n",
    "                        self.head_down_start_time = current_time\n",
    "                else:\n",
    "                    self.head_down_start_time = None\n",
    "                \n",
    "                # Check for head turned away (inattentiveness)\n",
    "                if abs(yaw) > Config.HEAD_TURNED_ANGLE:\n",
    "                    results['head_turned'] = True\n",
    "                    if self.head_turned_start_time is None:\n",
    "                        self.head_turned_start_time = current_time\n",
    "                else:\n",
    "                    self.head_turned_start_time = None\n",
    "                \n",
    "                # Calculate durations\n",
    "                if self.eyes_closed_start_time:\n",
    "                    eyes_closed_duration = current_time - self.eyes_closed_start_time\n",
    "                else:\n",
    "                    eyes_closed_duration = 0.0\n",
    "                \n",
    "                if self.head_down_start_time:\n",
    "                    head_down_duration = current_time - self.head_down_start_time\n",
    "                else:\n",
    "                    head_down_duration = 0.0\n",
    "                \n",
    "                if self.head_turned_start_time:\n",
    "                    inattentive_duration = current_time - self.head_turned_start_time\n",
    "                    results['inattentiveness_duration'] = inattentive_duration\n",
    "                \n",
    "                # Determine state based on combined indicators\n",
    "                # Drowsiness = (head down > 10s) OR (eyes closed > 3s)\n",
    "                if (head_down_duration > Config.DROWSINESS_HEAD_DOWN_THRESHOLD or\n",
    "                    eyes_closed_duration > Config.DROWSINESS_EYES_CLOSED_THRESHOLD):\n",
    "                    results['state'] = 'DROWSY'\n",
    "                    results['drowsiness_duration'] = max(head_down_duration, eyes_closed_duration)\n",
    "                elif inattentive_duration > Config.INATTENTIVENESS_THRESHOLD:\n",
    "                    results['state'] = 'INATTENTIVE'\n",
    "                else:\n",
    "                    results['state'] = 'ATTENTIVE'\n",
    "                \n",
    "                # Display metrics on frame\n",
    "                cv2.putText(frame, f\"EAR: {avg_ear:.2f}\", (10, 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                cv2.putText(frame, f\"Pitch: {pitch:.1f} Yaw: {yaw:.1f}\", (10, 60),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            else:\n",
    "                # Face not detected but person present\n",
    "                results['state'] = 'FACE_NOT_DETECTED'\n",
    "        \n",
    "        results['annotated_frame'] = frame\n",
    "        return results\n",
    "\n",
    "\n",
    "# ==================== ALERT MANAGER ====================\n",
    "class AlertManager:\n",
    "    \"\"\"\n",
    "    Manages the tiered alert system with escalation logic.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_alert_tier = 0\n",
    "        self.tier1_start_time = None\n",
    "        self.tier2_start_time = None\n",
    "        self.last_sound_time = 0\n",
    "        self.sound_cooldown = 2.0  # Minimum seconds between sounds\n",
    "        \n",
    "    def play_sound(self, sound_file: str):\n",
    "        \"\"\"Play sound file with cooldown to avoid spam\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_sound_time >= self.sound_cooldown:\n",
    "            if SOUND_AVAILABLE:\n",
    "                try:\n",
    "                    playsound(sound_file, block=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error playing sound: {e}\")\n",
    "            else:\n",
    "                print(f\"[SOUND] Would play: {sound_file}\")\n",
    "            self.last_sound_time = current_time\n",
    "    \n",
    "    def update(self, state: str, duration: float) -> dict:\n",
    "        \"\"\"\n",
    "        Update alert state based on detected condition.\n",
    "        \n",
    "        Returns dict with alert_tier, message, and color\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Reset if state is normal\n",
    "        if state in ['ATTENTIVE', 'PERSON_NOT_IN_POST']:\n",
    "            self.current_alert_tier = 0\n",
    "            self.tier1_start_time = None\n",
    "            self.tier2_start_time = None\n",
    "            return {\n",
    "                'tier': 0,\n",
    "                'message': f'Status: {state}',\n",
    "                'color': 'green'\n",
    "            }\n",
    "        \n",
    "        # Handle problematic states\n",
    "        if state in ['DROWSY', 'INATTENTIVE']:\n",
    "            # Tier 1: Initial warning\n",
    "            if self.current_alert_tier == 0:\n",
    "                self.current_alert_tier = 1\n",
    "                self.tier1_start_time = current_time\n",
    "                self.play_sound(Config.SOUND_WARNING)\n",
    "                return {\n",
    "                    'tier': 1,\n",
    "                    'message': f'WARNING: {state} detected!',\n",
    "                    'color': 'yellow'\n",
    "                }\n",
    "            \n",
    "            # Tier 2: Escalation after delay\n",
    "            elif self.current_alert_tier == 1:\n",
    "                tier1_duration = current_time - self.tier1_start_time\n",
    "                if tier1_duration > Config.TIER1_TO_TIER2_DELAY:\n",
    "                    self.current_alert_tier = 2\n",
    "                    self.tier2_start_time = current_time\n",
    "                    self.play_sound(Config.SOUND_ALARM)\n",
    "                    return {\n",
    "                        'tier': 2,\n",
    "                        'message': f'ALARM: {state} persisting! WAKE UP!',\n",
    "                        'color': 'red'\n",
    "                    }\n",
    "                return {\n",
    "                    'tier': 1,\n",
    "                    'message': f'WARNING: {state} ({tier1_duration:.0f}s)',\n",
    "                    'color': 'yellow'\n",
    "                }\n",
    "            \n",
    "            # Tier 3: Critical escalation\n",
    "            elif self.current_alert_tier == 2:\n",
    "                tier2_duration = current_time - self.tier2_start_time\n",
    "                if tier2_duration > Config.TIER2_TO_TIER3_DELAY:\n",
    "                    self.current_alert_tier = 3\n",
    "                    return {\n",
    "                        'tier': 3,\n",
    "                        'message': f'CRITICAL: {state} - Supervisor notified!',\n",
    "                        'color': 'red'\n",
    "                    }\n",
    "                return {\n",
    "                    'tier': 2,\n",
    "                    'message': f'ALARM: {state} ({tier2_duration:.0f}s)',\n",
    "                    'color': 'red'\n",
    "                }\n",
    "        \n",
    "        # Handle absence\n",
    "        if state == 'ABSENT':\n",
    "            self.current_alert_tier = 3\n",
    "            return {\n",
    "                'tier': 3,\n",
    "                'message': 'CRITICAL: Guard absent from post!',\n",
    "                'color': 'red'\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'tier': 0,\n",
    "            'message': f'Status: {state}',\n",
    "            'color': 'gray'\n",
    "        }\n",
    "\n",
    "\n",
    "# ==================== VIDEO PROCESSING THREAD ====================\n",
    "class VideoProcessorThread(QThread):\n",
    "    \"\"\"\n",
    "    Worker thread for video capture and processing.\n",
    "    Runs detection engine and emits results to GUI.\n",
    "    \"\"\"\n",
    "    \n",
    "    frame_processed = pyqtSignal(object)  # Emits detection results dict\n",
    "    error_occurred = pyqtSignal(str)\n",
    "    \n",
    "    def __init__(self, video_source):\n",
    "        super().__init__()\n",
    "        self.video_source = video_source\n",
    "        self.running = False\n",
    "        self.detector = GuardDetector()\n",
    "        self.alert_manager = AlertManager()\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Main processing loop\"\"\"\n",
    "        self.running = True\n",
    "        cap = cv2.VideoCapture(self.video_source)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            self.error_occurred.emit(f\"Failed to open video source: {self.video_source}\")\n",
    "            return\n",
    "        \n",
    "        while self.running:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                self.error_occurred.emit(\"Failed to read frame from video source\")\n",
    "                break\n",
    "            \n",
    "            # Process frame with detector\n",
    "            results = self.detector.process_frame(frame)\n",
    "            \n",
    "            # Update alert system\n",
    "            alert_info = self.alert_manager.update(\n",
    "                results['state'],\n",
    "                results.get('drowsiness_duration', 0.0) or \n",
    "                results.get('inattentiveness_duration', 0.0) or\n",
    "                results.get('absence_duration', 0.0)\n",
    "            )\n",
    "            \n",
    "            # Combine results\n",
    "            results['alert'] = alert_info\n",
    "            \n",
    "            # Emit to GUI\n",
    "            self.frame_processed.emit(results)\n",
    "        \n",
    "        cap.release()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the processing thread\"\"\"\n",
    "        self.running = False\n",
    "\n",
    "\n",
    "# ==================== MAIN GUI APPLICATION ====================\n",
    "class SmartATMGuardian(QMainWindow):\n",
    "    \"\"\"\n",
    "    Main application window for the Smart ATM Guardian system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.video_thread = None\n",
    "        self.init_ui()\n",
    "        \n",
    "    def init_ui(self):\n",
    "        \"\"\"Initialize the user interface\"\"\"\n",
    "        self.setWindowTitle(\"Smart ATM Guardian - Security Monitoring System\")\n",
    "        self.setGeometry(100, 100, 1200, 800)\n",
    "        \n",
    "        # Central widget\n",
    "        central_widget = QWidget()\n",
    "        self.setCentralWidget(central_widget)\n",
    "        main_layout = QVBoxLayout(central_widget)\n",
    "        \n",
    "        # Title\n",
    "        title_label = QLabel(\"ðŸ›¡ï¸ Smart ATM Guardian\")\n",
    "        title_font = QFont(\"Arial\", 20, QFont.Weight.Bold)\n",
    "        title_label.setFont(title_font)\n",
    "        title_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        main_layout.addWidget(title_label)\n",
    "        \n",
    "        # Video and info layout\n",
    "        content_layout = QHBoxLayout()\n",
    "        \n",
    "        # Left: Video display\n",
    "        video_layout = QVBoxLayout()\n",
    "        self.video_label = QLabel()\n",
    "        self.video_label.setMinimumSize(640, 480)\n",
    "        self.video_label.setStyleSheet(\"border: 2px solid #333; background-color: black;\")\n",
    "        self.video_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        video_layout.addWidget(self.video_label)\n",
    "        \n",
    "        # Control buttons\n",
    "        button_layout = QHBoxLayout()\n",
    "        self.start_button = QPushButton(\"â–¶ Start Monitoring\")\n",
    "        self.start_button.setStyleSheet(\"\"\"\n",
    "            QPushButton {\n",
    "                background-color: #4CAF50;\n",
    "                color: white;\n",
    "                font-size: 16px;\n",
    "                padding: 10px;\n",
    "                border-radius: 5px;\n",
    "            }\n",
    "            QPushButton:hover {\n",
    "                background-color: #45a049;\n",
    "            }\n",
    "        \"\"\")\n",
    "        self.start_button.clicked.connect(self.start_monitoring)\n",
    "        \n",
    "        self.stop_button = QPushButton(\"â¹ Stop Monitoring\")\n",
    "        self.stop_button.setStyleSheet(\"\"\"\n",
    "            QPushButton {\n",
    "                background-color: #f44336;\n",
    "                color: white;\n",
    "                font-size: 16px;\n",
    "                padding: 10px;\n",
    "                border-radius: 5px;\n",
    "            }\n",
    "            QPushButton:hover {\n",
    "                background-color: #da190b;\n",
    "            }\n",
    "            QPushButton:disabled {\n",
    "                background-color: #cccccc;\n",
    "            }\n",
    "        \"\"\")\n",
    "        self.stop_button.clicked.connect(self.stop_monitoring)\n",
    "        self.stop_button.setEnabled(False)\n",
    "        \n",
    "        button_layout.addWidget(self.start_button)\n",
    "        button_layout.addWidget(self.stop_button)\n",
    "        video_layout.addLayout(button_layout)\n",
    "        \n",
    "        content_layout.addLayout(video_layout, 2)\n",
    "        \n",
    "        # Right: Status and event log\n",
    "        info_layout = QVBoxLayout()\n",
    "        \n",
    "        # Status panel\n",
    "        status_frame = QFrame()\n",
    "        status_frame.setStyleSheet(\"border: 2px solid #333; border-radius: 5px; padding: 10px;\")\n",
    "        status_layout = QVBoxLayout(status_frame)\n",
    "        \n",
    "        status_title = QLabel(\"System Status\")\n",
    "        status_title.setFont(QFont(\"Arial\", 14, QFont.Weight.Bold))\n",
    "        status_layout.addWidget(status_title)\n",
    "        \n",
    "        self.status_label = QLabel(\"Status: Not Monitoring\")\n",
    "        self.status_label.setFont(QFont(\"Arial\", 12))\n",
    "        self.status_label.setStyleSheet(\"color: gray; padding: 10px;\")\n",
    "        self.status_label.setWordWrap(True)\n",
    "        status_layout.addWidget(self.status_label)\n",
    "        \n",
    "        self.metrics_label = QLabel(\"Metrics: --\")\n",
    "        self.metrics_label.setFont(QFont(\"Arial\", 10))\n",
    "        self.metrics_label.setWordWrap(True)\n",
    "        status_layout.addWidget(self.metrics_label)\n",
    "        \n",
    "        info_layout.addWidget(status_frame)\n",
    "        \n",
    "        # Event log\n",
    "        log_title = QLabel(\"Event Log\")\n",
    "        log_title.setFont(QFont(\"Arial\", 14, QFont.Weight.Bold))\n",
    "        info_layout.addWidget(log_title)\n",
    "        \n",
    "        self.event_log = QTextEdit()\n",
    "        self.event_log.setReadOnly(True)\n",
    "        self.event_log.setStyleSheet(\"border: 2px solid #333; border-radius: 5px;\")\n",
    "        info_layout.addWidget(self.event_log)\n",
    "        \n",
    "        content_layout.addLayout(info_layout, 1)\n",
    "        \n",
    "        main_layout.addLayout(content_layout)\n",
    "        \n",
    "        # Flash timer for alert\n",
    "        self.flash_timer = QTimer()\n",
    "        self.flash_timer.timeout.connect(self.toggle_flash)\n",
    "        self.flash_state = False\n",
    "        \n",
    "        # Initial log message\n",
    "        self.log_event(\"System initialized. Ready to start monitoring.\")\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start video monitoring\"\"\"\n",
    "        self.start_button.setEnabled(False)\n",
    "        self.stop_button.setEnabled(True)\n",
    "        \n",
    "        # Create and start video processing thread\n",
    "        self.video_thread = VideoProcessorThread(Config.VIDEO_SOURCE)\n",
    "        self.video_thread.frame_processed.connect(self.update_display)\n",
    "        self.video_thread.error_occurred.connect(self.handle_error)\n",
    "        self.video_thread.start()\n",
    "        \n",
    "        self.log_event(\"Monitoring started.\")\n",
    "        self.status_label.setText(\"Status: Monitoring Active\")\n",
    "        self.status_label.setStyleSheet(\"color: green; padding: 10px;\")\n",
    "        \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop video monitoring\"\"\"\n",
    "        if self.video_thread:\n",
    "            self.video_thread.stop()\n",
    "            self.video_thread.wait()\n",
    "            self.video_thread = None\n",
    "        \n",
    "        self.start_button.setEnabled(True)\n",
    "        self.stop_button.setEnabled(False)\n",
    "        self.flash_timer.stop()\n",
    "        \n",
    "        self.log_event(\"Monitoring stopped.\")\n",
    "        self.status_label.setText(\"Status: Not Monitoring\")\n",
    "        self.status_label.setStyleSheet(\"color: gray; padding: 10px;\")\n",
    "        self.video_label.clear()\n",
    "        self.video_label.setText(\"Video feed stopped\")\n",
    "        \n",
    "    def update_display(self, results: dict):\n",
    "        \"\"\"Update GUI with detection results\"\"\"\n",
    "        # Update video display\n",
    "        frame = results['annotated_frame']\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = rgb_frame.shape\n",
    "        bytes_per_line = ch * w\n",
    "        qt_image = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)\n",
    "        pixmap = QPixmap.fromImage(qt_image)\n",
    "        \n",
    "        # Scale to fit label while maintaining aspect ratio\n",
    "        scaled_pixmap = pixmap.scaled(\n",
    "            self.video_label.size(),\n",
    "            Qt.AspectRatioMode.KeepAspectRatio,\n",
    "            Qt.TransformationMode.SmoothTransformation\n",
    "        )\n",
    "        self.video_label.setPixmap(scaled_pixmap)\n",
    "        \n",
    "        # Update status\n",
    "        alert_info = results['alert']\n",
    "        self.status_label.setText(alert_info['message'])\n",
    "        \n",
    "        # Set color and flash for critical alerts\n",
    "        color = alert_info['color']\n",
    "        if alert_info['tier'] >= 2:\n",
    "            if not self.flash_timer.isActive():\n",
    "                self.flash_timer.start(500)  # Flash every 500ms\n",
    "        else:\n",
    "            self.flash_timer.stop()\n",
    "            self.status_label.setStyleSheet(f\"color: {color}; padding: 10px; font-weight: bold; font-size: 14px;\")\n",
    "        \n",
    "        # Update metrics\n",
    "        metrics_text = f\"\"\"\n",
    "        Person Detected: {'âœ“' if results['person_detected'] else 'âœ—'}\n",
    "        State: {results['state']}\n",
    "        Eye Aspect Ratio: {results['ear']:.3f}\n",
    "        Head Pitch: {results['pitch']:.1f}Â°\n",
    "        Head Yaw: {results['yaw']:.1f}Â°\n",
    "        Alert Tier: {alert_info['tier']}\n",
    "        \"\"\"\n",
    "        self.metrics_label.setText(metrics_text)\n",
    "        \n",
    "        # Log significant events\n",
    "        if alert_info['tier'] == 1 and not hasattr(self, '_last_tier1_log'):\n",
    "            self.log_event(f\"âš ï¸ TIER 1 WARNING: {results['state']} detected\")\n",
    "            self._last_tier1_log = True\n",
    "        elif alert_info['tier'] == 2 and not hasattr(self, '_last_tier2_log'):\n",
    "            self.log_event(f\"ðŸš¨ TIER 2 ALARM: {results['state']} persisting!\")\n",
    "            self._last_tier2_log = True\n",
    "            delattr(self, '_last_tier1_log')\n",
    "        elif alert_info['tier'] == 3 and not hasattr(self, '_last_tier3_log'):\n",
    "            self.log_event(f\"ðŸ”´ TIER 3 CRITICAL: {results['state']} - Escalation required!\")\n",
    "            self._last_tier3_log = True\n",
    "            delattr(self, '_last_tier2_log')\n",
    "        elif alert_info['tier'] == 0:\n",
    "            # Reset logging flags when back to normal\n",
    "            if hasattr(self, '_last_tier1_log'):\n",
    "                delattr(self, '_last_tier1_log')\n",
    "            if hasattr(self, '_last_tier2_log'):\n",
    "                delattr(self, '_last_tier2_log')\n",
    "            if hasattr(self, '_last_tier3_log'):\n",
    "                self.log_event(f\"âœ“ Guard returned to attentive state\")\n",
    "                delattr(self, '_last_tier3_log')\n",
    "    \n",
    "    def toggle_flash(self):\n",
    "        \"\"\"Toggle flash effect for critical alerts\"\"\"\n",
    "        self.flash_state = not self.flash_state\n",
    "        if self.flash_state:\n",
    "            self.status_label.setStyleSheet(\"color: white; background-color: red; padding: 10px; font-weight: bold; font-size: 14px;\")\n",
    "        else:\n",
    "            self.status_label.setStyleSheet(\"color: red; padding: 10px; font-weight: bold; font-size: 14px;\")\n",
    "    \n",
    "    def handle_error(self, error_message: str):\n",
    "        \"\"\"Handle errors from video processing thread\"\"\"\n",
    "        self.log_event(f\"âŒ ERROR: {error_message}\")\n",
    "        self.stop_monitoring()\n",
    "    \n",
    "    def log_event(self, message: str):\n",
    "        \"\"\"Add an event to the log with timestamp\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {message}\"\n",
    "        self.event_log.append(log_entry)\n",
    "        # Auto-scroll to bottom\n",
    "        self.event_log.verticalScrollBar().setValue(\n",
    "            self.event_log.verticalScrollBar().maximum()\n",
    "        )\n",
    "    \n",
    "    def closeEvent(self, event):\n",
    "        \"\"\"Clean up when closing the application\"\"\"\n",
    "        if self.video_thread:\n",
    "            self.video_thread.stop()\n",
    "            self.video_thread.wait()\n",
    "        event.accept()\n",
    "\n",
    "\n",
    "# ==================== MAIN ENTRY POINT ====================\n",
    "def main():\n",
    "    \"\"\"Main entry point for the application\"\"\"\n",
    "    app = QApplication(sys.argv)\n",
    "    \n",
    "    # Set application style\n",
    "    app.setStyle('Fusion')\n",
    "    \n",
    "    # Create and show main window\n",
    "    window = SmartATMGuardian()\n",
    "    window.show()\n",
    "    \n",
    "    sys.exit(app.exec())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1314788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422318b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# How many pixels of movement is \"not still\"?\n",
    "# This will need tuning based on your camera resolution and distance.\n",
    "STILLNESS_THRESHOLD_PX = 10\n",
    "\n",
    "# How many consecutive \"still\" frames trigger an alarm?\n",
    "# (e.g., 100 frames at ~20fps is ~5 seconds)\n",
    "SLEEP_ALARM_FRAMES = 100\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Sleeping Alert System (Dev Phase)\")\n",
    "        self.window.geometry(\"800x700\")\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- Face Detection ---\n",
    "        # Use the built-in Haar Cascade data from OpenCV\n",
    "        try:\n",
    "            face_cascade_path = os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml')\n",
    "            if not os.path.exists(face_cascade_path):\n",
    "                raise IOError(\"Haar cascade file not found.\")\n",
    "            self.face_cascade = cv2.CascadeClassifier(face_cascade_path)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to load Haar Cascade: {e}\\nPlease ensure OpenCV is correctly installed.\")\n",
    "            self.window.destroy()\n",
    "            return\n",
    "\n",
    "        # --- \"Sleeping\" Logic State ---\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "\n",
    "        # --- GUI Elements ---\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "        # Video Display Label\n",
    "        self.video_label = tk.Label(self.main_frame, bg=\"black\")\n",
    "        self.video_label.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Status Label\n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.main_frame, textvariable=self.status_text, font=(\"Arial\", 14), pady=10)\n",
    "        self.status_label.pack()\n",
    "\n",
    "        # Control Buttons\n",
    "        self.button_frame = tk.Frame(self.main_frame)\n",
    "        self.button_frame.pack()\n",
    "\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15, state=tk.DISABLED)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        # Set up the close protocol\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        \"\"\"\n",
    "        Starts the video capture in a new thread.\n",
    "        \"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)  # Use 0 for laptop webcam\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            \n",
    "            self.is_running = True\n",
    "            \n",
    "            # Start the video processing thread\n",
    "            # daemon=True ensures the thread will close when the main app closes\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            \n",
    "            self.start_button.config(state=tk.DISABLED)\n",
    "            self.stop_button.config(state=tk.NORMAL)\n",
    "            self.status_text.set(\"Camera running... looking for person.\")\n",
    "        \n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap:\n",
    "                self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        \"\"\"\n",
    "        Signals the video loop to stop.\n",
    "        \"\"\"\n",
    "        self.is_running = False\n",
    "        \n",
    "        # The thread will see self.is_running is False and exit\n",
    "        # We wait a moment for the thread to finish\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "            \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "        self.start_button.config(state=tk.NORMAL)\n",
    "        self.stop_button.config(state=tk.DISABLED)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        self.video_label.config(image=None) # Clear the image\n",
    "        \n",
    "        # Reset sleeping logic state\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "\n",
    "    def video_loop(self):\n",
    "        \"\"\"\n",
    "        The main loop for video processing. Runs in a separate thread.\n",
    "        \"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                # Flip for a \"mirror\" view, which is more intuitive\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Process the frame\n",
    "                processed_frame, status = self.detect_sleeping(frame)\n",
    "\n",
    "                # Update the status text\n",
    "                self.status_text.set(status)\n",
    "\n",
    "                # Convert the OpenCV (BGR) frame to a PIL (RGB) image\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                \n",
    "                # Resize image to fit the label (optional, but good for layout)\n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1: # Avoid division by zero on init\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "\n",
    "                # Convert PIL image to Tkinter-compatible image\n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "\n",
    "                # Update the video label in the GUI\n",
    "                # This must be done from the main thread, but tkinter seems to handle this call\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            # Control loop speed slightly\n",
    "            time.sleep(0.01) # ~100fps theoretical max, but processing will slow it down\n",
    "\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    def detect_sleeping(self, frame):\n",
    "        \"\"\"\n",
    "        Performs face detection and sleeping logic on a single frame.\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(50, 50) # Don't detect tiny faces\n",
    "        )\n",
    "\n",
    "        current_status = \"No Person Detected\"\n",
    "        alert_triggered = False\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            # We only track the largest/first face\n",
    "            (x, y, w, h) = faces[0]\n",
    "            \n",
    "            # Calculate the center of the face\n",
    "            center_x = x + w // 2\n",
    "            center_y = y + h // 2\n",
    "            current_center = (center_x, center_y)\n",
    "\n",
    "            current_status = \"Monitoring: Person Active\"\n",
    "\n",
    "            # Compare with the last known center\n",
    "            if self.last_face_center is not None:\n",
    "                # Calculate Euclidean distance\n",
    "                dist = np.linalg.norm(np.array(current_center) - np.array(self.last_face_center))\n",
    "\n",
    "                if dist < STILLNESS_THRESHOLD_PX:\n",
    "                    # Person is still\n",
    "                    self.stillness_counter += 1\n",
    "                else:\n",
    "                    # Person moved, reset counter\n",
    "                    self.stillness_counter = 0\n",
    "            \n",
    "            # Update the last known center\n",
    "            self.last_face_center = current_center\n",
    "\n",
    "            # Check if the stillness has crossed the alarm threshold\n",
    "            if self.stillness_counter > SLEEP_ALARM_FRAMES:\n",
    "                current_status = \"!!! ALERT: SLEEPING DETECTED !!!\"\n",
    "                alert_triggered = True\n",
    "            \n",
    "            # --- Draw on the frame ---\n",
    "            if alert_triggered:\n",
    "                # Draw a bright red box for alert\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "                cv2.putText(frame, \"ALERT!\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "                # We could also play a sound here\n",
    "                # e.g., import winsound; winsound.Beep(1000, 500)\n",
    "            else:\n",
    "                # Draw a standard blue box\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f\"Stillness: {self.stillness_counter}/{SLEEP_ALARM_FRAMES}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "        else:\n",
    "            # No face detected, reset the logic\n",
    "            self.last_face_center = None\n",
    "            self.stillness_counter = 0\n",
    "            current_status = \"No Person Detected\"\n",
    "\n",
    "        return frame, current_status\n",
    "\n",
    "    def on_closing(self):\n",
    "        \"\"\"\n",
    "        Handles the window close event.\n",
    "        \"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23929d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog  # Import simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# How many pixels of movement is \"not still\"?\n",
    "# This will need tuning based on your camera resolution and distance.\n",
    "STILLNESS_THRESHOLD_PX = 10\n",
    "\n",
    "# How many consecutive \"still\" frames trigger an alarm?\n",
    "# (e.g., 100 frames at ~20fps is ~5 seconds)\n",
    "SLEEP_ALARM_FRAMES = 100\n",
    "\n",
    "# New constant for tracking\n",
    "# If the closest-found face is further than this from the last frame,\n",
    "# we assume it's a new person or the tracker lost the original.\n",
    "MAX_TRACKING_JUMP_PX = 150\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Sleeping Alert System (Dev Phase)\")\n",
    "        self.window.geometry(\"800x700\")\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- Face Detection ---\n",
    "        # Use the built-in Haar Cascade data from OpenCV\n",
    "        try:\n",
    "            face_cascade_path = os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml')\n",
    "            if not os.path.exists(face_cascade_path):\n",
    "                raise IOError(\"Haar cascade file not found.\")\n",
    "            self.face_cascade = cv2.CascadeClassifier(face_cascade_path)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to load Haar Cascade: {e}\\nPlease ensure OpenCV is correctly installed.\")\n",
    "            self.window.destroy()\n",
    "            return\n",
    "\n",
    "        # --- \"Sleeping\" Logic State ---\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "        \n",
    "        # --- New Registration State ---\n",
    "        self.is_monitoring = False  # Are we actively monitoring a registered person?\n",
    "        self.registered_face_center_guess = None # The last known center of the monitored person\n",
    "        self.registered_person_name = None # Store the registered person's name\n",
    "        self.current_frame_for_registration = None # Temp stores frame for registration\n",
    "        self.registration_lock = threading.Lock() # Lock for accessing the frame\n",
    "\n",
    "        # --- GUI Elements ---\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "        # Video Display Label\n",
    "        self.video_label = tk.Label(self.main_frame, bg=\"black\")\n",
    "        self.video_label.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Status Label\n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.main_frame, textvariable=self.status_text, font=(\"Arial\", 14), pady=10)\n",
    "        self.status_label.pack()\n",
    "\n",
    "        # Control Buttons\n",
    "        self.button_frame = tk.Frame(self.main_frame)\n",
    "        self.button_frame.pack()\n",
    "\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15, state=tk.DISABLED)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # --- New Registration Buttons ---\n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.register_person, font=(\"Arial\", 12), width=18, state=tk.DISABLED)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear Registration\", command=self.clear_registration, font=(\"Arial\", 12), width=18, state=tk.DISABLED)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "\n",
    "        # Set up the close protocol\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        \"\"\"\n",
    "        Starts the video capture in a new thread.\n",
    "        \"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)  # Use 0 for laptop webcam\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            \n",
    "            self.is_running = True\n",
    "            \n",
    "            # Start the video processing thread\n",
    "            # daemon=True ensures the thread will close when the main app closes\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            \n",
    "            self.start_button.config(state=tk.DISABLED)\n",
    "            self.stop_button.config(state=tk.NORMAL)\n",
    "            self.register_button.config(state=tk.NORMAL) # Enable registration\n",
    "            self.status_text.set(\"Camera running. Click 'Register & Monitor'.\")\n",
    "        \n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap:\n",
    "                self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        \"\"\"\n",
    "        Signals the video loop to stop.\n",
    "        \"\"\"\n",
    "        self.is_running = False\n",
    "        \n",
    "        # The thread will see self.is_running is False and exit\n",
    "        # We wait a moment for the thread to finish\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "            \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "        self.start_button.config(state=tk.NORMAL)\n",
    "        self.stop_button.config(state=tk.DISABLED)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        self.video_label.config(image=None) # Clear the image\n",
    "        \n",
    "        # --- Reset all states ---\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None # Clear name\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "        self.register_button.config(state=tk.DISABLED)\n",
    "        self.clear_button.config(state=tk.DISABLED)\n",
    "\n",
    "    def video_loop(self):\n",
    "        \"\"\"\n",
    "        The main loop for video processing. Runs in a separate thread.\n",
    "        \"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                # Flip for a \"mirror\" view, which is more intuitive\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Store a copy of the frame for the registration function\n",
    "                with self.registration_lock:\n",
    "                    self.current_frame_for_registration = frame.copy()\n",
    "                \n",
    "                # Process the frame\n",
    "                processed_frame, status = self.process_frame_logic(frame)\n",
    "\n",
    "                # Update the status text\n",
    "                self.status_text.set(status)\n",
    "\n",
    "                # Convert the OpenCV (BGR) frame to a PIL (RGB) image\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                \n",
    "                # Resize image to fit the label (optional, but good for layout)\n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1: # Avoid division by zero on init\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "\n",
    "                # Convert PIL image to Tkinter-compatible image\n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "\n",
    "                # Update the video label in the GUI\n",
    "                # This must be done from the main thread, but tkinter seems to handle this call\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            # Control loop speed slightly\n",
    "            time.sleep(0.01) # ~100fps theoretical max, but processing will slow it down\n",
    "\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    def register_person(self):\n",
    "        \"\"\"\n",
    "        Registers the largest face currently in the frame for monitoring.\n",
    "        \"\"\"\n",
    "        with self.registration_lock:\n",
    "            if self.current_frame_for_registration is None:\n",
    "                messagebox.showwarning(\"Registration Error\", \"Camera not ready. Please try again.\")\n",
    "                return\n",
    "            \n",
    "            # Use the stored frame to find a face\n",
    "            gray = cv2.cvtColor(self.current_frame_for_registration, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.1,\n",
    "                minNeighbors=5,\n",
    "                minSize=(50, 50)\n",
    "            )\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            messagebox.showwarning(\"Registration Error\", \"No person detected in the frame. Please face the camera and try again.\")\n",
    "            return\n",
    "\n",
    "        # Register the largest face\n",
    "        # We sort by area to be sure we get the main person\n",
    "        faces_by_area = sorted(faces, key=lambda f: f[2] * f[3], reverse=True)\n",
    "        (x, y, w, h) = faces_by_area[0]\n",
    "\n",
    "        center_x = x + w // 2\n",
    "        center_y = y + h // 2\n",
    "        \n",
    "        # --- NEW: Ask for the person's name ---\n",
    "        name = simpledialog.askstring(\"Register Person\", \"Enter the person's name:\", parent=self.window)\n",
    "        \n",
    "        if not name:\n",
    "            messagebox.showwarning(\"Registration Cancelled\", \"Registration was cancelled (no name provided).\")\n",
    "            return\n",
    "        \n",
    "        # --- Lock in the registration ---\n",
    "        self.is_monitoring = True\n",
    "        self.registered_person_name = name\n",
    "        self.registered_face_center_guess = (center_x, center_y)\n",
    "        self.last_face_center = (center_x, center_y) # Start stillness check from this point\n",
    "        self.stillness_counter = 0\n",
    "\n",
    "        # Update GUI\n",
    "        self.register_button.config(state=tk.DISABLED)\n",
    "        self.clear_button.config(state=tk.NORMAL)\n",
    "        self.status_text.set(f\"Person registered: {self.registered_person_name}. Actively monitoring.\")\n",
    "        messagebox.showinfo(\"Registration Complete\", f\"{self.registered_person_name} registered. Monitoring has started.\")\n",
    "\n",
    "    def clear_registration(self):\n",
    "        \"\"\"\n",
    "        Clears the current registration and stops monitoring.\n",
    "        \"\"\"\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None # Clear name\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "\n",
    "        # Update GUI\n",
    "        if self.is_running: # Only enable if camera is on\n",
    "            self.register_button.config(state=tk.NORMAL)\n",
    "        self.clear_button.config(state=tk.DISABLED)\n",
    "        self.status_text.set(\"Monitoring stopped. Ready to register a new person.\")\n",
    "\n",
    "\n",
    "    def process_frame_logic(self, frame):\n",
    "        \"\"\"\n",
    "        Detects faces and applies monitoring logic based on registration status.\n",
    "        This function replaces the old 'detect_sleeping'.\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(50, 50) # Don't detect tiny faces\n",
    "        )\n",
    "\n",
    "        # If not monitoring, just show all faces found\n",
    "        if not self.is_monitoring:\n",
    "            current_status = \"Ready to register.\"\n",
    "            if self.is_running: # Check if camera is on\n",
    "                current_status = \"Click 'Register & Monitor' to begin.\"\n",
    "            \n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2) # Light blue box\n",
    "            \n",
    "            return frame, current_status\n",
    "\n",
    "        # --- If we ARE monitoring ---\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            # We were monitoring, but now the person is gone\n",
    "            current_status = f\"MONITORING: {self.registered_person_name} lost!\"\n",
    "            self.last_face_center = None # Stop stillness counter\n",
    "            self.stillness_counter = 0\n",
    "            return frame, current_status\n",
    "\n",
    "        # Find the face closest to our last known center\n",
    "        min_dist = float('inf')\n",
    "        tracked_face = None\n",
    "        current_center = None\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            center = (x + w // 2, y + h // 2)\n",
    "            dist = np.linalg.norm(np.array(center) - np.array(self.registered_face_center_guess))\n",
    "            \n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                tracked_face = (x, y, w, h)\n",
    "                current_center = center\n",
    "        \n",
    "        # Check if the closest face is \"reasonably\" close.\n",
    "        # If not, it's probably a different person.\n",
    "        if min_dist > MAX_TRACKING_JUMP_PX:\n",
    "             # The person we found is too far from the last spot.\n",
    "             current_status = f\"MONITORING: {self.registered_person_name} lost! (New face detected)\"\n",
    "             self.last_face_center = None\n",
    "             self.stillness_counter = 0\n",
    "             # Draw a box on the *new* face\n",
    "             (x, y, w, h) = tracked_face\n",
    "             cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 165, 255), 2) # Orange box\n",
    "             return frame, current_status\n",
    "\n",
    "        # --- We have successfully re-acquired the tracked face ---\n",
    "        (x, y, w, h) = tracked_face\n",
    "        \n",
    "        # Update our guess for the next frame\n",
    "        self.registered_face_center_guess = current_center\n",
    "        \n",
    "        current_status = f\"Monitoring: {self.registered_person_name} (Active)\"\n",
    "        alert_triggered = False\n",
    "\n",
    "        # Compare with the last known center\n",
    "        if self.last_face_center is not None:\n",
    "            dist_moved = np.linalg.norm(np.array(current_center) - np.array(self.last_face_center))\n",
    "\n",
    "            if dist_moved < STILLNESS_THRESHOLD_PX:\n",
    "                # Person is still\n",
    "                self.stillness_counter += 1\n",
    "            else:\n",
    "                # Person moved, reset counter\n",
    "                self.stillness_counter = 0\n",
    "        \n",
    "        # Update the last known center for the *next* frame's comparison\n",
    "        self.last_face_center = current_center\n",
    "\n",
    "        # Check if the stillness has crossed the alarm threshold\n",
    "        if self.stillness_counter > SLEEP_ALARM_FRAMES:\n",
    "            current_status = f\"!!! ALERT: {self.registered_person_name} IS SLEEPING !!!\"\n",
    "            alert_triggered = True\n",
    "        \n",
    "        # --- Draw on the frame ---\n",
    "        if alert_triggered:\n",
    "            # Draw a bright red box for alert\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "            cv2.putText(frame, f\"ALERT: {self.registered_person_name}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "        else:\n",
    "            # Draw a standard blue box for the tracked person\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, self.registered_person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f\"Stillness: {self.stillness_counter}/{SLEEP_ALARM_FRAMES}\", (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "        return frame, current_status\n",
    "\n",
    "\n",
    "    def detect_sleeping(self, frame):\n",
    "        \"\"\"\n",
    "        This function is no longer called directly by the video_loop.\n",
    "        Its logic has been moved into process_frame_logic.\n",
    "        \"\"\"\n",
    "        pass # Kept to show what was replaced\n",
    "        \n",
    "\n",
    "    def on_closing(self):\n",
    "        \"\"\"\n",
    "        Handles the window close event.\n",
    "        \"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01c3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3114",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
