{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Smart ATM Guardian - Foundational Architecture\n",
    "An expert-level implementation for real-time guard monitoring.\n",
    "\n",
    "Dependencies:\n",
    "pip install PyQt6 opencv-python mediapipe playsound\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from datetime import datetime\n",
    "from playsound import playsound\n",
    "import threading\n",
    "\n",
    "# --- PyQt6 Imports ---\n",
    "# from PyQt6.QtWidgets import (\n",
    "#     QApplication, QWidget, QLabel, QPushButton, \n",
    "#     QVBoxLayout, QHBoxLayout, QTextEdit\n",
    "# )\n",
    "# from PyQt6.QtCore import QThread, pyqtSignal, Qt\n",
    "# from PyQt6.QtGui import QImage, QPixmap, QColor, QFont\n",
    "\n",
    "# --- PyQt6 Imports ---\n",
    "from PyQt6.QtWidgets import (\n",
    "    QApplication, QWidget, QLabel, QPushButton, \n",
    "    QVBoxLayout, QHBoxLayout, QTextEdit\n",
    ")\n",
    "from PyQt6.QtCore import QThread, pyqtSignal, pyqtSlot, Qt  # <-- pyqtSlot added here\n",
    "from PyQt6.QtGui import QImage, QPixmap, QColor, QFont\n",
    "\n",
    "# === Configuration Constants ===\n",
    "\n",
    "# --- Video Source ---\n",
    "# Use 0 for webcam, or provide an RTSP stream URL\n",
    "VIDEO_SOURCE = 0 \n",
    "# e.g., \"rtsp://admin:password@192.168.1.108:554/cam/realmonitor?channel=1&subtype=0\"\n",
    "\n",
    "# --- Region of Interest (ROI) ---\n",
    "# Define the \"Watchman's Post\" [x1, y1, x2, y2]\n",
    "# If None, the whole frame is used.\n",
    "ROI_BOX = [100, 50, 500, 450] # Example: (x1, y1, x2, y2)\n",
    "\n",
    "# --- Detection Thresholds (in seconds) ---\n",
    "EYES_CLOSED_DURATION = 3.0    # Drowsiness: Eyes closed for 3s\n",
    "HEAD_SLUMP_DURATION = 10.0    # Drowsiness: Head slumped for 10s\n",
    "INATTENTIVE_DURATION = 60.0   # Inattentiveness: Head turned for 60s\n",
    "ABSENCE_DURATION = 120.0  # Absence: Guard missing for 2 mins\n",
    "\n",
    "# --- Alert Escalation Timers (in seconds) ---\n",
    "TIER_1_TO_TIER_2 = 15.0  # Time from Warning (Tier 1) to Alarm (Tier 2)\n",
    "TIER_2_TO_TIER_3 = 30.0  # Time from Alarm (Tier 2) to Escalation (Tier 3)\n",
    "\n",
    "# --- \"Smart\" Detection Logic Thresholds ---\n",
    "EAR_THRESHOLD = 0.21              # Eye Aspect Ratio threshold for closed eyes\n",
    "HEAD_SLUMP_THRESHOLD = 0.15       # Vertical distance (nose to shoulder-mid) as % of shoulder width\n",
    "HEAD_TURN_THRESHOLD_X = 0.25      # Horizontal distance (nose to shoulder-mid) as % of shoulder width\n",
    "HEAD_TURN_THRESHOLD_Z = -0.4      # Nose Z-landmark (depth) for looking down (e.g., at phone)\n",
    "PRESENCE_CONFIDENCE = 0.6         # MediaPipe Pose confidence to be considered 'present'\n",
    "\n",
    "# --- MediaPipe Imports ---\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# --- Sound Files (Placeholders) ---\n",
    "# Ensure these .wav files are in the same directory or provide full paths\n",
    "SOUND_TIER_1 = 'gentle_beep.wav'\n",
    "SOUND_TIER_2 = 'wake_up_alarm.wav'\n",
    "SOUND_TIER_3 = 'supervisor_alert.wav' # For escalation\n",
    "\n",
    "# Indices for MediaPipe Face Mesh (for EAR calculation)\n",
    "# (Left Eye, Right Eye)\n",
    "EYE_INDICES = {\n",
    "    \"LEFT\": [362, 385, 387, 263, 373, 380],\n",
    "    \"RIGHT\": [33, 160, 158, 133, 153, 144]\n",
    "}\n",
    "\n",
    "# Indices for MediaPipe Pose (for posture analysis)\n",
    "POSE_INDICES = {\n",
    "    \"NOSE\": mp_pose.PoseLandmark.NOSE,\n",
    "    \"LEFT_SHOULDER\": mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "    \"RIGHT_SHOULDER\": mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "}\n",
    "\n",
    "\n",
    "class VideoThread(QThread):\n",
    "    \"\"\"\n",
    "    Worker thread for handling OpenCV video capture and MediaPipe processing.\n",
    "    This prevents the main GUI thread from freezing.\n",
    "    \"\"\"\n",
    "    # --- Signals to Main Thread ---\n",
    "    # Emits the processed video frame\n",
    "    change_pixmap_signal = pyqtSignal(np.ndarray)\n",
    "    # Emits the status text and background color\n",
    "    update_status_signal = pyqtSignal(str, str)\n",
    "    # Emits a new event log message\n",
    "    log_event_signal = pyqtSignal(str)\n",
    "    # Emits an alert tier to trigger sound\n",
    "    trigger_alert_signal = pyqtSignal(int)\n",
    "\n",
    "    def __init__(self, video_source):\n",
    "        super().__init__()\n",
    "        self.video_source = video_source\n",
    "        self._is_running = True\n",
    "\n",
    "        # --- State Management Variables ---\n",
    "        self.current_state = \"ATTENTIVE\" # ATTENTIVE, DROWSY, INATTENTIVE, ABSENT\n",
    "        self.alert_level = 0             # 0: None, 1: Warning, 2: Alarm, 3: Escalation\n",
    "\n",
    "        # --- Timers for State Detection ---\n",
    "        # These track *how long* a condition has been true\n",
    "        self.eyes_closed_start_time = None\n",
    "        self.head_slump_start_time = None\n",
    "        self.head_turned_start_time = None\n",
    "        self.absence_start_time = None\n",
    "        \n",
    "        # --- Timers for Alert Escalation ---\n",
    "        # These track *how long* an alert has been active\n",
    "        self.tier_1_start_time = None\n",
    "        self.tier_2_start_time = None\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main processing loop\"\"\"\n",
    "        \n",
    "        # Initialize MediaPipe models\n",
    "        with mp_pose.Pose(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as pose, \\\n",
    "             mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True, # Gets iris and more detailed eye landmarks\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as face_mesh:\n",
    "\n",
    "            cap = cv2.VideoCapture(self.video_source)\n",
    "            if not cap.isOpened():\n",
    "                self.log_event_signal.emit(f\"Error: Could not open video source '{self.video_source}'\")\n",
    "                return\n",
    "\n",
    "            while self._is_running:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    self.log_event_signal.emit(\"Video feed ended or error.\")\n",
    "                    break\n",
    "                \n",
    "                # --- Core Detection Logic ---\n",
    "                frame, new_state = self.process_frame(frame, pose, face_mesh)\n",
    "                \n",
    "                # --- State Management and Alert FSM (Finite State Machine) ---\n",
    "                self.manage_state_and_alerts(new_state)\n",
    "\n",
    "                # --- Emit Frame ---\n",
    "                # Draw the ROI box on the frame\n",
    "                if ROI_BOX:\n",
    "                    cv2.rectangle(frame, (ROI_BOX[0], ROI_BOX[1]), (ROI_BOX[2], ROI_BOX[3]), (0, 255, 255), 2)\n",
    "                    cv2.putText(frame, \"Watchman's Post\", (ROI_BOX[0], ROI_BOX[1] - 10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "\n",
    "                self.change_pixmap_signal.emit(frame)\n",
    "\n",
    "            cap.release()\n",
    "            self.log_event_signal.emit(\"Monitoring stopped.\")\n",
    "\n",
    "    def process_frame(self, frame, pose, face_mesh):\n",
    "        \"\"\"Processes a single video frame for all detection logic.\"\"\"\n",
    "        \n",
    "        # Performance optimization\n",
    "        frame.flags.writeable = False\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process with MediaPipe\n",
    "        pose_results = pose.process(frame_rgb)\n",
    "        face_results = face_mesh.process(frame_rgb)\n",
    "        \n",
    "        frame.flags.writeable = True\n",
    "\n",
    "        # --- Detection Variables ---\n",
    "        is_present = False\n",
    "        is_eyes_closed = False\n",
    "        is_head_slumped = False\n",
    "        is_head_turned = False\n",
    "        \n",
    "        now = time.time()\n",
    "\n",
    "        # --- 1. Absence Detection ---\n",
    "        if pose_results.pose_landmarks:\n",
    "            lm_pose = pose_results.pose_landmarks.landmark\n",
    "            visibility = (lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]].visibility + \n",
    "                          lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]].visibility) / 2\n",
    "            \n",
    "            if visibility > PRESENCE_CONFIDENCE:\n",
    "                if ROI_BOX:\n",
    "                    # Check if person is inside the ROI\n",
    "                    is_present = self.check_in_roi(lm_pose, frame.shape[1], frame.shape[0])\n",
    "                else:\n",
    "                    # If no ROI, any detection counts as presence\n",
    "                    is_present = True\n",
    "\n",
    "        if not is_present:\n",
    "            # Draw landmarks if needed (even if outside ROI)\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "            \n",
    "            # Reset all other timers\n",
    "            self.eyes_closed_start_time = None\n",
    "            self.head_slump_start_time = None\n",
    "            self.head_turned_start_time = None\n",
    "            return frame, \"ABSENT\"\n",
    "\n",
    "        # --- If PRESENT, check Drowsiness and Inattentiveness ---\n",
    "        self.absence_start_time = None # Reset absence timer\n",
    "\n",
    "        # --- 2. Drowsiness Detection ---\n",
    "        # 2a. Eye Closure (EAR)\n",
    "        if face_results.multi_face_landmarks:\n",
    "            face_lm = face_results.multi_face_landmarks[0].landmark\n",
    "            ear = self.calculate_ear(face_lm)\n",
    "\n",
    "            if ear < EAR_THRESHOLD:\n",
    "                if self.eyes_closed_start_time is None:\n",
    "                    self.eyes_closed_start_time = now\n",
    "                elif now - self.eyes_closed_start_time > EYES_CLOSED_DURATION:\n",
    "                    is_eyes_closed = True\n",
    "            else:\n",
    "                self.eyes_closed_start_time = None\n",
    "            \n",
    "            # Draw face mesh\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=frame,\n",
    "                landmark_list=face_results.multi_face_landmarks[0],\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "        # 2b. Head Slump\n",
    "        lm_pose = pose_results.pose_landmarks.landmark\n",
    "        is_head_slumped = self.check_head_slump(lm_pose, frame.shape[1])\n",
    "        if is_head_slumped:\n",
    "            if self.head_slump_start_time is None:\n",
    "                self.head_slump_start_time = now\n",
    "            elif now - self.head_slump_start_time > HEAD_SLUMP_DURATION:\n",
    "                pass # This state will be handled by the logic below\n",
    "            else:\n",
    "                is_head_slumped = False # Not yet met duration\n",
    "        else:\n",
    "            self.head_slump_start_time = None\n",
    "\n",
    "        if is_eyes_closed or is_head_slumped:\n",
    "            self.head_turned_start_time = None # Drowsiness overrides inattentiveness\n",
    "            return frame, \"DROWSY\"\n",
    "\n",
    "        # --- 3. Inattentiveness Detection ---\n",
    "        is_head_turned = self.check_head_turn(lm_pose, frame.shape[1])\n",
    "        if is_head_turned:\n",
    "            if self.head_turned_start_time is None:\n",
    "                self.head_turned_start_time = now\n",
    "            elif now - self.head_turned_start_time > INATTENTIVE_DURATION:\n",
    "                pass # State will be handled\n",
    "            else:\n",
    "                is_head_turned = False # Not yet met duration\n",
    "        else:\n",
    "            self.head_turned_start_time = None\n",
    "\n",
    "        if is_head_turned:\n",
    "            return frame, \"INATTENTIVE\"\n",
    "\n",
    "        # --- 4. Attentive State ---\n",
    "        # If none of the above, the guard is attentive\n",
    "        self.eyes_closed_start_time = None\n",
    "        self.head_slump_start_time = None\n",
    "        self.head_turned_start_time = None\n",
    "        return frame, \"ATTENTIVE\"\n",
    "\n",
    "    # def manage_state_and_alerts(self, new_state):\n",
    "    #     \"\"\"Finite State Machine for managing alerts.\"\"\"\n",
    "    #     now = time.time()\n",
    "        \n",
    "    #     # If state changes, log it\n",
    "    #     if new_state != self.current_state:\n",
    "    #         self.log_event_signal.emit(f\"State Change: {self.current_state} -> {new_state}\")\n",
    "    #         self.current_state = new_state\n",
    "            \n",
    "    #         # Reset timers if moving to a non-alert state\n",
    "    #         if new_state == \"ATTENTIVE\":\n",
    "    #             self.tier_1_start_time = None\n",
    "    #             self.tier_2_start_time = None\n",
    "                \n",
    "    #         # If a new alert state is triggered, reset escalation\n",
    "    #         if new_state in [\"DROWSY\", \"INATTENTIVE\", \"ABSENT\"]:\n",
    "    #             self.tier_1_start_time = None\n",
    "    #             self.tier_2_start_time = None\n",
    "\n",
    "    def manage_state_and_alerts(self, new_state):\n",
    "        \"\"\"Finite State Machine for managing alerts.\"\"\"\n",
    "        now = time.time()\n",
    "                    \n",
    "        # If state changes, log it\n",
    "        if new_state != self.current_state:\n",
    "                self.log_event_signal.emit(f\"State Change: {self.current_state} -> {new_state}\")\n",
    "                self.current_state = new_state\n",
    "                        \n",
    "                # Reset timers only when moving to a non-alert state\n",
    "                if new_state == \"ATTENTIVE\":\n",
    "                    self.tier_1_start_time = None\n",
    "                    self.tier_2_start_time = None\n",
    "                        \n",
    "                        #\n",
    "                        # THE FAULTY BLOCK FROM LINES 309-311 IS NOW REMOVED\n",
    "                        #\n",
    "                        \n",
    "                    # --- Process Current State ---\n",
    "                    \n",
    "                # if self.current_state == \"ABSENT\":\n",
    "                #    if self.absence_start_time is None:\n",
    "                # ... (rest of the function is correct) ...\n",
    "\n",
    "\n",
    "        # --- Process Current State ---\n",
    "        \n",
    "        if self.current_state == \"ABSENT\":\n",
    "            if self.absence_start_time is None:\n",
    "                self.absence_start_time = now\n",
    "            \n",
    "            if now - self.absence_start_time > ABSENCE_DURATION:\n",
    "                if self.alert_level < 3:\n",
    "                    self.alert_level = 3\n",
    "                    self.update_status_signal.emit(\"ESCALATION: Guard Absent\", \"darkred\")\n",
    "                    self.log_event_signal.emit(\"Tier 3 Alert: Guard Absent. Notifying supervisor.\")\n",
    "                    self.trigger_alert_signal.emit(3)\n",
    "\n",
    "        elif self.current_state in [\"DROWSY\", \"INATTENTIVE\"]:\n",
    "            state_name = \"Drowsiness\" if self.current_state == \"DROWSY\" else \"Inattentiveness\"\n",
    "            \n",
    "            # Tier 1: Initial Warning\n",
    "            if self.alert_level == 0:\n",
    "                self.alert_level = 1\n",
    "                self.tier_1_start_time = now\n",
    "                self.update_status_signal.emit(f\"WARNING: {state_name} Detected\", \"orange\")\n",
    "                self.log_event_signal.emit(f\"Tier 1 Alert: {state_name} Warning\")\n",
    "                self.trigger_alert_signal.emit(1)\n",
    "            \n",
    "            # Tier 2: Escalation to Alarm\n",
    "            elif self.alert_level == 1 and (now - self.tier_1_start_time > TIER_1_TO_TIER_2):\n",
    "                self.alert_level = 2\n",
    "                self.tier_2_start_time = now\n",
    "                self.update_status_signal.emit(\"ALERT: WAKE UP\", \"red\")\n",
    "                self.log_event_signal.emit(f\"Tier 2 Alert: {state_name} Alarm\")\n",
    "                self.trigger_alert_signal.emit(2)\n",
    "\n",
    "            # Tier 3: Escalation to Supervisor\n",
    "            elif self.alert_level == 2 and (now - self.tier_2_start_time > TIER_2_TO_TIER_3):\n",
    "                self.alert_level = 3\n",
    "                self.update_status_signal.emit(\"ESCALATION: Unresponsive Guard\", \"darkred\")\n",
    "                self.log_event_signal.emit(\"Tier 3 Alert: Guard unresponsive. Notifying supervisor.\")\n",
    "                self.trigger_alert_signal.emit(3)\n",
    "        \n",
    "        elif self.current_state == \"ATTENTIVE\":\n",
    "            if self.alert_level != 0:\n",
    "                self.alert_level = 0\n",
    "                self.update_status_signal.emit(\"Attentive\", \"lightgreen\")\n",
    "                self.log_event_signal.emit(\"Status: Guard is Attentive. Alerts reset.\")\n",
    "\n",
    "    # --- Helper: Detection Logic Functions ---\n",
    "    \n",
    "    def check_in_roi(self, lm_pose, frame_w, frame_h):\n",
    "        \"\"\"Check if the person (shoulders) is inside the ROI.\"\"\"\n",
    "        ls = lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "        \n",
    "        # Calculate center of shoulders in pixel coordinates\n",
    "        center_x = ((ls.x + rs.x) / 2) * frame_w\n",
    "        center_y = ((ls.y + rs.y) / 2) * frame_h\n",
    "        \n",
    "        if (ROI_BOX[0] < center_x < ROI_BOX[2] and\n",
    "            ROI_BOX[1] < center_y < ROI_BOX[3]):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def calculate_ear(self, face_lm):\n",
    "        \"\"\"Calculates the Eye Aspect Ratio (EAR) for both eyes.\"\"\"\n",
    "        \n",
    "        # Calculate EAR for one eye (helper function)\n",
    "        def get_ear(eye_points_indices):\n",
    "            # Get landmark coordinates\n",
    "            points = [face_lm[i] for i in eye_points_indices]\n",
    "            \n",
    "            # Use 2D (x, y) coordinates\n",
    "            # Vertical distances\n",
    "            A = np.linalg.norm([points[1].x - points[5].x, points[1].y - points[5].y])\n",
    "            B = np.linalg.norm([points[2].x - points[4].x, points[2].y - points[4].y])\n",
    "            # Horizontal distance\n",
    "            C = np.linalg.norm([points[0].x - points[3].x, points[0].y - points[3].y])\n",
    "            \n",
    "            if C == 0: return 0.4 # Avoid division by zero, return a 'safe' open value\n",
    "            \n",
    "            # EAR formula\n",
    "            ear = (A + B) / (2.0 * C)\n",
    "            return ear\n",
    "\n",
    "        left_ear = get_ear(EYE_INDICES[\"LEFT\"])\n",
    "        right_ear = get_ear(EYE_INDICES[\"RIGHT\"])\n",
    "        \n",
    "        # Average EAR of both eyes\n",
    "        avg_ear = (left_ear + right_ear) / 2.0\n",
    "        return avg_ear\n",
    "        \n",
    "    def check_head_slump(self, lm_pose, frame_w):\n",
    "        \"\"\"Checks if the head is slumped down.\"\"\"\n",
    "        nose = lm_pose[POSE_INDICES[\"NOSE\"]]\n",
    "        ls = lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "\n",
    "        # Get Y-coordinate (vertical) of nose and shoulder midpoint\n",
    "        nose_y = nose.y\n",
    "        shoulder_mid_y = (ls.y + rs.y) / 2.0\n",
    "        \n",
    "        # Get shoulder width (in normalized coordinates) as a reference for scale\n",
    "        shoulder_width_norm = abs(ls.x - rs.x)\n",
    "        \n",
    "        # Check if nose is significantly *below* the shoulder midpoint\n",
    "        # This indicates a slumped head.\n",
    "        if nose_y > shoulder_mid_y + (shoulder_width_norm * HEAD_SLUMP_THRESHOLD):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_head_turn(self, lm_pose, frame_w):\n",
    "        \"\"\"Checks if the head is turned away (inattentive or on phone).\"\"\"\n",
    "        nose = lm_pose[POSE_INDICES[\"NOSE\"]]\n",
    "        ls = lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "\n",
    "        # 1. Check for 'looking down' (e.g., at a phone)\n",
    "        # The Z landmark indicates depth. A negative value is 'closer' to the camera.\n",
    "        # A very low value (e.g., < -0.4) often means the top of the head\n",
    "        # is tilted towards the camera (i.e., looking down).\n",
    "        if nose.z < HEAD_TURN_THRESHOLD_Z:\n",
    "            return True\n",
    "\n",
    "        # 2. Check for 'looking left/right'\n",
    "        shoulder_mid_x = (ls.x + rs.x) / 2.0\n",
    "        shoulder_width_norm = abs(ls.x - rs.x)\n",
    "        \n",
    "        # Calculate horizontal distance of nose from shoulder center\n",
    "        turn_distance = abs(nose.x - shoulder_mid_x)\n",
    "        \n",
    "        # If nose is horizontally offset by more than 25% of shoulder width\n",
    "        if turn_distance > (shoulder_width_norm * HEAD_TURN_THRESHOLD_X):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stops the thread.\"\"\"\n",
    "        self._is_running = False\n",
    "        self.log_event_signal.emit(\"Stop signal received. Shutting down thread...\")\n",
    "        self.wait() # Wait for the thread to finish\n",
    "\n",
    "\n",
    "class MainWindow(QWidget):\n",
    "    \"\"\"Main GUI Window\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Smart ATM Guardian\")\n",
    "        self.setGeometry(100, 100, 1000, 750)\n",
    "        \n",
    "        # --- GUI Widgets ---\n",
    "        self.video_label = QLabel(\"Connecting to video stream...\")\n",
    "        self.video_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        self.video_label.setMinimumSize(640, 480)\n",
    "        self.video_label.setStyleSheet(\"background-color: black; border: 2px solid #555;\")\n",
    "\n",
    "        self.status_label = QLabel(\"Status: IDLE\")\n",
    "        self.status_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        font = QFont(\"Arial\", 20, QFont.Weight.Bold)\n",
    "        self.status_label.setFont(font)\n",
    "        self.status_label.setStyleSheet(\"background-color: gray; color: white; padding: 10px;\")\n",
    "\n",
    "        self.start_stop_button = QPushButton(\"Start Monitoring\")\n",
    "        self.start_stop_button.setFont(QFont(\"Arial\", 12))\n",
    "        self.start_stop_button.setCheckable(True)\n",
    "        self.start_stop_button.setStyleSheet(\n",
    "            \"QPushButton { padding: 10px; background-color: #4CAF50; color: white; }\"\n",
    "            \"QPushButton:checked { background-color: #f44336; }\"\n",
    "        )\n",
    "\n",
    "        self.log_box_label = QLabel(\"Event Log:\")\n",
    "        self.log_box_label.setFont(QFont(\"Arial\", 10, QFont.Weight.Bold))\n",
    "        \n",
    "        self.log_box = QTextEdit()\n",
    "        self.log_box.setReadOnly(True)\n",
    "        self.log_box.setFont(QFont(\"Courier New\", 9))\n",
    "        self.log_box.setStyleSheet(\"background-color: #f0f0f0;\")\n",
    "\n",
    "        # --- Layouts ---\n",
    "        # Main vertical layout\n",
    "        v_layout = QVBoxLayout()\n",
    "        v_layout.addWidget(self.status_label)\n",
    "        \n",
    "        # Horizontal layout for video and logs\n",
    "        h_layout = QHBoxLayout()\n",
    "        h_layout.addWidget(self.video_label, 2) # Video takes 2/3 of space\n",
    "        \n",
    "        # Right-side panel for controls and logs\n",
    "        right_panel_layout = QVBoxLayout()\n",
    "        right_panel_layout.addWidget(self.start_stop_button)\n",
    "        right_panel_layout.addWidget(self.log_box_label)\n",
    "        right_panel_layout.addWidget(self.log_box)\n",
    "        \n",
    "        h_layout.addLayout(right_panel_layout, 1) # Logs/controls take 1/3 of space\n",
    "        \n",
    "        v_layout.addLayout(h_layout)\n",
    "        self.setLayout(v_layout)\n",
    "\n",
    "        # --- Connections ---\n",
    "        self.start_stop_button.clicked.connect(self.toggle_monitoring)\n",
    "        \n",
    "        self.video_thread = None\n",
    "        self.log_event(f\"Application Initialized. Ready to start monitoring.\")\n",
    "\n",
    "    def toggle_monitoring(self, checked):\n",
    "        if checked:\n",
    "            self.log_event(\"Starting monitoring...\")\n",
    "            self.video_thread = VideoThread(VIDEO_SOURCE)\n",
    "            \n",
    "            # Connect signals\n",
    "            self.video_thread.change_pixmap_signal.connect(self.update_image)\n",
    "            self.video_thread.update_status_signal.connect(self.update_status)\n",
    "            self.video_thread.log_event_signal.connect(self.log_event)\n",
    "            self.video_thread.trigger_alert_signal.connect(self.trigger_alert)\n",
    "            self.video_thread.finished.connect(self.thread_finished)\n",
    "            \n",
    "            self.video_thread.start()\n",
    "            self.start_stop_button.setText(\"Stop Monitoring\")\n",
    "        else:\n",
    "            if self.video_thread:\n",
    "                self.video_thread.stop()\n",
    "            self.start_stop_button.setText(\"Start Monitoring\")\n",
    "            self.video_label.setText(\"Monitoring Stopped.\")\n",
    "            self.video_label.setStyleSheet(\"background-color: black; color: white; border: 2px solid #555;\")\n",
    "            self.update_status(\"IDLE\", \"gray\")\n",
    "\n",
    "    def thread_finished(self):\n",
    "        \"\"\"Handles cleanup when the thread is done.\"\"\"\n",
    "        self.log_event(\"Video thread has finished.\")\n",
    "        self.start_stop_button.setChecked(False)\n",
    "        self.start_stop_button.setText(\"Start Monitoring\")\n",
    "        self.video_label.setText(\"Monitoring Stopped.\")\n",
    "        self.video_label.setStyleSheet(\"background-color: black; color: white; border: 2px solid #555;\")\n",
    "        self.update_status(\"IDLE\", \"gray\")\n",
    "\n",
    "    def closeEvent(self, event):\n",
    "        \"\"\"Ensure the thread is stopped when the window is closed.\"\"\"\n",
    "        self.log_event(\"Application closing...\")\n",
    "        if self.video_thread and self.video_thread.isRunning():\n",
    "            self.video_thread.stop()\n",
    "        event.accept()\n",
    "\n",
    "    # --- Slot Functions (receive signals from QThread) ---\n",
    "\n",
    "    @pyqtSlot(np.ndarray)\n",
    "    def update_image(self, cv_img):\n",
    "        \"\"\"Updates the video_label with a new frame from the thread.\"\"\"\n",
    "        qt_img = self.convert_cv_to_qt(cv_img)\n",
    "        self.video_label.setPixmap(qt_img)\n",
    "\n",
    "    @pyqtSlot(str, str)\n",
    "    def update_status(self, status_text, color):\n",
    "        \"\"\"Updates the status label text and color.\"\"\"\n",
    "        self.status_label.setText(f\"Status: {status_text}\")\n",
    "        \n",
    "        # Create flashing effect for alerts\n",
    "        if color == \"red\" or color == \"darkred\":\n",
    "            style = f\"\"\"\n",
    "                QLabel {{\n",
    "                    background-color: {color};\n",
    "                    color: white;\n",
    "                    padding: 10px;\n",
    "                    border: 3px solid yellow;\n",
    "                }}\n",
    "            \"\"\"\n",
    "            # Simple blink logic (could be improved with QTimer)\n",
    "            if int(time.time() * 2) % 2 == 0:\n",
    "                style = \"\"\"\n",
    "                    QLabel {\n",
    "                        background-color: yellow;\n",
    "                        color: black;\n",
    "                        padding: 10px;\n",
    "                        border: 3px solid red;\n",
    "                    }\n",
    "                \"\"\"\n",
    "            self.status_label.setStyleSheet(style)\n",
    "        else:\n",
    "            self.status_label.setStyleSheet(f\"background-color: {color}; color: black; padding: 10px;\")\n",
    "\n",
    "    @pyqtSlot(str)\n",
    "    def log_event(self, message):\n",
    "        \"\"\"Appends a new message to the event log box.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self.log_box.append(f\"[{timestamp}] {message}\")\n",
    "        self.log_box.verticalScrollBar().setValue(self.log_box.verticalScrollBar().maximum())\n",
    "\n",
    "    @pyqtSlot(int)\n",
    "    def trigger_alert(self, tier):\n",
    "        \"\"\"Triggers the alert sound in a separate thread to avoid GUI freeze.\"\"\"\n",
    "        if tier == 1:\n",
    "            sound_file = SOUND_TIER_1\n",
    "        elif tier == 2:\n",
    "            sound_file = SOUND_TIER_2\n",
    "        elif tier == 3:\n",
    "            sound_file = SOUND_TIER_3\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "        # Run playsound in a daemon thread so it doesn't block\n",
    "        threading.Thread(target=self.play_sound, args=(sound_file,), daemon=True).start()\n",
    "\n",
    "    def play_sound(self, sound_file):\n",
    "        \"\"\"Wrapper for playsound to catch exceptions.\"\"\"\n",
    "        try:\n",
    "            playsound(sound_file)\n",
    "        except Exception as e:\n",
    "            # Emit log message back to main thread (safer way)\n",
    "            # For simplicity here, we just print\n",
    "            print(f\"Error playing sound {sound_file}: {e}\")\n",
    "            # A more robust way would be to create another signal\n",
    "            # self.log_event_signal.emit(f\"Error playing sound: {e}\")\n",
    "\n",
    "    def convert_cv_to_qt(self, cv_img):\n",
    "        \"\"\"Converts an OpenCV image (BGR) to a QPixmap (RGB).\"\"\"\n",
    "        rgb_image = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = rgb_image.shape\n",
    "        bytes_per_line = ch * w\n",
    "        convert_to_Qt_format = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)\n",
    "        \n",
    "        # Scale image to fit the label while maintaining aspect ratio\n",
    "        scaled_pixmap = QPixmap.fromImage(convert_to_Qt_format).scaled(\n",
    "            self.video_label.size(), \n",
    "            Qt.AspectRatioMode.KeepAspectRatio, \n",
    "            Qt.TransformationMode.SmoothTransformation\n",
    "        )\n",
    "        return scaled_pixmap\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    main_window = MainWindow()\n",
    "    main_window.show()\n",
    "    sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18565b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Smart ATM Guardian - Version 2 (Revised and Stabilized)\n",
    "\n",
    "Dependencies:\n",
    "pip install PyQt6 opencv-python mediapipe\n",
    "(PyQt6 automatically includes QtMultimedia for sound)\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from datetime import datetime\n",
    "import os # For sound file paths\n",
    "\n",
    "# --- PyQt6 Imports ---\n",
    "from PyQt6.QtWidgets import (\n",
    "    QApplication, QWidget, QLabel, QPushButton, \n",
    "    QVBoxLayout, QHBoxLayout, QTextEdit\n",
    ")\n",
    "from PyQt6.QtCore import QThread, pyqtSignal, pyqtSlot, Qt, QTimer, QUrl\n",
    "from PyQt6.QtGui import QImage, QPixmap, QColor, QFont\n",
    "from PyQt6.QtMultimedia import QMediaPlayer, QAudioOutput # <-- Robust sound\n",
    "\n",
    "# === Configuration Constants ===\n",
    "\n",
    "# --- Video Source ---\n",
    "VIDEO_SOURCE = 0 \n",
    "\n",
    "# --- Region of Interest (ROI) ---\n",
    "ROI_BOX = None # [100, 50, 500, 450] # Example: (x1, y1, x2, y2)\n",
    "\n",
    "# --- Detection Thresholds (in seconds) ---\n",
    "EYES_CLOSED_DURATION = 3.0    # Drowsiness: Eyes closed for 3s\n",
    "HEAD_SLUMP_DURATION = 10.0    # Drowsiness: Head slumped for 10s\n",
    "INATTENTIVE_DURATION = 60.0   # Inattentiveness: Head turned for 60s\n",
    "ABSENCE_DURATION = 120.0  # Absence: Guard missing for 2 mins\n",
    "\n",
    "# --- Alert Escalation Timers (in seconds) ---\n",
    "TIER_1_TO_TIER_2 = 15.0  # Time from Warning (Tier 1) to Alarm (Tier 2)\n",
    "TIER_2_TO_TIER_3 = 30.0  # Time from Alarm (Tier 2) to Escalation (Tier 3)\n",
    "\n",
    "# --- \"Smart\" Detection Logic Thresholds ---\n",
    "# !!! TUNE THESE VALUES FOR YOUR CAMERA/ENVIRONMENT !!!\n",
    "EAR_THRESHOLD = 0.21              # Eye Aspect Ratio threshold\n",
    "HEAD_SLUMP_THRESHOLD = 0.15       # Vertical distance (nose to shoulder-mid) as % of shoulder width\n",
    "HEAD_TURN_THRESHOLD_X = 0.25      # Horizontal distance (nose to shoulder-mid) as % of shoulder width\n",
    "PRESENCE_CONFIDENCE = 0.6         # MediaPipe Pose confidence\n",
    "\n",
    "# --- MediaPipe Imports ---\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# --- Sound Files (Placeholders) ---\n",
    "# Create these files or use your own .wav or .mp3\n",
    "SOUND_TIER_1_FILE = 'gentle_beep.wav'\n",
    "SOUND_TIER_2_FILE = 'wake_up_alarm.wav'\n",
    "SOUND_TIER_3_FILE = 'supervisor_alert.wav'\n",
    "\n",
    "# Indices for MediaPipe Face Mesh (for EAR calculation)\n",
    "EYE_INDICES = {\n",
    "    \"LEFT\": [362, 385, 387, 263, 373, 380],\n",
    "    \"RIGHT\": [33, 160, 158, 133, 153, 144]\n",
    "}\n",
    "\n",
    "# Indices for MediaPipe Pose (for posture analysis)\n",
    "POSE_INDICES = {\n",
    "    \"NOSE\": mp_pose.PoseLandmark.NOSE,\n",
    "    \"LEFT_SHOULDER\": mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "    \"RIGHT_SHOULDER\": mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "}\n",
    "\n",
    "\n",
    "class VideoThread(QThread):\n",
    "    \"\"\"\n",
    "    Worker thread for handling OpenCV video capture and MediaPipe processing.\n",
    "    \"\"\"\n",
    "    change_pixmap_signal = pyqtSignal(np.ndarray)\n",
    "    update_status_signal = pyqtSignal(str, str)\n",
    "    log_event_signal = pyqtSignal(str)\n",
    "    trigger_alert_signal = pyqtSignal(int)\n",
    "\n",
    "    def __init__(self, video_source):\n",
    "        super().__init__()\n",
    "        self.video_source = video_source\n",
    "        self._is_running = True\n",
    "\n",
    "        # --- State Management Variables ---\n",
    "        self.current_state = \"ATTENTIVE\"\n",
    "        self.alert_level = 0\n",
    "\n",
    "        # --- Timers for State Detection ---\n",
    "        self.eyes_closed_start_time = None\n",
    "        self.head_slump_start_time = None\n",
    "        self.head_turned_start_time = None\n",
    "        self.absence_start_time = None\n",
    "        \n",
    "        # --- Timers for Alert Escalation ---\n",
    "        self.tier_1_start_time = None\n",
    "        self.tier_2_start_time = None\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main processing loop\"\"\"\n",
    "        with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose, \\\n",
    "             mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "\n",
    "            cap = cv2.VideoCapture(self.video_source)\n",
    "            if not cap.isOpened():\n",
    "                self.log_event_signal.emit(f\"Error: Could not open video source '{self.video_source}'\")\n",
    "                return\n",
    "\n",
    "            while self._is_running:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    self.log_event_signal.emit(\"Video feed ended or error.\")\n",
    "                    break\n",
    "                \n",
    "                frame, new_state = self.process_frame(frame, pose, face_mesh)\n",
    "                self.manage_state_and_alerts(new_state)\n",
    "\n",
    "                if ROI_BOX:\n",
    "                    cv2.rectangle(frame, (ROI_BOX[0], ROI_BOX[1]), (ROI_BOX[2], ROI_BOX[3]), (0, 255, 255), 2)\n",
    "\n",
    "                self.change_pixmap_signal.emit(frame)\n",
    "\n",
    "            cap.release()\n",
    "            self.log_event_signal.emit(\"Monitoring stopped.\")\n",
    "\n",
    "    def process_frame(self, frame, pose, face_mesh):\n",
    "        \"\"\"Processes a single video frame for all detection logic.\"\"\"\n",
    "        \n",
    "        frame.flags.writeable = False\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        pose_results = pose.process(frame_rgb)\n",
    "        face_results = face_mesh.process(frame_rgb)\n",
    "        \n",
    "        frame.flags.writeable = True\n",
    "\n",
    "        is_present = False\n",
    "        is_eyes_closed = False\n",
    "        is_head_slumped = False\n",
    "        is_head_turned = False\n",
    "        \n",
    "        now = time.time()\n",
    "\n",
    "        # --- 1. Absence Detection ---\n",
    "        if pose_results.pose_landmarks:\n",
    "            lm_pose = pose_results.pose_landmarks.landmark\n",
    "            visibility = (lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]].visibility + \n",
    "                          lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]].visibility) / 2\n",
    "            \n",
    "            if visibility > PRESENCE_CONFIDENCE:\n",
    "                if ROI_BOX:\n",
    "                    is_present = self.check_in_roi(lm_pose, frame.shape[1], frame.shape[0])\n",
    "                else:\n",
    "                    is_present = True\n",
    "\n",
    "        if not is_present:\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "            \n",
    "            self.eyes_closed_start_time = None\n",
    "            self.head_slump_start_time = None\n",
    "            self.head_turned_start_time = None\n",
    "            return frame, \"ABSENT\"\n",
    "\n",
    "        # --- If PRESENT, check Drowsiness and Inattentiveness ---\n",
    "        self.absence_start_time = None \n",
    "\n",
    "        # --- 2. Drowsiness Detection ---\n",
    "        ear = 0.5 # Default to 'eyes open' if no face detected\n",
    "        if face_results.multi_face_landmarks:\n",
    "            face_lm = face_results.multi_face_landmarks[0].landmark\n",
    "            ear = self.calculate_ear(face_lm)\n",
    "\n",
    "            if ear < EAR_THRESHOLD:\n",
    "                if self.eyes_closed_start_time is None:\n",
    "                    self.eyes_closed_start_time = now\n",
    "                elif now - self.eyes_closed_start_time > EYES_CLOSED_DURATION:\n",
    "                    is_eyes_closed = True\n",
    "            else:\n",
    "                self.eyes_closed_start_time = None\n",
    "            \n",
    "            # --- For Tuning: Display the EAR value ---\n",
    "            cv2.putText(frame, f\"EAR: {ear:.2f}\", (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=frame,\n",
    "                landmark_list=face_results.multi_face_landmarks[0],\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "        lm_pose = pose_results.pose_landmarks.landmark\n",
    "        is_head_slumped = self.check_head_slump(lm_pose)\n",
    "        if is_head_slumped:\n",
    "            if self.head_slump_start_time is None:\n",
    "                self.head_slump_start_time = now\n",
    "            elif now - self.head_slump_start_time > HEAD_SLUMP_DURATION:\n",
    "                pass \n",
    "            else:\n",
    "                is_head_slumped = False\n",
    "        else:\n",
    "            self.head_slump_start_time = None\n",
    "\n",
    "        if is_eyes_closed or is_head_slumped:\n",
    "            self.head_turned_start_time = None \n",
    "            return frame, \"DROWSY\"\n",
    "\n",
    "        # --- 3. Inattentiveness Detection ---\n",
    "        is_head_turned = self.check_head_turn(lm_pose)\n",
    "        if is_head_turned:\n",
    "            if self.head_turned_start_time is None:\n",
    "                self.head_turned_start_time = now\n",
    "            elif now - self.head_turned_start_time > INATTENTIVE_DURATION:\n",
    "                pass\n",
    "            else:\n",
    "                is_head_turned = False\n",
    "        else:\n",
    "            self.head_turned_start_time = None\n",
    "\n",
    "        if is_head_turned:\n",
    "            return frame, \"INATTENTIVE\"\n",
    "\n",
    "        # --- 4. Attentive State ---\n",
    "        self.eyes_closed_start_time = None\n",
    "        self.head_slump_start_time = None\n",
    "        self.head_turned_start_time = None\n",
    "        return frame, \"ATTENTIVE\"\n",
    "\n",
    "    def manage_state_and_alerts(self, new_state):\n",
    "        \"\"\"Finite State Machine for managing alerts.\"\"\"\n",
    "        now = time.time()\n",
    "        \n",
    "        if new_state != self.current_state:\n",
    "            self.log_event_signal.emit(f\"State Change: {self.current_state} -> {new_state}\")\n",
    "            self.current_state = new_state\n",
    "            \n",
    "            if new_state == \"ATTENTIVE\":\n",
    "                self.tier_1_start_time = None\n",
    "                self.tier_2_start_time = None\n",
    "                self.alert_level = 0 # Reset alert level\n",
    "                self.update_status_signal.emit(\"Attentive\", \"lightgreen\")\n",
    "            # *** BUG FIX ***: Do NOT reset timers here if moving to another alert state\n",
    "            # This was the cause of the NoneType error.\n",
    "\n",
    "        # --- Process Current State ---\n",
    "        \n",
    "        if self.current_state == \"ABSENT\":\n",
    "            if self.absence_start_time is None:\n",
    "                self.absence_start_time = now\n",
    "            \n",
    "            if now - self.absence_start_time > ABSENCE_DURATION:\n",
    "                if self.alert_level < 3:\n",
    "                    self.alert_level = 3\n",
    "                    self.update_status_signal.emit(\"ESCALATION: Guard Absent\", \"darkred\")\n",
    "                    self.log_event_signal.emit(\"Tier 3 Alert: Guard Absent.\")\n",
    "                    self.trigger_alert_signal.emit(3)\n",
    "\n",
    "        elif self.current_state in [\"DROWSY\", \"INATTENTIVE\"]:\n",
    "            state_name = \"Drowsiness\" if self.current_state == \"DROWSY\" else \"Inattentiveness\"\n",
    "            \n",
    "            # Tier 1: Initial Warning\n",
    "            if self.alert_level == 0:\n",
    "                self.alert_level = 1\n",
    "                self.tier_1_start_time = now # Set timer on first detection\n",
    "                self.update_status_signal.emit(f\"WARNING: {state_name} Detected\", \"orange\")\n",
    "                self.log_event_signal.emit(f\"Tier 1 Alert: {state_name} Warning\")\n",
    "                self.trigger_alert_signal.emit(1)\n",
    "            \n",
    "            # Tier 2: Escalation to Alarm\n",
    "            # *** BUG FIX ***: Check if tier_1_start_time is not None before subtracting\n",
    "            elif self.alert_level == 1 and self.tier_1_start_time and (now - self.tier_1_start_time > TIER_1_TO_TIER_2):\n",
    "                self.alert_level = 2\n",
    "                self.tier_2_start_time = now\n",
    "                self.update_status_signal.emit(\"ALERT: WAKE UP\", \"red\")\n",
    "                self.log_event_signal.emit(f\"Tier 2 Alert: {state_name} Alarm\")\n",
    "                self.trigger_alert_signal.emit(2)\n",
    "\n",
    "            # Tier 3: Escalation to Supervisor\n",
    "            elif self.alert_level == 2 and self.tier_2_start_time and (now - self.tier_2_start_time > TIER_2_TO_TIER_3):\n",
    "                self.alert_level = 3\n",
    "                self.update_status_signal.emit(\"ESCALATION: Unresponsive Guard\", \"darkred\")\n",
    "                self.log_event_signal.emit(\"Tier 3 Alert: Guard unresponsive.\")\n",
    "                self.trigger_alert_signal.emit(3)\n",
    "        \n",
    "        elif self.current_state == \"ATTENTIVE\":\n",
    "            if self.alert_level != 0:\n",
    "                self.alert_level = 0\n",
    "                self.update_status_signal.emit(\"Attentive\", \"lightgreen\")\n",
    "                self.log_event_signal.emit(\"Status: Guard is Attentive. Alerts reset.\")\n",
    "\n",
    "    # --- Helper: Detection Logic Functions ---\n",
    "    \n",
    "    def check_in_roi(self, lm_pose, frame_w, frame_h):\n",
    "        ls = lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "        center_x = ((ls.x + rs.x) / 2) * frame_w\n",
    "        center_y = ((ls.y + rs.y) / 2) * frame_h\n",
    "        return (ROI_BOX[0] < center_x < ROI_BOX[2] and ROI_BOX[1] < center_y < ROI_BOX[3])\n",
    "\n",
    "    def calculate_ear(self, face_lm):\n",
    "        def get_ear(eye_points_indices):\n",
    "            points = [face_lm[i] for i in eye_points_indices]\n",
    "            A = np.linalg.norm([points[1].x - points[5].x, points[1].y - points[5].y])\n",
    "            B = np.linalg.norm([points[2].x - points[4].x, points[2].y - points[4].y])\n",
    "            C = np.linalg.norm([points[0].x - points[3].x, points[0].y - points[3].y])\n",
    "            return (A + B) / (2.0 * C) if C > 0 else 0.4\n",
    "\n",
    "        return (get_ear(EYE_INDICES[\"LEFT\"]) + get_ear(EYE_INDICES[\"RIGHT\"])) / 2.0\n",
    "        \n",
    "    def check_head_slump(self, lm_pose):\n",
    "        \"\"\"Checks if the head is slumped down (for sleep or phone).\"\"\"\n",
    "        nose = lm_pose[POSE_INDICES[\"NOSE\"]]\n",
    "        ls = lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "\n",
    "        if nose.visibility < 0.5 or ls.visibility < 0.5 or rs.visibility < 0.5:\n",
    "            return False # Not confident in landmarks\n",
    "\n",
    "        nose_y = nose.y\n",
    "        shoulder_mid_y = (ls.y + rs.y) / 2.0\n",
    "        shoulder_width_norm = abs(ls.x - rs.x)\n",
    "        \n",
    "        if shoulder_width_norm == 0:\n",
    "            return False\n",
    "\n",
    "        # If nose is below shoulder midpoint by a threshold\n",
    "        return nose_y > shoulder_mid_y + (shoulder_width_norm * HEAD_SLUMP_THRESHOLD)\n",
    "\n",
    "    def check_head_turn(self, lm_pose):\n",
    "        \"\"\"Checks if the head is turned away (left/right).\"\"\"\n",
    "        nose = lm_pose[POSE_INDICES[\"NOSE\"]]\n",
    "        ls = lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "\n",
    "        if nose.visibility < 0.5 or ls.visibility < 0.5 or rs.visibility < 0.5:\n",
    "            return False\n",
    "\n",
    "        shoulder_mid_x = (ls.x + rs.x) / 2.0\n",
    "        shoulder_width_norm = abs(ls.x - rs.x)\n",
    "\n",
    "        if shoulder_width_norm == 0:\n",
    "            return False\n",
    "            \n",
    "        turn_distance = abs(nose.x - shoulder_mid_x)\n",
    "        \n",
    "        # If nose is horizontally offset\n",
    "        return turn_distance > (shoulder_width_norm * HEAD_TURN_THRESHOLD_X)\n",
    "\n",
    "    def stop(self):\n",
    "        self._is_running = False\n",
    "        self.log_event_signal.emit(\"Stop signal received. Shutting down thread...\")\n",
    "        self.wait()\n",
    "\n",
    "\n",
    "class MainWindow(QWidget):\n",
    "    \"\"\"Main GUI Window\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Smart ATM Guardian (v2 - Stabilized)\")\n",
    "        self.setGeometry(100, 100, 1000, 750)\n",
    "        \n",
    "        # --- GUI Widgets ---\n",
    "        self.video_label = QLabel(\"Connecting to video stream...\")\n",
    "        self.video_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        self.video_label.setMinimumSize(640, 480)\n",
    "        self.video_label.setStyleSheet(\"background-color: black; border: 2px solid #555;\")\n",
    "\n",
    "        self.status_label = QLabel(\"Status: IDLE\")\n",
    "        self.status_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        font = QFont(\"Arial\", 20, QFont.Weight.Bold)\n",
    "        self.status_label.setFont(font)\n",
    "        self.update_status(\"IDLE\", \"gray\") # Initial style\n",
    "\n",
    "        self.start_stop_button = QPushButton(\"Start Monitoring\")\n",
    "        self.start_stop_button.setFont(QFont(\"Arial\", 12))\n",
    "        self.start_stop_button.setCheckable(True)\n",
    "        self.start_stop_button.setStyleSheet(\n",
    "            \"QPushButton { padding: 10px; background-color: #4CAF50; color: white; border: none; }\"\n",
    "            \"QPushButton:checked { background-color: #f44336; }\"\n",
    "        )\n",
    "\n",
    "        self.log_box = QTextEdit()\n",
    "        self.log_box.setReadOnly(True)\n",
    "        self.log_box.setFont(QFont(\"Courier New\", 9))\n",
    "\n",
    "        # --- Layouts ---\n",
    "        v_layout = QVBoxLayout()\n",
    "        v_layout.addWidget(self.status_label)\n",
    "        h_layout = QHBoxLayout()\n",
    "        h_layout.addWidget(self.video_label, 2)\n",
    "        right_panel_layout = QVBoxLayout()\n",
    "        right_panel_layout.addWidget(self.start_stop_button)\n",
    "        right_panel_layout.addWidget(QLabel(\"Event Log:\"))\n",
    "        right_panel_layout.addWidget(self.log_box)\n",
    "        h_layout.addLayout(right_panel_layout, 1)\n",
    "        v_layout.addLayout(h_layout)\n",
    "        self.setLayout(v_layout)\n",
    "\n",
    "        # --- Sound Player ---\n",
    "        self.player = QMediaPlayer()\n",
    "        self.audio_output = QAudioOutput()\n",
    "        self.player.setAudioOutput(self.audio_output)\n",
    "        self.audio_output.setVolume(1.0) # Volume 0.0 to 1.0\n",
    "\n",
    "        # --- Flashing Timer ---\n",
    "        self.flash_timer = QTimer(self)\n",
    "        self.flash_timer.setInterval(500) # 500ms blink rate\n",
    "        self.flash_timer.timeout.connect(self.toggle_flash)\n",
    "        self.is_flashing = False\n",
    "        self.flash_colors = (\"red\", \"yellow\")\n",
    "        self.flash_index = 0\n",
    "\n",
    "        # --- Connections ---\n",
    "        self.start_stop_button.clicked.connect(self.toggle_monitoring)\n",
    "        self.video_thread = None\n",
    "        self.log_event(f\"Application Initialized. Ready to start monitoring.\")\n",
    "\n",
    "    def toggle_monitoring(self, checked):\n",
    "        if checked:\n",
    "            self.log_event(\"Starting monitoring...\")\n",
    "            self.video_thread = VideoThread(VIDEO_SOURCE)\n",
    "            self.video_thread.change_pixmap_signal.connect(self.update_image)\n",
    "            self.video_thread.update_status_signal.connect(self.update_status)\n",
    "            self.video_thread.log_event_signal.connect(self.log_event)\n",
    "            self.video_thread.trigger_alert_signal.connect(self.trigger_alert)\n",
    "            self.video_thread.finished.connect(self.thread_finished)\n",
    "            self.video_thread.start()\n",
    "            self.start_stop_button.setText(\"Stop Monitoring\")\n",
    "        else:\n",
    "            if self.video_thread:\n",
    "                self.video_thread.stop()\n",
    "\n",
    "    def thread_finished(self):\n",
    "        self.log_event(\"Video thread has finished.\")\n",
    "        self.start_stop_button.setChecked(False)\n",
    "        self.start_stop_button.setText(\"Start Monitoring\")\n",
    "        self.video_label.setText(\"Monitoring Stopped.\")\n",
    "        self.video_label.setStyleSheet(\"background-color: black; color: white; border: 2px solid #555;\")\n",
    "        self.update_status(\"IDLE\", \"gray\")\n",
    "\n",
    "    def closeEvent(self, event):\n",
    "        self.log_event(\"Application closing...\")\n",
    "        if self.video_thread and self.video_thread.isRunning():\n",
    "            self.video_thread.stop()\n",
    "        event.accept()\n",
    "\n",
    "    # --- Slot Functions ---\n",
    "\n",
    "    @pyqtSlot(np.ndarray)\n",
    "    def update_image(self, cv_img):\n",
    "        qt_img = self.convert_cv_to_qt(cv_img)\n",
    "        self.video_label.setPixmap(qt_img)\n",
    "\n",
    "    @pyqtSlot(str, str)\n",
    "    def update_status(self, status_text, color):\n",
    "        self.status_label.setText(f\"Status: {status_text}\")\n",
    "        \n",
    "        if color in (\"red\", \"darkred\"):\n",
    "            if not self.flash_timer.isActive():\n",
    "                self.is_flashing = True\n",
    "                self.flash_colors = (\"red\", \"yellow\") if color == \"red\" else (\"darkred\", \"white\")\n",
    "                self.flash_index = 0\n",
    "                self.flash_timer.start()\n",
    "                self.toggle_flash() # Start immediately\n",
    "        else:\n",
    "            self.is_flashing = False\n",
    "            self.flash_timer.stop()\n",
    "            text_color = \"black\" if color in (\"lightgreen\", \"orange\", \"yellow\") else \"white\"\n",
    "            self.status_label.setStyleSheet(f\"background-color: {color}; color: {text_color}; padding: 10px;\")\n",
    "    \n",
    "    def toggle_flash(self):\n",
    "        if self.is_flashing:\n",
    "            color = self.flash_colors[self.flash_index]\n",
    "            text_color = \"black\" if color == \"yellow\" else \"white\"\n",
    "            self.status_label.setStyleSheet(f\"background-color: {color}; color: {text_color}; padding: 10px; border: 2px solid {self.flash_colors[1]};\")\n",
    "            self.flash_index = (self.flash_index + 1) % 2\n",
    "        else:\n",
    "            self.flash_timer.stop()\n",
    "\n",
    "    @pyqtSlot(str)\n",
    "    def log_event(self, message):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self.log_box.append(f\"[{timestamp}] {message}\")\n",
    "        self.log_box.verticalScrollBar().setValue(self.log_box.verticalScrollBar().maximum())\n",
    "\n",
    "    @pyqtSlot(int)\n",
    "    def trigger_alert(self, tier):\n",
    "        if tier == 1:\n",
    "            sound_file = SOUND_TIER_1_FILE\n",
    "        elif tier == 2:\n",
    "            sound_file = SOUND_TIER_2_FILE\n",
    "        elif tier == 3:\n",
    "            sound_file = SOUND_TIER_3_FILE\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "        file_path = os.path.join(os.path.dirname(__file__), sound_file)\n",
    "        if not os.path.exists(file_path):\n",
    "            self.log_event(f\"Sound file not found: {sound_file}\")\n",
    "            return\n",
    "            \n",
    "        self.player.setSource(QUrl.fromLocalFile(file_path))\n",
    "        self.player.play()\n",
    "\n",
    "    def convert_cv_to_qt(self, cv_img):\n",
    "        rgb_image = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = rgb_image.shape\n",
    "        bytes_per_line = ch * w\n",
    "        convert_to_Qt_format = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)\n",
    "        scaled_pixmap = QPixmap.fromImage(convert_to_Qt_format).scaled(\n",
    "            self.video_label.size(), \n",
    "            Qt.AspectRatioMode.KeepAspectRatio, \n",
    "            Qt.TransformationMode.SmoothTransformation\n",
    "        )\n",
    "        return scaled_pixmap\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    main_window = MainWindow()\n",
    "    main_window.show()\n",
    "    sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Smart ATM Guardian - Version 3 (Corrected and Stabilized)\n",
    "\n",
    "This single-file script contains all fixes for the previously identified\n",
    "NameError, TypeError, and AttributeError. It uses QMediaPlayer for stable,\n",
    "non-blocking sound.\n",
    "\n",
    "Dependencies:\n",
    "pip install PyQt6 opencv-python mediapipe\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from datetime import datetime\n",
    "import os # For sound file paths\n",
    "\n",
    "# --- PyQt6 Imports ---\n",
    "from PyQt6.QtWidgets import (\n",
    "    QApplication, QWidget, QLabel, QPushButton, \n",
    "    QVBoxLayout, QHBoxLayout, QTextEdit\n",
    ")\n",
    "from PyQt6.QtCore import (\n",
    "    QThread, pyqtSignal, pyqtSlot, Qt, QTimer, QUrl\n",
    ")\n",
    "from PyQt6.QtGui import QImage, QPixmap, QColor, QFont\n",
    "from PyQt6.QtMultimedia import QMediaPlayer, QAudioOutput # For stable sound\n",
    "\n",
    "# === Configuration Constants ===\n",
    "\n",
    "# --- Video Source ---\n",
    "# Use 0 for webcam, or provide an RTSP stream URL\n",
    "VIDEO_SOURCE = 0 \n",
    "# e.g., \"rtsp://admin:password@192.168.1.108:554/cam/realmonitor?channel=1&subtype=0\"\n",
    "\n",
    "# --- Region of Interest (ROI) ---\n",
    "# Define the \"Watchman's Post\" [x1, y1, x2, y2]\n",
    "# If None, the whole frame is used.\n",
    "ROI_BOX = None # Example: [100, 50, 500, 450] \n",
    "\n",
    "# --- Detection Thresholds (in seconds) ---\n",
    "EYES_CLOSED_DURATION = 3.0    # Drowsiness: Eyes closed for 3s\n",
    "HEAD_SLUMP_DURATION = 10.0    # Drowsiness: Head slumped for 10s\n",
    "INATTENTIVE_DURATION = 60.0   # Inattentiveness: Head turned for 60s\n",
    "ABSENCE_DURATION = 120.0  # Absence: Guard missing for 2 mins\n",
    "\n",
    "# --- Alert Escalation Timers (in seconds) ---\n",
    "TIER_1_TO_TIER_2 = 15.0  # Time from Warning (Tier 1) to Alarm (Tier 2)\n",
    "TIER_2_TO_TIER_3 = 30.0  # Time from Alarm (Tier 2) to Escalation (Tier 3)\n",
    "\n",
    "# --- \"Smart\" Detection Logic Thresholds ---\n",
    "# !!! TUNE THESE VALUES FOR YOUR CAMERA/ENVIRONMENT !!!\n",
    "EAR_THRESHOLD = 0.21              # Eye Aspect Ratio threshold\n",
    "HEAD_SLUMP_THRESHOLD = 0.15       # Vertical distance (nose to shoulder-mid) as % of shoulder width\n",
    "HEAD_TURN_THRESHOLD_X = 0.25      # Horizontal distance (nose to shoulder-mid) as % of shoulder width\n",
    "PRESENCE_CONFIDENCE = 0.6         # MediaPipe Pose confidence\n",
    "\n",
    "# --- MediaPipe Imports ---\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# --- Sound Files (Placeholders) ---\n",
    "# Create these files or use your own .wav or .mp3\n",
    "SOUND_TIER_1_FILE = 'gentle_beep.wav'\n",
    "SOUND_TIER_2_FILE = 'wake_up_alarm.wav'\n",
    "SOUND_TIER_3_FILE = 'supervisor_alert.wav'\n",
    "\n",
    "# Indices for MediaPipe Face Mesh (for EAR calculation)\n",
    "EYE_INDICES = {\n",
    "    \"LEFT\": [362, 385, 387, 263, 373, 380],\n",
    "    \"RIGHT\": [33, 160, 158, 133, 153, 144]\n",
    "}\n",
    "\n",
    "# Indices for MediaPipe Pose (for posture analysis)\n",
    "POSE_INDICES = {\n",
    "    \"NOSE\": mp_pose.PoseLandmark.NOSE,\n",
    "    \"LEFT_SHOULDER\": mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "    \"RIGHT_SHOULDER\": mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "}\n",
    "\n",
    "\n",
    "class VideoThread(QThread):\n",
    "    \"\"\"\n",
    "    Worker thread for handling OpenCV video capture and MediaPipe processing.\n",
    "    \"\"\"\n",
    "    # --- Signals to Main Thread ---\n",
    "    change_pixmap_signal = pyqtSignal(np.ndarray)\n",
    "    update_status_signal = pyqtSignal(str, str)\n",
    "    log_event_signal = pyqtSignal(str)\n",
    "    trigger_alert_signal = pyqtSignal(int)\n",
    "\n",
    "    def __init__(self, video_source):\n",
    "        super().__init__()\n",
    "        self.video_source = video_source\n",
    "        self._is_running = True\n",
    "\n",
    "        # --- State Management Variables ---\n",
    "        self.current_state = \"ATTENTIVE\" \n",
    "        self.alert_level = 0             \n",
    "\n",
    "        # --- Timers for State Detection ---\n",
    "        self.eyes_closed_start_time = None\n",
    "        self.head_slump_start_time = None\n",
    "        self.head_turned_start_time = None\n",
    "        self.absence_start_time = None\n",
    "        \n",
    "        # --- Timers for Alert Escalation ---\n",
    "        self.tier_1_start_time = None\n",
    "        self.tier_2_start_time = None\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main processing loop\"\"\"\n",
    "        \n",
    "        # Initialize MediaPipe models\n",
    "        with mp_pose.Pose(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as pose, \\\n",
    "             mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True, # Gets iris and more detailed eye landmarks\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as face_mesh:\n",
    "\n",
    "            cap = cv2.VideoCapture(self.video_source)\n",
    "            if not cap.isOpened():\n",
    "                self.log_event_signal.emit(f\"Error: Could not open video source '{self.video_source}'\")\n",
    "                return\n",
    "\n",
    "            while self._is_running:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    self.log_event_signal.emit(\"Video feed ended or error.\")\n",
    "                    break\n",
    "                \n",
    "                # --- Core Detection Logic ---\n",
    "                frame, new_state = self.process_frame(frame, pose, face_mesh)\n",
    "                \n",
    "                # --- State Management and Alert FSM (Finite State Machine) ---\n",
    "                self.manage_state_and_alerts(new_state)\n",
    "\n",
    "                # --- Emit Frame ---\n",
    "                if ROI_BOX:\n",
    "                    cv2.rectangle(frame, (ROI_BOX[0], ROI_BOX[1]), (ROI_BOX[2], ROI_BOX[3]), (0, 255, 255), 2)\n",
    "\n",
    "                self.change_pixmap_signal.emit(frame)\n",
    "\n",
    "            cap.release()\n",
    "            self.log_event_signal.emit(\"Monitoring stopped.\")\n",
    "\n",
    "    def process_frame(self, frame, pose, face_mesh):\n",
    "        \"\"\"Processes a single video frame for all detection logic.\"\"\"\n",
    "        \n",
    "        frame.flags.writeable = False\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        pose_results = pose.process(frame_rgb)\n",
    "        face_results = face_mesh.process(frame_rgb)\n",
    "        \n",
    "        frame.flags.writeable = True\n",
    "\n",
    "        is_present = False\n",
    "        is_eyes_closed = False\n",
    "        is_head_slumped = False\n",
    "        is_head_turned = False\n",
    "        \n",
    "        now = time.time()\n",
    "\n",
    "        # --- 1. Absence Detection ---\n",
    "        if pose_results.pose_landmarks:\n",
    "            lm_pose = pose_results.pose_landmarks.landmark\n",
    "            visibility = (lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]].visibility + \n",
    "                          lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]].visibility) / 2\n",
    "            \n",
    "            if visibility > PRESENCE_CONFIDENCE:\n",
    "                if ROI_BOX:\n",
    "                    is_present = self.check_in_roi(lm_pose, frame.shape[1], frame.shape[0])\n",
    "                else:\n",
    "                    is_present = True\n",
    "\n",
    "        if not is_present:\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "            \n",
    "            # Reset all other timers\n",
    "            self.eyes_closed_start_time = None\n",
    "            self.head_slump_start_time = None\n",
    "            self.head_turned_start_time = None\n",
    "            return frame, \"ABSENT\"\n",
    "\n",
    "        # --- If PRESENT, check Drowsiness and Inattentiveness ---\n",
    "        self.absence_start_time = None # Reset absence timer\n",
    "\n",
    "        # --- 2. Drowsiness Detection ---\n",
    "        ear = 0.5 # Default to 'eyes open' if no face detected\n",
    "        if face_results.multi_face_landmarks:\n",
    "            face_lm = face_results.multi_face_landmarks[0].landmark\n",
    "            ear = self.calculate_ear(face_lm)\n",
    "\n",
    "            if ear < EAR_THRESHOLD:\n",
    "                if self.eyes_closed_start_time is None:\n",
    "                    self.eyes_closed_start_time = now\n",
    "                elif now - self.eyes_closed_start_time > EYES_CLOSED_DURATION:\n",
    "                    is_eyes_closed = True\n",
    "            else:\n",
    "                self.eyes_closed_start_time = None\n",
    "            \n",
    "            # --- For Tuning: Display the EAR value ---\n",
    "            cv2.putText(frame, f\"EAR: {ear:.2f}\", (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Draw face mesh\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=frame,\n",
    "                landmark_list=face_results.multi_face_landmarks[0],\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "        lm_pose = pose_results.pose_landmarks.landmark\n",
    "        is_head_slumped = self.check_head_slump(lm_pose)\n",
    "        if is_head_slumped:\n",
    "            if self.head_slump_start_time is None:\n",
    "                self.head_slump_start_time = now\n",
    "            elif now - self.head_slump_start_time > HEAD_SLUMP_DURATION:\n",
    "                pass # This state will be handled by the logic below\n",
    "            else:\n",
    "                is_head_slumped = False # Not yet met duration\n",
    "        else:\n",
    "            self.head_slump_start_time = None\n",
    "\n",
    "        if is_eyes_closed or is_head_slumped:\n",
    "            self.head_turned_start_time = None # Drowsiness overrides inattentiveness\n",
    "            return frame, \"DROWSY\"\n",
    "\n",
    "        # --- 3. Inattentiveness Detection ---\n",
    "        is_head_turned = self.check_head_turn(lm_pose)\n",
    "        if is_head_turned:\n",
    "            if self.head_turned_start_time is None:\n",
    "                self.head_turned_start_time = now\n",
    "            elif now - self.head_turned_start_time > INATTENTIVE_DURATION:\n",
    "                pass # State will be handled\n",
    "            else:\n",
    "                is_head_turned = False # Not yet met duration\n",
    "        else:\n",
    "            self.head_turned_start_time = None\n",
    "\n",
    "        if is_head_turned:\n",
    "            return frame, \"INATTENTIVE\"\n",
    "\n",
    "        # --- 4. Attentive State ---\n",
    "        # If none of the above, the guard is attentive\n",
    "        self.eyes_closed_start_time = None\n",
    "        self.head_slump_start_time = None\n",
    "        self.head_turned_start_time = None\n",
    "        return frame, \"ATTENTIVE\"\n",
    "\n",
    "    def manage_state_and_alerts(self, new_state):\n",
    "        \"\"\"Finite State Machine for managing alerts.\"\"\"\n",
    "        now = time.time()\n",
    "        \n",
    "        # If state changes, log it\n",
    "        if new_state != self.current_state:\n",
    "            self.log_event_signal.emit(f\"State Change: {self.current_state} -> {new_state}\")\n",
    "            self.current_state = new_state\n",
    "            \n",
    "            # Reset timers only when moving to a non-alert state\n",
    "            if new_state == \"ATTENTIVE\":\n",
    "                self.tier_1_start_time = None\n",
    "                self.tier_2_start_time = None\n",
    "                self.alert_level = 0 # Reset alert level\n",
    "                self.update_status_signal.emit(\"Attentive\", \"lightgreen\")\n",
    "            # *** BUG FIX ***: We DO NOT reset timers here if moving to another alert state\n",
    "            # This was the cause of the NoneType error.\n",
    "\n",
    "        # --- Process Current State ---\n",
    "        \n",
    "        if self.current_state == \"ABSENT\":\n",
    "            if self.absence_start_time is None:\n",
    "                self.absence_start_time = now\n",
    "            \n",
    "            if now - self.absence_start_time > ABSENCE_DURATION:\n",
    "                if self.alert_level < 3:\n",
    "                    self.alert_level = 3\n",
    "                    self.update_status_signal.emit(\"ESCALATION: Guard Absent\", \"darkred\")\n",
    "                    self.log_event_signal.emit(\"Tier 3 Alert: Guard Absent. Notifying supervisor.\")\n",
    "                    self.trigger_alert_signal.emit(3)\n",
    "\n",
    "        elif self.current_state in [\"DROWSY\", \"INATTENTIVE\"]:\n",
    "            state_name = \"Drowsiness\" if self.current_state == \"DROWSY\" else \"Inattentiveness\"\n",
    "            \n",
    "            # Tier 1: Initial Warning\n",
    "            if self.alert_level == 0:\n",
    "                self.alert_level = 1\n",
    "                self.tier_1_start_time = now # Set timer on first detection\n",
    "                self.update_status_signal.emit(f\"WARNING: {state_name} Detected\", \"orange\")\n",
    "                self.log_event_signal.emit(f\"Tier 1 Alert: {state_name} Warning\")\n",
    "                self.trigger_alert_signal.emit(1)\n",
    "            \n",
    "            # Tier 2: Escalation to Alarm\n",
    "            # *** BUG FIX ***: Check if tier_1_start_time is not None before subtracting\n",
    "            elif self.alert_level == 1 and self.tier_1_start_time and (now - self.tier_1_start_time > TIER_1_TO_TIER_2):\n",
    "                self.alert_level = 2\n",
    "                self.tier_2_start_time = now\n",
    "                self.update_status_signal.emit(\"ALERT: WAKE UP\", \"red\")\n",
    "                self.log_event_signal.emit(f\"Tier 2 Alert: {state_name} Alarm\")\n",
    "                self.trigger_alert_signal.emit(2)\n",
    "\n",
    "            # Tier 3: Escalation to Supervisor\n",
    "            elif self.alert_level == 2 and self.tier_2_start_time and (now - self.tier_2_start_time > TIER_2_TO_TIER_3):\n",
    "                self.alert_level = 3\n",
    "                self.update_status_signal.emit(\"ESCALATION: Unresponsive Guard\", \"darkred\")\n",
    "                self.log_event_signal.emit(\"Tier 3 Alert: Guard unresponsive. Notifying supervisor.\")\n",
    "                self.trigger_alert_signal.emit(3)\n",
    "        \n",
    "        elif self.current_state == \"ATTENTIVE\":\n",
    "            # This block resets alerts if the state becomes attentive\n",
    "            if self.alert_level != 0:\n",
    "                self.alert_level = 0\n",
    "                self.update_status_signal.emit(\"Attentive\", \"lightgreen\")\n",
    "                self.log_event_signal.emit(\"Status: Guard is Attentive. Alerts reset.\")\n",
    "\n",
    "    # --- Helper: Detection Logic Functions ---\n",
    "    \n",
    "    def check_in_roi(self, lm_pose, frame_w, frame_h):\n",
    "        \"\"\"Check if the person (shoulders) is inside the ROI.\"\"\"\n",
    "        ls = lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "        \n",
    "        center_x = ((ls.x + rs.x) / 2) * frame_w\n",
    "        center_y = ((ls.y + rs.y) / 2) * frame_h\n",
    "        \n",
    "        return (ROI_BOX[0] < center_x < ROI_BOX[2] and ROI_BOX[1] < center_y < ROI_BOX[3])\n",
    "\n",
    "    def calculate_ear(self, face_lm):\n",
    "        \"\"\"Calculates the Eye Aspect Ratio (EAR) for both eyes.\"\"\"\n",
    "        \n",
    "        def get_ear(eye_points_indices):\n",
    "            # Get landmark coordinates\n",
    "            points = [face_lm[i] for i in eye_points_indices]\n",
    "            \n",
    "            # Use 2D (x, y) coordinates\n",
    "            # Vertical distances\n",
    "            A = np.linalg.norm([points[1].x - points[5].x, points[1].y - points[5].y])\n",
    "            B = np.linalg.norm([points[2].x - points[4].x, points[2].y - points[4].y])\n",
    "            # Horizontal distance\n",
    "            C = np.linalg.norm([points[0].x - points[3].x, points[0].y - points[3].y])\n",
    "            \n",
    "            if C == 0: return 0.4 # Avoid division by zero, return a 'safe' open value\n",
    "            \n",
    "            # EAR formula\n",
    "            ear = (A + B) / (2.0 * C)\n",
    "            return ear\n",
    "\n",
    "        left_ear = get_ear(EYE_INDICES[\"LEFT\"])\n",
    "        right_ear = get_ear(EYE_INDICES[\"RIGHT\"])\n",
    "        \n",
    "        # Average EAR of both eyes\n",
    "        avg_ear = (left_ear + right_ear) / 2.0\n",
    "        return avg_ear\n",
    "        \n",
    "    def check_head_slump(self, lm_pose):\n",
    "        \"\"\"Checks if the head is slumped down (for sleep or phone).\"\"\"\n",
    "        nose = lm_pose[POSE_INDICES[\"NOSE\"]]\n",
    "        ls = lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "\n",
    "        # If landmarks aren't visible, can't make a determination\n",
    "        if nose.visibility < 0.5 or ls.visibility < 0.5 or rs.visibility < 0.5:\n",
    "            return False\n",
    "\n",
    "        nose_y = nose.y\n",
    "        shoulder_mid_y = (ls.y + rs.y) / 2.0\n",
    "        \n",
    "        # Get shoulder width (in normalized coordinates) as a reference for scale\n",
    "        shoulder_width_norm = abs(ls.x - rs.x)\n",
    "        \n",
    "        if shoulder_width_norm == 0:\n",
    "            return False\n",
    "\n",
    "        # If nose is below shoulder midpoint by a threshold\n",
    "        return nose_y > shoulder_mid_y + (shoulder_width_norm * HEAD_SLUMP_THRESHOLD)\n",
    "\n",
    "    def check_head_turn(self, lm_pose):\n",
    "        \"\"\"Checks if the head is turned away (left/right).\"\"\"\n",
    "        nose = lm_pose[POSE_INDICES[\"NOSE\"]]\n",
    "        ls = lm_pose[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "\n",
    "        if nose.visibility < 0.5 or ls.visibility < 0.5 or rs.visibility < 0.5:\n",
    "            return False\n",
    "\n",
    "        shoulder_mid_x = (ls.x + rs.x) / 2.0\n",
    "        shoulder_width_norm = abs(ls.x - rs.x)\n",
    "\n",
    "        if shoulder_width_norm == 0:\n",
    "            return False\n",
    "            \n",
    "        turn_distance = abs(nose.x - shoulder_mid_x)\n",
    "        \n",
    "        # If nose is horizontally offset\n",
    "        return turn_distance > (shoulder_width_norm * HEAD_TURN_THRESHOLD_X)\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stops the thread.\"\"\"\n",
    "        self._is_running = False\n",
    "        self.log_event_signal.emit(\"Stop signal received. Shutting down thread...\")\n",
    "        self.wait() # Wait for the thread to finish\n",
    "\n",
    "\n",
    "class MainWindow(QWidget):\n",
    "    \"\"\"Main GUI Window\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Smart ATM Guardian (V3 - Corrected)\")\n",
    "        self.setGeometry(100, 100, 1000, 750)\n",
    "        \n",
    "        # --- GUI Widgets ---\n",
    "        self.video_label = QLabel(\"Connecting to video stream...\")\n",
    "        self.video_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        self.video_label.setMinimumSize(640, 480)\n",
    "        self.video_label.setStyleSheet(\"background-color: black; border: 2px solid #555;\")\n",
    "\n",
    "        # *** ATTRIBUTEERROR FIX ***\n",
    "        # The flash_timer must be initialized *BEFORE* update_status is called\n",
    "        # --- Flashing Timer ---\n",
    "        self.flash_timer = QTimer(self)\n",
    "        self.flash_timer.setInterval(500) # 500ms blink rate\n",
    "        self.flash_timer.timeout.connect(self.toggle_flash)\n",
    "        self.is_flashing = False\n",
    "        self.flash_colors = (\"red\", \"yellow\")\n",
    "        self.flash_index = 0\n",
    "\n",
    "        self.status_label = QLabel(\"Status: IDLE\")\n",
    "        self.status_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        font = QFont(\"Arial\", 20, QFont.Weight.Bold)\n",
    "        self.status_label.setFont(font)\n",
    "        # This call is now SAFE because self.flash_timer exists\n",
    "        self.update_status(\"IDLE\", \"gray\") \n",
    "\n",
    "        self.start_stop_button = QPushButton(\"Start Monitoring\")\n",
    "        self.start_stop_button.setFont(QFont(\"Arial\", 12))\n",
    "        self.start_stop_button.setCheckable(True)\n",
    "        self.start_stop_button.setStyleSheet(\n",
    "            \"QPushButton { padding: 10px; background-color: #4CAF50; color: white; border: none; }\"\n",
    "            \"QPushButton:checked { background-color: #f44336; }\"\n",
    "        )\n",
    "\n",
    "        self.log_box_label = QLabel(\"Event Log:\")\n",
    "        self.log_box_label.setFont(QFont(\"Arial\", 10, QFont.Weight.Bold))\n",
    "        \n",
    "        self.log_box = QTextEdit()\n",
    "        self.log_box.setReadOnly(True)\n",
    "        self.log_box.setFont(QFont(\"Courier New\", 9))\n",
    "        self.log_box.setStyleSheet(\"background-color: #f0f0f0;\")\n",
    "\n",
    "        # --- Layouts ---\n",
    "        v_layout = QVBoxLayout()\n",
    "        v_layout.addWidget(self.status_label)\n",
    "        h_layout = QHBoxLayout()\n",
    "        h_layout.addWidget(self.video_label, 2)\n",
    "        right_panel_layout = QVBoxLayout()\n",
    "        right_panel_layout.addWidget(self.start_stop_button)\n",
    "        right_panel_layout.addWidget(self.log_box_label)\n",
    "        right_panel_layout.addWidget(self.log_box)\n",
    "        h_layout.addLayout(right_panel_layout, 1)\n",
    "        v_layout.addLayout(h_layout)\n",
    "        self.setLayout(v_layout)\n",
    "\n",
    "        # --- Sound Player (Robust Replacement for playsound) ---\n",
    "        self.player = QMediaPlayer()\n",
    "        self.audio_output = QAudioOutput()\n",
    "        self.player.setAudioOutput(self.audio_output)\n",
    "        self.audio_output.setVolume(1.0) # Volume 0.0 to 1.0\n",
    "\n",
    "        # --- Connections ---\n",
    "        self.start_stop_button.clicked.connect(self.toggle_monitoring)\n",
    "        \n",
    "        self.video_thread = None\n",
    "        self.log_event(f\"Application Initialized. Ready to start monitoring.\")\n",
    "\n",
    "    def toggle_monitoring(self, checked):\n",
    "        if checked:\n",
    "            self.log_event(\"Starting monitoring...\")\n",
    "            self.video_thread = VideoThread(VIDEO_SOURCE)\n",
    "            \n",
    "            # Connect signals\n",
    "            self.video_thread.change_pixmap_signal.connect(self.update_image)\n",
    "            self.video_thread.update_status_signal.connect(self.update_status)\n",
    "            self.video_thread.log_event_signal.connect(self.log_event)\n",
    "            self.video_thread.trigger_alert_signal.connect(self.trigger_alert)\n",
    "            self.video_thread.finished.connect(self.thread_finished)\n",
    "            \n",
    "            self.video_thread.start()\n",
    "            self.start_stop_button.setText(\"Stop Monitoring\")\n",
    "        else:\n",
    "            if self.video_thread:\n",
    "                self.video_thread.stop()\n",
    "                # The thread_finished slot will handle the rest\n",
    "\n",
    "    def thread_finished(self):\n",
    "        \"\"\"Handles cleanup when the thread is done.\"\"\"\n",
    "        self.log_event(\"Video thread has finished.\")\n",
    "        self.start_stop_button.setChecked(False)\n",
    "        self.start_stop_button.setText(\"Start Monitoring\")\n",
    "        self.video_label.setText(\"Monitoring Stopped.\")\n",
    "        self.video_label.setStyleSheet(\"background-color: black; color: white; border: 2px solid #555;\")\n",
    "        self.update_status(\"IDLE\", \"gray\")\n",
    "\n",
    "    def closeEvent(self, event):\n",
    "        \"\"\"Ensure the thread is stopped when the window is closed.\"\"\"\n",
    "        self.log_event(\"Application closing...\")\n",
    "        if self.video_thread and self.video_thread.isRunning():\n",
    "            self.video_thread.stop()\n",
    "        event.accept()\n",
    "\n",
    "    # --- Slot Functions (receive signals from QThread) ---\n",
    "\n",
    "    @pyqtSlot(np.ndarray)\n",
    "    def update_image(self, cv_img):\n",
    "        \"\"\"Updates the video_label with a new frame from the thread.\"\"\"\n",
    "        qt_img = self.convert_cv_to_qt(cv_img)\n",
    "        self.video_label.setPixmap(qt_img)\n",
    "\n",
    "    @pyqtSlot(str, str)\n",
    "    def update_status(self, status_text, color):\n",
    "        \"\"\"Updates the status label text and color.\"\"\"\n",
    "        self.status_label.setText(f\"Status: {status_text}\")\n",
    "        \n",
    "        if color in (\"red\", \"darkred\"):\n",
    "            if not self.flash_timer.isActive():\n",
    "                self.is_flashing = True\n",
    "                self.flash_colors = (\"red\", \"yellow\") if color == \"red\" else (\"darkred\", \"white\")\n",
    "                self.flash_index = 0\n",
    "                self.flash_timer.start()\n",
    "                self.toggle_flash() # Start immediately\n",
    "        else:\n",
    "            # This block is now safe\n",
    "            self.is_flashing = False\n",
    "            self.flash_timer.stop()\n",
    "            text_color = \"black\" if color in (\"lightgreen\", \"orange\", \"yellow\") else \"white\"\n",
    "            self.status_label.setStyleSheet(f\"background-color: {color}; color: {text_color}; padding: 10px;\")\n",
    "    \n",
    "    def toggle_flash(self):\n",
    "        \"\"\"Called by QTimer to create a flashing effect.\"\"\"\n",
    "        if self.is_flashing:\n",
    "            color = self.flash_colors[self.flash_index]\n",
    "            text_color = \"black\" if color == \"yellow\" else \"white\"\n",
    "            self.status_label.setStyleSheet(\n",
    "                f\"background-color: {color}; color: {text_color}; \"\n",
    "                f\"padding: 10px; border: 2px solid {self.flash_colors[1]};\"\n",
    "            )\n",
    "            self.flash_index = (self.flash_index + 1) % 2\n",
    "        else:\n",
    "            self.flash_timer.stop()\n",
    "\n",
    "    @pyqtSlot(str)\n",
    "    def log_event(self, message):\n",
    "        \"\"\"Appends a new message to the event log box.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self.log_box.append(f\"[{timestamp}] {message}\")\n",
    "        self.log_box.verticalScrollBar().setValue(self.log_box.verticalScrollBar().maximum())\n",
    "\n",
    "    @pyqtSlot(int)\n",
    "    def trigger_alert(self, tier):\n",
    "        \"\"\"Triggers the alert sound using the non-blocking QMediaPlayer.\"\"\"\n",
    "        if tier == 1:\n",
    "            sound_file = SOUND_TIER_1_FILE\n",
    "        elif tier == 2:\n",
    "            sound_file = SOUND_TIER_2_FILE\n",
    "        elif tier == 3:\n",
    "            sound_file = SOUND_TIER_3_FILE\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "        # Get absolute path to the sound file\n",
    "        file_path = os.path.join(os.path.dirname(__file__), sound_file)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            self.log_event(f\"Sound file not found: {sound_file}\")\n",
    "            return\n",
    "            \n",
    "        self.player.setSource(QUrl.fromLocalFile(file_path))\n",
    "        self.player.play()\n",
    "\n",
    "    def convert_cv_to_qt(self, cv_img):\n",
    "        \"\"\"Converts an OpenCV image (BGR) to a QPixmap (RGB).\"\"\"\n",
    "        rgb_image = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = rgb_image.shape\n",
    "        bytes_per_line = ch * w\n",
    "        convert_to_Qt_format = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)\n",
    "        \n",
    "        # Scale image to fit the label while maintaining aspect ratio\n",
    "        scaled_pixmap = QPixmap.fromImage(convert_to_Qt_format).scaled(\n",
    "            self.video_label.size(), \n",
    "            Qt.AspectRatioMode.KeepAspectRatio, \n",
    "            Qt.TransformationMode.SmoothTransformation\n",
    "        )\n",
    "        return scaled_pixmap\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    main_window = MainWindow()\n",
    "    main_window.show()\n",
    "    sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dadc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Smart ATM Guardian - Version 4 (Multi-Person Isolation)\n",
    "\n",
    "This is a complete, expert-level script that solves the multi-person problem\n",
    "by using a \"Guard Post\" ROI. It isolates the guard and performs targeted \n",
    "analysis, ignoring all other people in the frame.\n",
    "\n",
    "Dependencies:\n",
    "pip install PyQt6 opencv-python mediapipe\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# --- PyQt6 Imports ---\n",
    "from PyQt6.QtWidgets import (\n",
    "    QApplication, QWidget, QLabel, QPushButton, \n",
    "    QVBoxLayout, QHBoxLayout, QTextEdit\n",
    ")\n",
    "from PyQt6.QtCore import (\n",
    "    QThread, pyqtSignal, pyqtSlot, Qt, QTimer, QUrl\n",
    ")\n",
    "from PyQt6.QtGui import QImage, QPixmap, QColor, QFont\n",
    "from PyQt6.QtMultimedia import QMediaPlayer, QAudioOutput \n",
    "\n",
    "# === Configuration Constants ===\n",
    "\n",
    "# --- Video Source ---\n",
    "VIDEO_SOURCE = 0 \n",
    "\n",
    "# --- (NEW) GUARD POST ROI ---\n",
    "# This is the \"registered\" zone for the guard.\n",
    "# Format: [x1, y1, x2, y2]\n",
    "# TUNE THIS: Set to a box around the guard's chair/desk\n",
    "GUARD_POST_ROI = [100, 50, 500, 450] # Example: (x1, y1, x2, y2)\n",
    "\n",
    "# --- Detection Thresholds (in seconds) ---\n",
    "EYES_CLOSED_DURATION = 3.0    # Drowsiness: Eyes closed for 3s\n",
    "HEAD_SLUMP_DURATION = 10.0    # Drowsiness: Head slumped for 10s\n",
    "INATTENTIVE_DURATION = 60.0   # Inattentiveness: Head turned for 60s\n",
    "ABSENCE_DURATION = 120.0  # Absence: Guard missing for 2 mins\n",
    "\n",
    "# --- Alert Escalation Timers (in seconds) ---\n",
    "TIER_1_TO_TIER_2 = 15.0  # Time from Warning (Tier 1) to Alarm (Tier 2)\n",
    "TIER_2_TO_TIER_3 = 30.0  # Time from Alarm (Tier 2) to Escalation (Tier 3)\n",
    "\n",
    "# --- Detection Logic Thresholds ---\n",
    "EAR_THRESHOLD = 0.21              \n",
    "HEAD_SLUMP_THRESHOLD = 0.15       \n",
    "HEAD_TURN_THRESHOLD_X = 0.25      \n",
    "PRESENCE_CONFIDENCE = 0.6         \n",
    "\n",
    "# --- MediaPipe Imports ---\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# --- Sound Files ---\n",
    "SOUND_TIER_1_FILE = 'gentle_beep.wav'\n",
    "SOUND_TIER_2_FILE = 'wake_up_alarm.wav'\n",
    "SOUND_TIER_3_FILE = 'supervisor_alert.wav'\n",
    "\n",
    "# Indices for MediaPipe Face Mesh (for EAR calculation)\n",
    "EYE_INDICES = {\n",
    "    \"LEFT\": [362, 385, 387, 263, 373, 380],\n",
    "    \"RIGHT\": [33, 160, 158, 133, 153, 144]\n",
    "}\n",
    "\n",
    "# Indices for MediaPipe Pose (for posture analysis)\n",
    "POSE_INDICES = {\n",
    "    \"NOSE\": mp_pose.PoseLandmark.NOSE,\n",
    "    \"LEFT_SHOULDER\": mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "    \"RIGHT_SHOULDER\": mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "    \"LEFT_HIP\": mp_pose.PoseLandmark.LEFT_HIP,\n",
    "    \"RIGHT_HIP\": mp_pose.PoseLandmark.RIGHT_HIP,\n",
    "}\n",
    "\n",
    "\n",
    "class VideoThread(QThread):\n",
    "    \"\"\"\n",
    "    Worker thread for handling OpenCV video capture and MediaPipe processing.\n",
    "    \"\"\"\n",
    "    change_pixmap_signal = pyqtSignal(np.ndarray)\n",
    "    update_status_signal = pyqtSignal(str, str)\n",
    "    log_event_signal = pyqtSignal(str)\n",
    "    trigger_alert_signal = pyqtSignal(int)\n",
    "\n",
    "    def __init__(self, video_source):\n",
    "        super().__init__()\n",
    "        self.video_source = video_source\n",
    "        self._is_running = True\n",
    "        self.current_state = \"ATTENTIVE\" \n",
    "        self.alert_level = 0             \n",
    "\n",
    "        self.eyes_closed_start_time = None\n",
    "        self.head_slump_start_time = None\n",
    "        self.head_turned_start_time = None\n",
    "        self.absence_start_time = None\n",
    "        self.tier_1_start_time = None\n",
    "        self.tier_2_start_time = None\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main processing loop\"\"\"\n",
    "        \n",
    "        # --- NEW LOGIC ---\n",
    "        # Initialize TWO models. Pose is for everyone, Face Mesh is just for the guard.\n",
    "        # We enable multi-person pose detection.\n",
    "        # with mp_pose.Pose(\n",
    "        #     min_detection_confidence=0.5,\n",
    "        #     min_tracking_confidence=0.5) as pose, \\\n",
    "        #      mp_face_mesh.FaceMesh(\n",
    "        with mp_pose.Pose(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5,\n",
    "            max_num_poses=5) as pose, \\\n",
    "             mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1, # We only ever pass it one face (the guard's)\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as face_mesh:\n",
    "\n",
    "            cap = cv2.VideoCapture(self.video_source)\n",
    "            if not cap.isOpened():\n",
    "                self.log_event_signal.emit(f\"Error: Could not open video source '{self.video_source}'\")\n",
    "                return\n",
    "\n",
    "            while self._is_running:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    self.log_event_signal.emit(\"Video feed ended or error.\")\n",
    "                    break\n",
    "                \n",
    "                # --- NEW ARCHITECTURE ---\n",
    "                # 1. Process the *entire* frame for *all* poses\n",
    "                frame.flags.writeable = False\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pose_results = pose.process(frame_rgb)\n",
    "                frame.flags.writeable = True\n",
    "\n",
    "                # 2. Isolate the Guard\n",
    "                guard_is_present = False\n",
    "                guard_pose_landmarks = None\n",
    "                new_state = \"ABSENT\" # Default state is ABSENT\n",
    "\n",
    "                if pose_results.pose_landmarks:\n",
    "                    # Loop through all people detected\n",
    "                    for person_landmarks in pose_results.pose_landmarks:\n",
    "                        # Check if this person is our guard\n",
    "                        if self.check_in_roi(person_landmarks, frame.shape[1], frame.shape[0]):\n",
    "                            guard_is_present = True\n",
    "                            guard_pose_landmarks = person_landmarks\n",
    "                            \n",
    "                            # Draw this person in GREEN (our guard)\n",
    "                            mp_drawing.draw_landmarks(\n",
    "                                frame, guard_pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                                connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "                            )\n",
    "                            break # Found our guard, stop looping\n",
    "                        else:\n",
    "                            # Draw other people in RED (ignored)\n",
    "                            mp_drawing.draw_landmarks(\n",
    "                                frame, person_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1, circle_radius=1),\n",
    "                                connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1, circle_radius=1)\n",
    "                            )\n",
    "\n",
    "                # 3. Analyze the Guard (if found)\n",
    "                if guard_is_present:\n",
    "                    self.absence_start_time = None # Reset absence timer\n",
    "                    \n",
    "                    # --- Run Drowsiness/Inattention Logic ---\n",
    "                    \n",
    "                    # A) Head Slump / Turn (from Pose)\n",
    "                    is_head_slumped = self.check_head_slump(guard_pose_landmarks)\n",
    "                    is_head_turned = self.check_head_turn(guard_pose_landmarks)\n",
    "                    \n",
    "                    # B) Eye Closure (from Face Mesh)\n",
    "                    # --- NEW: Run Face Mesh *only* on the guard's face ---\n",
    "                    ear, face_box = self.get_guard_ear(frame_rgb, guard_pose_landmarks, face_mesh)\n",
    "                    is_eyes_closed = False\n",
    "                    \n",
    "                    # Draw the face box\n",
    "                    if face_box:\n",
    "                        cv2.rectangle(frame, (face_box[0], face_box[1]), (face_box[2], face_box[3]), (255, 0, 0), 2)\n",
    "                        cv2.putText(frame, f\"EAR: {ear:.2f}\", (face_box[0], face_box[1] - 10), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                    if ear < EAR_THRESHOLD:\n",
    "                        is_eyes_closed = True\n",
    "\n",
    "                    # 4. Determine Guard's State\n",
    "                    now = time.time()\n",
    "                    \n",
    "                    # Check for Drowsiness (Eyes or Slump)\n",
    "                    if is_eyes_closed or is_head_slumped:\n",
    "                        self.head_turned_start_time = None # Drowsiness overrides inattentiveness\n",
    "                        \n",
    "                        # Use the correct timer\n",
    "                        timer_to_check = self.eyes_closed_start_time if is_eyes_closed else self.head_slump_start_time\n",
    "                        duration = EYES_CLOSED_DURATION if is_eyes_closed else HEAD_SLUMP_DURATION\n",
    "                        \n",
    "                        if timer_to_check is None:\n",
    "                            if is_eyes_closed: self.eyes_closed_start_time = now\n",
    "                            if is_head_slumped: self.head_slump_start_time = now\n",
    "                            new_state = \"ATTENTIVE\" # Not yet met duration\n",
    "                        elif now - timer_to_check > duration:\n",
    "                            new_state = \"DROWSY\"\n",
    "                        else:\n",
    "                            new_state = \"ATTENTIVE\" # Not yet met duration\n",
    "                            \n",
    "                    # Check for Inattentiveness\n",
    "                    elif is_head_turned:\n",
    "                        self.eyes_closed_start_time = None\n",
    "                        self.head_slump_start_time = None\n",
    "                        if self.head_turned_start_time is None:\n",
    "                            self.head_turned_start_time = now\n",
    "                            new_state = \"ATTENTIVE\"\n",
    "                        elif now - self.head_turned_start_time > INATTENTIVE_DURATION:\n",
    "                            new_state = \"INATTENTIVE\"\n",
    "                        else:\n",
    "                            new_state = \"ATTENTIVE\"\n",
    "                            \n",
    "                    # Else, guard is Attentive\n",
    "                    else:\n",
    "                        self.eyes_closed_start_time = None\n",
    "                        self.head_slump_start_time = None\n",
    "                        self.head_turned_start_time = None\n",
    "                        new_state = \"ATTENTIVE\"\n",
    "                \n",
    "                else: # Guard is not present\n",
    "                    new_state = \"ABSENT\"\n",
    "                \n",
    "                # 5. Manage Alerts based on state\n",
    "                self.manage_state_and_alerts(new_state)\n",
    "\n",
    "                # 6. Draw the Guard Post ROI and emit frame\n",
    "                cv2.rectangle(frame, (GUARD_POST_ROI[0], GUARD_POST_ROI[1]), (GUARD_POST_ROI[2], GUARD_POST_ROI[3]), (0, 255, 255), 3)\n",
    "                cv2.putText(frame, \"GUARD POST\", (GUARD_POST_ROI[0], GUARD_POST_ROI[1] - 10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                self.change_pixmap_signal.emit(frame)\n",
    "\n",
    "            cap.release()\n",
    "            self.log_event_signal.emit(\"Monitoring stopped.\")\n",
    "\n",
    "    def get_guard_ear(self, frame_rgb, guard_pose, face_mesh_model):\n",
    "        \"\"\"\n",
    "        Runs Face Mesh on a cropped image of the guard's face.\n",
    "        \"\"\"\n",
    "        # Get face landmarks from POSE model (less accurate, but gives us a box)\n",
    "        h, w, _ = frame_rgb.shape\n",
    "        nose = guard_pose.landmark[POSE_INDICES[\"NOSE\"]]\n",
    "        \n",
    "        # Estimate a bounding box for the face\n",
    "        # This is a rough estimate\n",
    "        face_center_x = int(nose.x * w)\n",
    "        face_center_y = int(nose.y * h)\n",
    "        \n",
    "        # Estimate shoulder width as a proxy for head size\n",
    "        ls = guard_pose.landmark[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = guard_pose.landmark[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "        shoulder_width_px = abs(ls.x - rs.x) * w\n",
    "        \n",
    "        if shoulder_width_px < 50: # Handle edge case\n",
    "            shoulder_width_px = 150 # Default size\n",
    "            \n",
    "        box_size = int(shoulder_width_px * 0.8) # Face is ~80% of shoulder width\n",
    "        \n",
    "        x1 = max(0, face_center_x - box_size)\n",
    "        y1 = max(0, face_center_y - box_size)\n",
    "        x2 = min(w, face_center_x + box_size)\n",
    "        y2 = min(h, face_center_y + box_size)\n",
    "\n",
    "        if x1 >= x2 or y1 >= y2:\n",
    "            return 0.5, None # Invalid box, return 'eyes open'\n",
    "\n",
    "        # Crop the image to the face\n",
    "        face_crop_rgb = frame_rgb[y1:y2, x1:x2]\n",
    "\n",
    "        if face_crop_rgb.size == 0:\n",
    "            return 0.5, None # Invalid crop\n",
    "\n",
    "        # Run Face Mesh on the tiny crop\n",
    "        face_crop_rgb.flags.writeable = False\n",
    "        face_results = face_mesh_model.process(face_crop_rgb)\n",
    "        face_crop_rgb.flags.writeable = True\n",
    "\n",
    "        if face_results.multi_face_landmarks:\n",
    "            face_lm = face_results.multi_face_landmarks[0].landmark\n",
    "            return self.calculate_ear(face_lm), (x1, y1, x2, y2)\n",
    "        \n",
    "        return 0.5, (x1, y1, x2, y2) # No face detected in crop, return 'eyes open'\n",
    "\n",
    "    def manage_state_and_alerts(self, new_state):\n",
    "        \"\"\"Finite State Machine for managing alerts. (Same as V3)\"\"\"\n",
    "        now = time.time()\n",
    "        \n",
    "        if new_state != self.current_state:\n",
    "            self.log_event_signal.emit(f\"State Change: {self.current_state} -> {new_state}\")\n",
    "            self.current_state = new_state\n",
    "            \n",
    "            if new_state == \"ATTENTIVE\":\n",
    "                self.tier_1_start_time = None\n",
    "                self.tier_2_start_time = None\n",
    "                self.alert_level = 0\n",
    "                self.update_status_signal.emit(\"Attentive (Guard)\", \"lightgreen\")\n",
    "\n",
    "        # Process Current State\n",
    "        if self.current_state == \"ABSENT\":\n",
    "            self.eyes_closed_start_time = None\n",
    "            self.head_slump_start_time = None\n",
    "            self.head_turned_start_time = None\n",
    "            \n",
    "            if self.absence_start_time is None:\n",
    "                self.absence_start_time = now\n",
    "            \n",
    "            if now - self.absence_start_time > ABSENCE_DURATION:\n",
    "                if self.alert_level < 3:\n",
    "                    self.alert_level = 3\n",
    "                    self.update_status_signal.emit(\"ESCALATION: Guard Absent\", \"darkred\")\n",
    "                    self.log_event_signal.emit(\"Tier 3 Alert: Guard Absent.\")\n",
    "                    self.trigger_alert_signal.emit(3)\n",
    "\n",
    "        elif self.current_state in [\"DROWSY\", \"INATTENTIVE\"]:\n",
    "            state_name = \"Drowsiness\" if self.current_state == \"DROWSY\" else \"Inattentiveness\"\n",
    "            self.absence_start_time = None\n",
    "            \n",
    "            if self.alert_level == 0:\n",
    "                self.alert_level = 1\n",
    "                self.tier_1_start_time = now\n",
    "                self.update_status_signal.emit(f\"WARNING: {state_name} Detected\", \"orange\")\n",
    "                self.log_event_signal.emit(f\"Tier 1 Alert: {state_name} Warning\")\n",
    "                self.trigger_alert_signal.emit(1)\n",
    "            \n",
    "            elif self.alert_level == 1 and self.tier_1_start_time and (now - self.tier_1_start_time > TIER_1_TO_TIER_2):\n",
    "                self.alert_level = 2\n",
    "                self.tier_2_start_time = now\n",
    "                self.update_status_signal.emit(\"ALERT: WAKE UP\", \"red\")\n",
    "                self.log_event_signal.emit(f\"Tier 2 Alert: {state_name} Alarm\")\n",
    "                self.trigger_alert_signal.emit(2)\n",
    "\n",
    "            elif self.alert_level == 2 and self.tier_2_start_time and (now - self.tier_2_start_time > TIER_2_TO_TIER_3):\n",
    "                self.alert_level = 3\n",
    "                self.update_status_signal.emit(\"ESCALATION: Unresponsive Guard\", \"darkred\")\n",
    "                self.log_event_signal.emit(\"Tier 3 Alert: Guard unresponsive.\")\n",
    "                self.trigger_alert_signal.emit(3)\n",
    "        \n",
    "        elif self.current_state == \"ATTENTIVE\":\n",
    "            self.absence_start_time = None\n",
    "            if self.alert_level != 0:\n",
    "                self.alert_level = 0\n",
    "                self.update_status_signal.emit(\"Attentive (Guard)\", \"lightgreen\")\n",
    "                self.log_event_signal.emit(\"Status: Guard is Attentive. Alerts reset.\")\n",
    "\n",
    "    # --- Helper: Detection Logic Functions ---\n",
    "    \n",
    "    def check_in_roi(self, lm_pose, frame_w, frame_h):\n",
    "        \"\"\"Checks if the person's center mass (hips) is inside the ROI.\"\"\"\n",
    "        ls = lm_pose.landmark[POSE_INDICES[\"LEFT_HIP\"]]\n",
    "        rs = lm_pose.landmark[POSE_INDICES[\"RIGHT_HIP\"]]\n",
    "        \n",
    "        # Check visibility\n",
    "        if ls.visibility < 0.5 or rs.visibility < 0.5:\n",
    "            return False\n",
    "        \n",
    "        center_x = ((ls.x + rs.x) / 2) * frame_w\n",
    "        center_y = ((ls.y + rs.y) / 2) * frame_h\n",
    "        \n",
    "        return (GUARD_POST_ROI[0] < center_x < GUARD_POST_ROI[2] and \n",
    "                GUARD_POST_ROI[1] < center_y < GUARD_POST_ROI[3])\n",
    "\n",
    "    def calculate_ear(self, face_lm):\n",
    "        \"\"\"Calculates the Eye Aspect Ratio (EAR) for both eyes.\"\"\"\n",
    "        def get_ear(eye_points_indices):\n",
    "            points = [face_lm[i] for i in eye_points_indices]\n",
    "            A = np.linalg.norm([points[1].x - points[5].x, points[1].y - points[5].y])\n",
    "            B = np.linalg.norm([points[2].x - points[4].x, points[2].y - points[4].y])\n",
    "            C = np.linalg.norm([points[0].x - points[3].x, points[0].y - points[3].y])\n",
    "            if C == 0: return 0.4\n",
    "            return (A + B) / (2.0 * C)\n",
    "\n",
    "        return (get_ear(EYE_INDICES[\"LEFT\"]) + get_ear(EYE_INDICES[\"RIGHT\"])) / 2.0\n",
    "        \n",
    "    def check_head_slump(self, lm_pose):\n",
    "        \"\"\"Checks if the head is slumped down (for sleep or phone).\"\"\"\n",
    "        nose = lm_pose.landmark[POSE_INDICES[\"NOSE\"]]\n",
    "        ls = lm_pose.landmark[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose.landmark[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "\n",
    "        if nose.visibility < 0.5 or ls.visibility < 0.5 or rs.visibility < 0.5:\n",
    "            return False\n",
    "\n",
    "        nose_y = nose.y\n",
    "        shoulder_mid_y = (ls.y + rs.y) / 2.0\n",
    "        shoulder_width_norm = abs(ls.x - rs.x)\n",
    "        \n",
    "        if shoulder_width_norm == 0: return False\n",
    "        return nose_y > shoulder_mid_y + (shoulder_width_norm * HEAD_SLUMP_THRESHOLD)\n",
    "\n",
    "    def check_head_turn(self, lm_pose):\n",
    "        \"\"\"Checks if the head is turned away (left/right).\"\"\"\n",
    "        nose = lm_pose.landmark[POSE_INDICES[\"NOSE\"]]\n",
    "        ls = lm_pose.landmark[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose.landmark[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "\n",
    "        if nose.visibility < 0.5 or ls.visibility < 0.5 or rs.visibility < 0.5:\n",
    "            return False\n",
    "\n",
    "        shoulder_mid_x = (ls.x + rs.x) / 2.0\n",
    "        shoulder_width_norm = abs(ls.x - rs.x)\n",
    "\n",
    "        if shoulder_width_norm == 0: return False\n",
    "        turn_distance = abs(nose.x - shoulder_mid_x)\n",
    "        return turn_distance > (shoulder_width_norm * HEAD_TURN_THRESHOLD_X)\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stops the thread.\"\"\"\n",
    "        self._is_running = False\n",
    "        self.log_event_signal.emit(\"Stop signal received. Shutting down thread...\")\n",
    "        self.wait()\n",
    "\n",
    "\n",
    "class MainWindow(QWidget):\n",
    "    \"\"\"Main GUI Window (Same as V3, all fixes included)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Smart ATM Guardian (V4 - Multi-Person Isolation)\")\n",
    "        self.setGeometry(100, 100, 1000, 750)\n",
    "        \n",
    "        # --- GUI Widgets ---\n",
    "        self.video_label = QLabel(\"Connecting to video stream...\")\n",
    "        self.video_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        self.video_label.setMinimumSize(640, 480)\n",
    "        self.video_label.setStyleSheet(\"background-color: black; border: 2px solid #555;\")\n",
    "\n",
    "        # --- Flashing Timer (FIXED: Initialized first) ---\n",
    "        self.flash_timer = QTimer(self)\n",
    "        self.flash_timer.setInterval(500)\n",
    "        self.flash_timer.timeout.connect(self.toggle_flash)\n",
    "        self.is_flashing = False\n",
    "        self.flash_colors = (\"red\", \"yellow\")\n",
    "        self.flash_index = 0\n",
    "\n",
    "        self.status_label = QLabel(\"Status: IDLE\")\n",
    "        self.status_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        font = QFont(\"Arial\", 20, QFont.Weight.Bold)\n",
    "        self.status_label.setFont(font)\n",
    "        self.update_status(\"IDLE\", \"gray\") # SAFE to call now\n",
    "\n",
    "        self.start_stop_button = QPushButton(\"Start Monitoring\")\n",
    "        self.start_stop_button.setFont(QFont(\"Arial\", 12))\n",
    "        self.start_stop_button.setCheckable(True)\n",
    "        self.start_stop_button.setStyleSheet(\n",
    "            \"QPushButton { padding: 10px; background-color: #4CAF50; color: white; border: none; }\"\n",
    "            \"QPushButton:checked { background-color: #f44336; }\"\n",
    "        )\n",
    "\n",
    "        self.log_box_label = QLabel(\"Event Log:\")\n",
    "        self.log_box_label.setFont(QFont(\"Arial\", 10, QFont.Weight.Bold))\n",
    "        \n",
    "        self.log_box = QTextEdit()\n",
    "        self.log_box.setReadOnly(True)\n",
    "        self.log_box.setFont(QFont(\"Courier New\", 9))\n",
    "\n",
    "        # --- Layouts ---\n",
    "        v_layout = QVBoxLayout()\n",
    "        v_layout.addWidget(self.status_label)\n",
    "        h_layout = QHBoxLayout()\n",
    "        h_layout.addWidget(self.video_label, 2)\n",
    "        right_panel_layout = QVBoxLayout()\n",
    "        right_panel_layout.addWidget(self.start_stop_button)\n",
    "        right_panel_layout.addWidget(self.log_box_label)\n",
    "        right_panel_layout.addWidget(self.log_box)\n",
    "        h_layout.addLayout(right_panel_layout, 1)\n",
    "        v_layout.addLayout(h_layout)\n",
    "        self.setLayout(v_layout)\n",
    "\n",
    "        # --- Sound Player ---\n",
    "        self.player = QMediaPlayer()\n",
    "        self.audio_output = QAudioOutput()\n",
    "        self.player.setAudioOutput(self.audio_output)\n",
    "        self.audio_output.setVolume(1.0) \n",
    "\n",
    "        # --- Connections ---\n",
    "        self.start_stop_button.clicked.connect(self.toggle_monitoring)\n",
    "        self.video_thread = None\n",
    "        self.log_event(f\"Application Initialized. Ready to start monitoring.\")\n",
    "\n",
    "    def toggle_monitoring(self, checked):\n",
    "        if checked:\n",
    "            self.log_event(\"Starting monitoring...\")\n",
    "            self.video_thread = VideoThread(VIDEO_SOURCE)\n",
    "            self.video_thread.change_pixmap_signal.connect(self.update_image)\n",
    "            self.video_thread.update_status_signal.connect(self.update_status)\n",
    "            self.video_thread.log_event_signal.connect(self.log_event)\n",
    "            self.video_thread.trigger_alert_signal.connect(self.trigger_alert)\n",
    "            self.video_thread.finished.connect(self.thread_finished)\n",
    "            self.video_thread.start()\n",
    "            self.start_stop_button.setText(\"Stop Monitoring\")\n",
    "        else:\n",
    "            if self.video_thread:\n",
    "                self.video_thread.stop()\n",
    "\n",
    "    def thread_finished(self):\n",
    "        \"\"\"Handles cleanup when the thread is done.\"\"\"\n",
    "        self.log_event(\"Video thread has finished.\")\n",
    "        self.start_stop_button.setChecked(False)\n",
    "        self.start_stop_button.setText(\"Start Monitoring\")\n",
    "        self.video_label.setText(\"Monitoring Stopped.\")\n",
    "        self.video_label.setStyleSheet(\"background-color: black; color: white; border: 2px solid #555;\")\n",
    "        self.update_status(\"IDLE\", \"gray\")\n",
    "\n",
    "    def closeEvent(self, event):\n",
    "        \"\"\"Ensure the thread is stopped when the window is closed.\"\"\"\n",
    "        self.log_event(\"Application closing...\")\n",
    "        if self.video_thread and self.video_thread.isRunning():\n",
    "            self.video_thread.stop()\n",
    "        event.accept()\n",
    "\n",
    "    # --- Slot Functions ---\n",
    "\n",
    "    @pyqtSlot(np.ndarray)\n",
    "    def update_image(self, cv_img):\n",
    "        qt_img = self.convert_cv_to_qt(cv_img)\n",
    "        self.video_label.setPixmap(qt_img)\n",
    "\n",
    "    @pyqtSlot(str, str)\n",
    "    def update_status(self, status_text, color):\n",
    "        self.status_label.setText(f\"Status: {status_text}\")\n",
    "        \n",
    "        if color in (\"red\", \"darkred\"):\n",
    "            if not self.flash_timer.isActive():\n",
    "                self.is_flashing = True\n",
    "                self.flash_colors = (\"red\", \"yellow\") if color == \"red\" else (\"darkred\", \"white\")\n",
    "                self.flash_index = 0\n",
    "                self.flash_timer.start()\n",
    "                self.toggle_flash()\n",
    "        else:\n",
    "            self.is_flashing = False\n",
    "            self.flash_timer.stop()\n",
    "            text_color = \"black\" if color in (\"lightgreen\", \"orange\", \"yellow\") else \"white\"\n",
    "            self.status_label.setStyleSheet(f\"background-color: {color}; color: {text_color}; padding: 10px;\")\n",
    "    \n",
    "    def toggle_flash(self):\n",
    "        if self.is_flashing:\n",
    "            color = self.flash_colors[self.flash_index]\n",
    "            text_color = \"black\" if color == \"yellow\" else \"white\"\n",
    "            self.status_label.setStyleSheet(\n",
    "                f\"background-color: {color}; color: {text_color}; \"\n",
    "                f\"padding: 10px; border: 2px solid {self.flash_colors[1]};\"\n",
    "            )\n",
    "            self.flash_index = (self.flash_index + 1) % 2\n",
    "        else:\n",
    "            self.flash_timer.stop()\n",
    "\n",
    "    @pyqtSlot(str)\n",
    "    def log_event(self, message):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self.log_box.append(f\"[{timestamp}] {message}\")\n",
    "        self.log_box.verticalScrollBar().setValue(self.log_box.verticalScrollBar().maximum())\n",
    "\n",
    "    @pyqtSlot(int)\n",
    "    def trigger_alert(self, tier):\n",
    "        \"\"\"Triggers the alert sound using the non-blocking QMediaPlayer.\"\"\"\n",
    "        if tier == 1:\n",
    "            sound_file = SOUND_TIER_1_FILE\n",
    "        elif tier == 2:\n",
    "            sound_file = SOUND_TIER_2_FILE\n",
    "        elif tier == 3:\n",
    "            sound_file = SOUND_TIER_3_FILE\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "        file_path = os.path.join(os.path.dirname(__file__), sound_file)\n",
    "        if not os.path.exists(file_path):\n",
    "            self.log_event(f\"Sound file not found: {sound_file}\")\n",
    "            return\n",
    "            \n",
    "        self.player.setSource(QUrl.fromLocalFile(file_path))\n",
    "        self.player.play()\n",
    "\n",
    "    def convert_cv_to_qt(self, cv_img):\n",
    "        \"\"\"Converts an OpenCV image (BGR) to a QPixmap (RGB).\"\"\"\n",
    "        rgb_image = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = rgb_image.shape\n",
    "        bytes_per_line = ch * w\n",
    "        convert_to_Qt_format = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)\n",
    "        \n",
    "        scaled_pixmap = QPixmap.fromImage(convert_to_Qt_format).scaled(\n",
    "            self.video_label.size(), \n",
    "            Qt.AspectRatioMode.KeepAspectRatio, \n",
    "            Qt.TransformationMode.SmoothTransformation\n",
    "        )\n",
    "        return scaled_pixmap\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    main_window = MainWindow()\n",
    "    main_window.show()\n",
    "    sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91562895",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\venv3114\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Smart ATM Guardian - Version 5 (FINAL)\n",
    "\"Crop-then-Process\" Architecture\n",
    "\n",
    "This is the correct, stable, and efficient solution. It isolates the guard\n",
    "by defining a \"Guard Post\" ROI and only running analysis on that small,\n",
    "cropped section of the video. It ignores all other people.\n",
    "\n",
    "All previous bugs have been fixed.\n",
    "\n",
    "Dependencies:\n",
    "pip install PyQt6 opencv-python mediapipe\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# --- PyQt6 Imports ---\n",
    "from PyQt6.QtWidgets import (\n",
    "    QApplication, QWidget, QLabel, QPushButton, \n",
    "    QVBoxLayout, QHBoxLayout, QTextEdit\n",
    ")\n",
    "from PyQt6.QtCore import (\n",
    "    QThread, pyqtSignal, pyqtSlot, Qt, QTimer, QUrl\n",
    ")\n",
    "from PyQt6.QtGui import QImage, QPixmap, QColor, QFont\n",
    "from PyQt6.QtMultimedia import QMediaPlayer, QAudioOutput \n",
    "\n",
    "# === Configuration Constants ===\n",
    "\n",
    "# --- Video Source ---\n",
    "VIDEO_SOURCE = 0 \n",
    "\n",
    "# --- (CRITICAL) GUARD POST ROI ---\n",
    "# This is the \"registered\" zone for the guard.\n",
    "# Format: [x1, y1, x2, y2]\n",
    "# TUNE THIS: Set to a box around the guard's chair/desk\n",
    "GUARD_POST_ROI = [100, 50, 500, 450] # Example: (x1, y1, x2, y2)\n",
    "\n",
    "# --- Detection Thresholds (in seconds) ---\n",
    "EYES_CLOSED_DURATION = 3.0    # Drowsiness: Eyes closed for 3s\n",
    "HEAD_SLUMP_DURATION = 10.0    # Drowsiness: Head slumped for 10s\n",
    "INATTENTIVE_DURATION = 60.0   # Inattentiveness: Head turned for 60s\n",
    "ABSENCE_DURATION = 120.0  # Absence: Guard missing for 2 mins\n",
    "\n",
    "# --- Alert Escalation Timers (in seconds) ---\n",
    "TIER_1_TO_TIER_2 = 15.0  # Time from Warning (Tier 1) to Alarm (Tier 2)\n",
    "TIER_2_TO_TIER_3 = 30.0  # Time from Alarm (Tier 2) to Escalation (Tier 3)\n",
    "\n",
    "# --- Detection Logic Thresholds ---\n",
    "EAR_THRESHOLD = 0.21              \n",
    "HEAD_SLUMP_THRESHOLD = 0.15       \n",
    "HEAD_TURN_THRESHOLD_X = 0.25      \n",
    "\n",
    "# --- MediaPipe Imports ---\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# --- Sound Files ---\n",
    "SOUND_TIER_1_FILE = 'gentle_beep.wav'\n",
    "SOUND_TIER_2_FILE = 'wake_up_alarm.wav'\n",
    "SOUND_TIER_3_FILE = 'supervisor_alert.wav'\n",
    "\n",
    "# Indices for MediaPipe Face Mesh (for EAR calculation)\n",
    "EYE_INDICES = {\n",
    "    \"LEFT\": [362, 385, 387, 263, 373, 380],\n",
    "    \"RIGHT\": [33, 160, 158, 133, 153, 144]\n",
    "}\n",
    "\n",
    "# Indices for MediaPipe Pose (for posture analysis)\n",
    "POSE_INDICES = {\n",
    "    \"NOSE\": mp_pose.PoseLandmark.NOSE,\n",
    "    \"LEFT_SHOULDER\": mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "    \"RIGHT_SHOULDER\": mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "}\n",
    "\n",
    "\n",
    "class VideoThread(QThread):\n",
    "    \"\"\"\n",
    "    Worker thread for handling OpenCV video capture and MediaPipe processing.\n",
    "    \"\"\"\n",
    "    change_pixmap_signal = pyqtSignal(np.ndarray)\n",
    "    update_status_signal = pyqtSignal(str, str)\n",
    "    log_event_signal = pyqtSignal(str)\n",
    "    trigger_alert_signal = pyqtSignal(int)\n",
    "\n",
    "    def __init__(self, video_source):\n",
    "        super().__init__()\n",
    "        self.video_source = video_source\n",
    "        self._is_running = True\n",
    "        self.current_state = \"ATTENTIVE\" \n",
    "        self.alert_level = 0             \n",
    "\n",
    "        # Detection duration timers\n",
    "        self.eyes_closed_start_time = None\n",
    "        self.head_slump_start_time = None\n",
    "        self.head_turned_start_time = None\n",
    "        self.absence_start_time = None\n",
    "        \n",
    "        # Alert escalation timers\n",
    "        self.tier_1_start_time = None\n",
    "        self.tier_2_start_time = None\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main processing loop\"\"\"\n",
    "        \n",
    "        # Initialize ONE of each model.\n",
    "        # They are now single-person detectors, which is correct.\n",
    "        with mp_pose.Pose(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as pose, \\\n",
    "             mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as face_mesh:\n",
    "\n",
    "            cap = cv2.VideoCapture(self.video_source)\n",
    "            if not cap.isOpened():\n",
    "                self.log_event_signal.emit(f\"Error: Could not open video source '{self.video_source}'\")\n",
    "                return\n",
    "\n",
    "            while self._is_running:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    self.log_event_signal.emit(\"Video feed ended or error.\")\n",
    "                    break\n",
    "                \n",
    "                # === V5 \"Crop-then-Process\" Architecture ===\n",
    "                \n",
    "                # 1. Define ROI coordinates\n",
    "                x1, y1, x2, y2 = GUARD_POST_ROI\n",
    "                \n",
    "                # 2. Crop the frame to *only* the Guard Post\n",
    "                # Add padding to ROI definition to avoid crash\n",
    "                frame_h, frame_w = frame.shape[:2]\n",
    "                x1 = max(0, x1)\n",
    "                y1 = max(0, y1)\n",
    "                x2 = min(frame_w, x2)\n",
    "                y2 = min(frame_h, y2)\n",
    "\n",
    "                crop_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # 3. Analyze *only* the crop\n",
    "                if crop_img.size == 0:\n",
    "                    # If ROI is invalid (e.g., [0,0,0,0]), skip processing\n",
    "                    self.log_event_signal.emit(\"Error: GUARD_POST_ROI is invalid or outside frame.\")\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "\n",
    "                crop_rgb = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)\n",
    "                crop_rgb.flags.writeable = False\n",
    "                \n",
    "                pose_results = pose.process(crop_rgb)\n",
    "                face_results = face_mesh.process(crop_rgb)\n",
    "                \n",
    "                crop_rgb.flags.writeable = True\n",
    "                \n",
    "                # 4. Determine state based *only* on the crop\n",
    "                guard_pose = pose_results.pose_landmarks\n",
    "                guard_face = face_results.multi_face_landmarks\n",
    "                now = time.time()\n",
    "                new_state = \"ATTENTIVE\" # Default if present\n",
    "\n",
    "                if guard_pose:\n",
    "                    # A person is IN the box. This is our guard.\n",
    "                    self.absence_start_time = None # Reset absence timer\n",
    "\n",
    "                    # --- Run Multi-Aspect Analysis ---\n",
    "                    is_slumped = self.check_head_slump(guard_pose)\n",
    "                    is_turned = self.check_head_turn(guard_pose)\n",
    "                    is_eyes_closed = False\n",
    "\n",
    "                    if guard_face:\n",
    "                        face_lm = guard_face[0].landmark\n",
    "                        ear = self.calculate_ear(face_lm)\n",
    "                        if ear < EAR_THRESHOLD:\n",
    "                            is_eyes_closed = True\n",
    "                        \n",
    "                        # Draw EAR on the *crop*\n",
    "                        cv2.putText(crop_img, f\"EAR: {ear:.2f}\", (10, 30), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                        \n",
    "                        # Draw Face Mesh on the *crop*\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            image=crop_img,\n",
    "                            landmark_list=guard_face[0],\n",
    "                            connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                            landmark_drawing_spec=None,\n",
    "                            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "                    \n",
    "                    # --- State Detection Logic ---\n",
    "                    if is_eyes_closed or is_slumped:\n",
    "                        # Use appropriate timer\n",
    "                        if is_eyes_closed:\n",
    "                            if self.eyes_closed_start_time is None: self.eyes_closed_start_time = now\n",
    "                            if now - self.eyes_closed_start_time > EYES_CLOSED_DURATION: new_state = \"DROWSY\"\n",
    "                        if is_slumped:\n",
    "                            if self.head_slump_start_time is None: self.head_slump_start_time = now\n",
    "                            if now - self.head_slump_start_time > HEAD_SLUMP_DURATION: new_state = \"DROWSY\"\n",
    "                    else:\n",
    "                        self.eyes_closed_start_time = None\n",
    "                        self.head_slump_start_time = None\n",
    "\n",
    "                    # Inattention only matters if not drowsy\n",
    "                    if new_state != \"DROWSY\":\n",
    "                        if is_turned:\n",
    "                            if self.head_turned_start_time is None: self.head_turned_start_time = now\n",
    "                            if now - self.head_turned_start_time > INATTENTIVE_DURATION: new_state = \"INATTENTIVE\"\n",
    "                        else:\n",
    "                            self.head_turned_start_time = None\n",
    "                    \n",
    "                    # If attentive, reset all detection timers\n",
    "                    if new_state == \"ATTENTIVE\":\n",
    "                        self.eyes_closed_start_time = None\n",
    "                        self.head_slump_start_time = None\n",
    "                        self.head_turned_start_time = None\n",
    "                    \n",
    "                    # Draw Pose on the *crop*\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        crop_img, guard_pose, mp_pose.POSE_CONNECTIONS,\n",
    "                        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "                \n",
    "                else:\n",
    "                    # NO person in the box. Guard is ABSENT.\n",
    "                    new_state = \"ABSENT\"\n",
    "                    # Reset all other timers\n",
    "                    self.eyes_closed_start_time = None\n",
    "                    self.head_slump_start_time = None\n",
    "                    self.head_turned_start_time = None\n",
    "\n",
    "                # 5. Manage Alert Escalation\n",
    "                self.manage_state_and_alerts(new_state)\n",
    "\n",
    "                # 6. Paste the processed crop back into the main frame\n",
    "                frame[y1:y2, x1:x2] = crop_img\n",
    "                \n",
    "                # 7. Draw the ROI box on the main frame\n",
    "                color = (0, 255, 0) if new_state == \"ATTENTIVE\" else (0, 255, 255)\n",
    "                if self.alert_level > 0: color = (0, 0, 255)\n",
    "                \n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)\n",
    "                cv2.putText(frame, \"GUARD POST\", (x1, y1 - 10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                \n",
    "                # 8. Emit the final, annotated frame\n",
    "                self.change_pixmap_signal.emit(frame)\n",
    "\n",
    "            cap.release()\n",
    "            self.log_event_signal.emit(\"Monitoring stopped.\")\n",
    "\n",
    "    def manage_state_and_alerts(self, new_state):\n",
    "        \"\"\"Finite State Machine for managing alert escalation.\"\"\"\n",
    "        now = time.time()\n",
    "        \n",
    "        if new_state != self.current_state:\n",
    "            self.log_event_signal.emit(f\"State Change: {self.current_state} -> {new_state}\")\n",
    "            self.current_state = new_state\n",
    "            \n",
    "            if new_state == \"ATTENTIVE\":\n",
    "                self.tier_1_start_time = None\n",
    "                self.tier_2_start_time = None\n",
    "                self.alert_level = 0\n",
    "                self.update_status_signal.emit(\"Attentive (Guard)\", \"lightgreen\")\n",
    "\n",
    "        # Process Current State\n",
    "        if self.current_state == \"ABSENT\":\n",
    "            if self.absence_start_time is None:\n",
    "                self.absence_start_time = now\n",
    "            \n",
    "            if now - self.absence_start_time > ABSENCE_DURATION:\n",
    "                if self.alert_level < 3:\n",
    "                    self.alert_level = 3\n",
    "                    self.update_status_signal.emit(\"ESCALATION: Guard Absent\", \"darkred\")\n",
    "                    self.log_event_signal.emit(\"Tier 3 Alert: Guard Absent.\")\n",
    "                    self.trigger_alert_signal.emit(3)\n",
    "\n",
    "        elif self.current_state in [\"DROWSY\", \"INATTENTIVE\"]:\n",
    "            state_name = \"Drowsiness\" if self.current_state == \"DROWSY\" else \"Inattentiveness\"\n",
    "            self.absence_start_time = None # Reset absence timer\n",
    "            \n",
    "            if self.alert_level == 0:\n",
    "                self.alert_level = 1\n",
    "                self.tier_1_start_time = now\n",
    "                self.update_status_signal.emit(f\"WARNING: {state_name} Detected\", \"orange\")\n",
    "                self.log_event_signal.emit(f\"Tier 1 Alert: {state_name} Warning\")\n",
    "                self.trigger_alert_signal.emit(1)\n",
    "            \n",
    "            elif self.alert_level == 1 and self.tier_1_start_time and (now - self.tier_1_start_time > TIER_1_TO_TIER_2):\n",
    "                self.alert_level = 2\n",
    "                self.tier_2_start_time = now\n",
    "                self.update_status_signal.emit(\"ALERT: WAKE UP\", \"red\")\n",
    "                self.log_event_signal.emit(f\"Tier 2 Alert: {state_name} Alarm\")\n",
    "                self.trigger_alert_signal.emit(2)\n",
    "\n",
    "            elif self.alert_level == 2 and self.tier_2_start_time and (now - self.tier_2_start_time > TIER_2_TO_TIER_3):\n",
    "                self.alert_level = 3\n",
    "                self.update_status_signal.emit(\"ESCALATION: Unresponsive Guard\", \"darkred\")\n",
    "                self.log_event_signal.emit(\"Tier 3 Alert: Guard unresponsive.\")\n",
    "                self.trigger_alert_signal.emit(3)\n",
    "        \n",
    "        elif self.current_state == \"ATTENTIVE\":\n",
    "            self.absence_start_time = None\n",
    "            if self.alert_level != 0:\n",
    "                self.alert_level = 0\n",
    "                self.update_status_signal.emit(\"Attentive (Guard)\", \"lightgreen\")\n",
    "                self.log_event_signal.emit(\"Status: Guard is Attentive. Alerts reset.\")\n",
    "\n",
    "    # --- Helper: Detection Logic Functions ---\n",
    "    # These functions now operate on landmarks from a cropped image\n",
    "    \n",
    "    def calculate_ear(self, face_lm):\n",
    "        \"\"\"Calculates the Eye Aspect Ratio (EAR) for both eyes.\"\"\"\n",
    "        def get_ear(eye_points_indices):\n",
    "            points = [face_lm[i] for i in eye_points_indices]\n",
    "            A = np.linalg.norm([points[1].x - points[5].x, points[1].y - points[5].y])\n",
    "            B = np.linalg.norm([points[2].x - points[4].x, points[2].y - points[4].y])\n",
    "            C = np.linalg.norm([points[0].x - points[3].x, points[0].y - points[3].y])\n",
    "            if C == 0: return 0.4\n",
    "            return (A + B) / (2.0 * C)\n",
    "\n",
    "        return (get_ear(EYE_INDICES[\"LEFT\"]) + get_ear(EYE_INDICES[\"RIGHT\"])) / 2.0\n",
    "        \n",
    "    def check_head_slump(self, lm_pose):\n",
    "        \"\"\"Checks if the head is slumped down (for sleep or phone).\"\"\"\n",
    "        nose = lm_pose.landmark[POSE_INDICES[\"NOSE\"]]\n",
    "        ls = lm_pose.landmark[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose.landmark[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "\n",
    "        if nose.visibility < 0.5 or ls.visibility < 0.5 or rs.visibility < 0.5:\n",
    "            return False\n",
    "\n",
    "        nose_y = nose.y\n",
    "        shoulder_mid_y = (ls.y + rs.y) / 2.0\n",
    "        shoulder_width_norm = abs(ls.x - rs.x)\n",
    "        \n",
    "        if shoulder_width_norm == 0: return False\n",
    "        return nose_y > shoulder_mid_y + (shoulder_width_norm * HEAD_SLUMP_THRESHOLD)\n",
    "\n",
    "    def check_head_turn(self, lm_pose):\n",
    "        \"\"\"Checks if the head is turned away (left/right).\"\"\"\n",
    "        nose = lm_pose.landmark[POSE_INDICES[\"NOSE\"]]\n",
    "        ls = lm_pose.landmark[POSE_INDICES[\"LEFT_SHOULDER\"]]\n",
    "        rs = lm_pose.landmark[POSE_INDICES[\"RIGHT_SHOULDER\"]]\n",
    "\n",
    "        if nose.visibility < 0.5 or ls.visibility < 0.5 or rs.visibility < 0.5:\n",
    "            return False\n",
    "\n",
    "        shoulder_mid_x = (ls.x + rs.x) / 2.0\n",
    "        shoulder_width_norm = abs(ls.x - rs.x)\n",
    "\n",
    "        if shoulder_width_norm == 0: return False\n",
    "        turn_distance = abs(nose.x - shoulder_mid_x)\n",
    "        return turn_distance > (shoulder_width_norm * HEAD_TURN_THRESHOLD_X)\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stops the thread.\"\"\"\n",
    "        self._is_running = False\n",
    "        self.log_event_signal.emit(\"Stop signal received. Shutting down thread...\")\n",
    "        self.wait()\n",
    "\n",
    "\n",
    "class MainWindow(QWidget):\n",
    "    \"\"\"Main GUI Window (All fixes from V3 included)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Smart ATM Guardian (V5 - Crop-then-Process)\")\n",
    "        self.setGeometry(100, 100, 1000, 750)\n",
    "        \n",
    "        # --- GUI Widgets ---\n",
    "        self.video_label = QLabel(\"Connecting to video stream...\")\n",
    "        self.video_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        self.video_label.setMinimumSize(640, 480)\n",
    "        self.video_label.setStyleSheet(\"background-color: black; border: 2px solid #555;\")\n",
    "\n",
    "        # --- Flashing Timer (FIXED: Initialized first) ---\n",
    "        self.flash_timer = QTimer(self)\n",
    "        self.flash_timer.setInterval(500)\n",
    "        self.flash_timer.timeout.connect(self.toggle_flash)\n",
    "        self.is_flashing = False\n",
    "        self.flash_colors = (\"red\", \"yellow\")\n",
    "        self.flash_index = 0\n",
    "\n",
    "        self.status_label = QLabel(\"Status: IDLE\")\n",
    "        self.status_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        font = QFont(\"Arial\", 20, QFont.Weight.Bold)\n",
    "        self.status_label.setFont(font)\n",
    "        self.update_status(\"IDLE\", \"gray\") # SAFE to call now\n",
    "\n",
    "        self.start_stop_button = QPushButton(\"Start Monitoring\")\n",
    "        self.start_stop_button.setFont(QFont(\"Arial\", 12))\n",
    "        self.start_stop_button.setCheckable(True)\n",
    "        self.start_stop_button.setStyleSheet(\n",
    "            \"QPushButton { padding: 10px; background-color: #4CAF50; color: white; border: none; }\"\n",
    "            \"QPushButton:checked { background-color: #f44336; }\"\n",
    "        )\n",
    "\n",
    "        self.log_box_label = QLabel(\"Event Log:\")\n",
    "        self.log_box_label.setFont(QFont(\"Arial\", 10, QFont.Weight.Bold))\n",
    "        \n",
    "        self.log_box = QTextEdit()\n",
    "        self.log_box.setReadOnly(True)\n",
    "        self.log_box.setFont(QFont(\"Courier New\", 9))\n",
    "\n",
    "        # --- Layouts ---\n",
    "        v_layout = QVBoxLayout()\n",
    "        v_layout.addWidget(self.status_label)\n",
    "        h_layout = QHBoxLayout()\n",
    "        h_layout.addWidget(self.video_label, 2)\n",
    "        right_panel_layout = QVBoxLayout()\n",
    "        right_panel_layout.addWidget(self.start_stop_button)\n",
    "        right_panel_layout.addWidget(self.log_box_label)\n",
    "        right_panel_layout.addWidget(self.log_box)\n",
    "        h_layout.addLayout(right_panel_layout, 1)\n",
    "        v_layout.addLayout(h_layout)\n",
    "        self.setLayout(v_layout)\n",
    "\n",
    "        # --- Sound Player ---\n",
    "        self.player = QMediaPlayer()\n",
    "        self.audio_output = QAudioOutput()\n",
    "        self.player.setAudioOutput(self.audio_output)\n",
    "        self.audio_output.setVolume(1.0) \n",
    "\n",
    "        # --- Connections ---\n",
    "        self.start_stop_button.clicked.connect(self.toggle_monitoring)\n",
    "        self.video_thread = None\n",
    "        self.log_event(f\"Application Initialized. Ready to start monitoring.\")\n",
    "        self.log_event(f\"CRITICAL: Adjust GUARD_POST_ROI coordinates at top of script.\")\n",
    "\n",
    "    def toggle_monitoring(self, checked):\n",
    "        if checked:\n",
    "            if GUARD_POST_ROI is None or (GUARD_POST_ROI[2] - GUARD_POST_ROI[0] < 50):\n",
    "                self.log_event(\"Error: GUARD_POST_ROI is not set. Please edit the script.\")\n",
    "                self.start_stop_button.setChecked(False)\n",
    "                return\n",
    "            \n",
    "            self.log_event(\"Starting monitoring...\")\n",
    "            self.video_thread = VideoThread(VIDEO_SOURCE)\n",
    "            self.video_thread.change_pixmap_signal.connect(self.update_image)\n",
    "            self.video_thread.update_status_signal.connect(self.update_status)\n",
    "            self.video_thread.log_event_signal.connect(self.log_event)\n",
    "            self.video_thread.trigger_alert_signal.connect(self.trigger_alert)\n",
    "            self.video_thread.finished.connect(self.thread_finished)\n",
    "            self.video_thread.start()\n",
    "            self.start_stop_button.setText(\"Stop Monitoring\")\n",
    "        else:\n",
    "            if self.video_thread:\n",
    "                self.video_thread.stop()\n",
    "\n",
    "    def thread_finished(self):\n",
    "        \"\"\"Handles cleanup when the thread is done.\"\"\"\n",
    "        self.log_event(\"Video thread has finished.\")\n",
    "        self.start_stop_button.setChecked(False)\n",
    "        self.start_stop_button.setText(\"Start Monitoring\")\n",
    "        self.video_label.setText(\"Monitoring Stopped.\")\n",
    "        self.video_label.setStyleSheet(\"background-color: black; color: white; border: 2px solid #555;\")\n",
    "        self.update_status(\"IDLE\", \"gray\")\n",
    "\n",
    "    def closeEvent(self, event):\n",
    "        \"\"\"Ensure the thread is stopped when the window is closed.\"\"\"\n",
    "        self.log_event(\"Application closing...\")\n",
    "        if self.video_thread and self.video_thread.isRunning():\n",
    "            self.video_thread.stop()\n",
    "        event.accept()\n",
    "\n",
    "    # --- Slot Functions ---\n",
    "\n",
    "    @pyqtSlot(np.ndarray)\n",
    "    def update_image(self, cv_img):\n",
    "        qt_img = self.convert_cv_to_qt(cv_img)\n",
    "        self.video_label.setPixmap(qt_img)\n",
    "\n",
    "    @pyqtSlot(str, str)\n",
    "    def update_status(self, status_text, color):\n",
    "        self.status_label.setText(f\"Status: {status_text}\")\n",
    "        \n",
    "        if color in (\"red\", \"darkred\"):\n",
    "            if not self.flash_timer.isActive():\n",
    "                self.is_flashing = True\n",
    "                self.flash_colors = (\"red\", \"yellow\") if color == \"red\" else (\"darkred\", \"white\")\n",
    "                self.flash_index = 0\n",
    "                self.flash_timer.start()\n",
    "                self.toggle_flash()\n",
    "        else:\n",
    "            self.is_flashing = False\n",
    "            self.flash_timer.stop()\n",
    "            text_color = \"black\" if color in (\"lightgreen\", \"orange\", \"yellow\") else \"white\"\n",
    "            self.status_label.setStyleSheet(f\"background-color: {color}; color: {text_color}; padding: 10px;\")\n",
    "    \n",
    "    def toggle_flash(self):\n",
    "        if self.is_flashing:\n",
    "            color = self.flash_colors[self.flash_index]\n",
    "            text_color = \"black\" if color == \"yellow\" else \"white\"\n",
    "            self.status_label.setStyleSheet(\n",
    "                f\"background-color: {color}; color: {text_color}; \"\n",
    "                f\"padding: 10px; border: 2px solid {self.flash_colors[1]};\"\n",
    "            )\n",
    "            self.flash_index = (self.flash_index + 1) % 2\n",
    "        else:\n",
    "            self.flash_timer.stop()\n",
    "\n",
    "    @pyqtSlot(str)\n",
    "    def log_event(self, message):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self.log_box.append(f\"[{timestamp}] {message}\")\n",
    "        self.log_box.verticalScrollBar().setValue(self.log_box.verticalScrollBar().maximum())\n",
    "\n",
    "    @pyqtSlot(int)\n",
    "    def trigger_alert(self, tier):\n",
    "        \"\"\"Triggers the alert sound using the non-blocking QMediaPlayer.\"\"\"\n",
    "        if tier == 1:\n",
    "            sound_file = SOUND_TIER_1_FILE\n",
    "        elif tier == 2:\n",
    "            sound_file = SOUND_TIER_2_FILE\n",
    "        elif tier == 3:\n",
    "            sound_file = SOUND_TIER_3_FILE\n",
    "        else:\n",
    "            return\n",
    "            \n",
    "        file_path = os.path.join(os.path.dirname(__file__), sound_file)\n",
    "        if not os.path.exists(file_path):\n",
    "            self.log_event(f\"Sound file not found: {sound_file}\")\n",
    "            return\n",
    "            \n",
    "        self.player.setSource(QUrl.fromLocalFile(file_path))\n",
    "        self.player.play()\n",
    "\n",
    "    def convert_cv_to_qt(self, cv_img):\n",
    "        \"\"\"Converts an OpenCV image (BGR) to a QPixmap (RGB).\"\"\"\n",
    "        rgb_image = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = rgb_image.shape\n",
    "        bytes_per_line = ch * w\n",
    "        convert_to_Qt_format = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format.Format_RGB888)\n",
    "        \n",
    "        scaled_pixmap = QPixmap.fromImage(convert_to_Qt_format).scaled(\n",
    "            self.video_label.size(), \n",
    "            Qt.AspectRatioMode.KeepAspectRatio, \n",
    "            Qt.TransformationMode.SmoothTransformation\n",
    "        )\n",
    "        return scaled_pixmap\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    main_window = MainWindow()\n",
    "    main_window.show()\n",
    "    sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99c29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3114",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
