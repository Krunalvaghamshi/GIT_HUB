{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67360490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog  # Import simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# How many pixels of movement is \"not still\"?\n",
    "# This will need tuning based on your camera resolution and distance.\n",
    "STILLNESS_THRESHOLD_PX = 10\n",
    "\n",
    "# How many consecutive \"still\" frames trigger an alarm?\n",
    "# (e.g., 100 frames at ~20fps is ~5 seconds)\n",
    "SLEEP_ALARM_FRAMES = 100\n",
    "\n",
    "# New constant for tracking\n",
    "# If the closest-found face is further than this from the last frame,\n",
    "# we assume it's a new person or the tracker lost the original.\n",
    "MAX_TRACKING_JUMP_PX = 150\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Sleeping Alert System (Dev Phase)\")\n",
    "        self.window.geometry(\"800x700\")\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- Face Detection ---\n",
    "        # Use the built-in Haar Cascade data from OpenCV\n",
    "        try:\n",
    "            face_cascade_path = os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml')\n",
    "            if not os.path.exists(face_cascade_path):\n",
    "                raise IOError(\"Haar cascade file not found.\")\n",
    "            self.face_cascade = cv2.CascadeClassifier(face_cascade_path)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to load Haar Cascade: {e}\\nPlease ensure OpenCV is correctly installed.\")\n",
    "            self.window.destroy()\n",
    "            return\n",
    "\n",
    "        # --- \"Sleeping\" Logic State ---\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "        \n",
    "        # --- New Registration State ---\n",
    "        self.is_monitoring = False  # Are we actively monitoring a registered person?\n",
    "        self.registered_face_center_guess = None # The last known center of the monitored person\n",
    "        self.registered_person_name = None # Store the registered person's name\n",
    "        self.current_frame_for_registration = None # Temp stores frame for registration\n",
    "        self.registration_lock = threading.Lock() # Lock for accessing the frame\n",
    "\n",
    "        # --- GUI Elements ---\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "        # Video Display Label\n",
    "        self.video_label = tk.Label(self.main_frame, bg=\"black\")\n",
    "        self.video_label.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Status Label\n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.main_frame, textvariable=self.status_text, font=(\"Arial\", 14), pady=10)\n",
    "        self.status_label.pack()\n",
    "\n",
    "        # Control Buttons\n",
    "        self.button_frame = tk.Frame(self.main_frame)\n",
    "        self.button_frame.pack()\n",
    "\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # --- New Registration Buttons ---\n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.register_person, font=(\"Arial\", 12), width=18)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear Registration\", command=self.clear_registration, font=(\"Arial\", 12), width=18)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "\n",
    "        # Set up the close protocol\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        \"\"\"\n",
    "        Starts the video capture in a new thread.\n",
    "        \"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)  # Use 0 for laptop webcam\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            \n",
    "            self.is_running = True\n",
    "            \n",
    "            # Start the video processing thread\n",
    "            # daemon=True ensures the thread will close when the main app closes\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            \n",
    "            # self.start_button.config(state=tk.DISABLED)\n",
    "            # self.stop_button.config(state=tk.NORMAL)\n",
    "            # self.register_button.config(state=tk.NORMAL) # Enable registration\n",
    "            self.status_text.set(\"Camera running. Click 'Register & Monitor'.\")\n",
    "        \n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap:\n",
    "                self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        \"\"\"\n",
    "        Signals the video loop to stop.\n",
    "        \"\"\"\n",
    "        # Add guard clause\n",
    "        if not self.is_running:\n",
    "            return\n",
    "            \n",
    "        self.is_running = False\n",
    "        \n",
    "        # The thread will see self.is_running is False and exit\n",
    "        # We wait a moment for the thread to finish\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "            \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "        # self.start_button.config(state=tk.NORMAL)\n",
    "        # self.stop_button.config(state=tk.DISABLED)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        self.video_label.config(image=None) # Clear the image\n",
    "        \n",
    "        # --- Reset all states ---\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None # Clear name\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "        # self.register_button.config(state=tk.DISABLED)\n",
    "        # self.clear_button.config(state=tk.DISABLED)\n",
    "\n",
    "    def video_loop(self):\n",
    "        \"\"\"\n",
    "        The main loop for video processing. Runs in a separate thread.\n",
    "        \"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                # Flip for a \"mirror\" view, which is more intuitive\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Store a copy of the frame for the registration function\n",
    "                with self.registration_lock:\n",
    "                    self.current_frame_for_registration = frame.copy()\n",
    "                \n",
    "                # Process the frame\n",
    "                processed_frame, status = self.process_frame_logic(frame)\n",
    "\n",
    "                # Update the status text\n",
    "                self.status_text.set(status)\n",
    "\n",
    "                # Convert the OpenCV (BGR) frame to a PIL (RGB) image\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                \n",
    "                # Resize image to fit the label (optional, but good for layout)\n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1: # Avoid division by zero on init\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "\n",
    "                # Convert PIL image to Tkinter-compatible image\n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "\n",
    "                # Update the video label in the GUI\n",
    "                # This must be done from the main thread, but tkinter seems to handle this call\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            # Control loop speed slightly\n",
    "            time.sleep(0.01) # ~100fps theoretical max, but processing will slow it down\n",
    "\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    def register_person(self):\n",
    "        \"\"\"\n",
    "        Registers the largest face currently in the frame for monitoring.\n",
    "        \"\"\"\n",
    "        # --- Add Guard Clauses ---\n",
    "        if not self.is_running:\n",
    "            messagebox.showwarning(\"Error\", \"Camera is not running. Please start the camera first.\")\n",
    "            return\n",
    "        \n",
    "        if self.is_monitoring:\n",
    "            messagebox.showwarning(\"Error\", \"A person is already being monitored. Please clear the registration first.\")\n",
    "            return\n",
    "\n",
    "        with self.registration_lock:\n",
    "            if self.current_frame_for_registration is None:\n",
    "                messagebox.showwarning(\"Registration Error\", \"Camera not ready. Please try again.\")\n",
    "                return\n",
    "            \n",
    "            # Use the stored frame to find a face\n",
    "            gray = cv2.cvtColor(self.current_frame_for_registration, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.1,\n",
    "                minNeighbors=5,\n",
    "                minSize=(50, 50)\n",
    "            )\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            messagebox.showwarning(\"Registration Error\", \"No person detected in the frame. Please face the camera and try again.\")\n",
    "            return\n",
    "\n",
    "        # Register the largest face\n",
    "        # We sort by area to be sure we get the main person\n",
    "        faces_by_area = sorted(faces, key=lambda f: f[2] * f[3], reverse=True)\n",
    "        (x, y, w, h) = faces_by_area[0]\n",
    "\n",
    "        center_x = x + w // 2\n",
    "        center_y = y + h // 2\n",
    "        \n",
    "        # --- NEW: Ask for the person's name ---\n",
    "        name = simpledialog.askstring(\"Register Person\", \"Enter the person's name:\", parent=self.window)\n",
    "        \n",
    "        if not name:\n",
    "            messagebox.showwarning(\"Registration Cancelled\", \"Registration was cancelled (no name provided).\")\n",
    "            return\n",
    "        \n",
    "        # --- Lock in the registration ---\n",
    "        self.is_monitoring = True\n",
    "        self.registered_person_name = name\n",
    "        self.registered_face_center_guess = (center_x, center_y)\n",
    "        self.last_face_center = (center_x, center_y) # Start stillness check from this point\n",
    "        self.stillness_counter = 0\n",
    "\n",
    "        # Update GUI\n",
    "        # self.register_button.config(state=tk.DISABLED)\n",
    "        # self.clear_button.config(state=tk.NORMAL)\n",
    "        self.status_text.set(f\"Person registered: {self.registered_person_name}. Actively monitoring.\")\n",
    "        messagebox.showinfo(\"Registration Complete\", f\"{self.registered_person_name} registered. Monitoring has started.\")\n",
    "\n",
    "    def clear_registration(self):\n",
    "        \"\"\"\n",
    "        Clears the current registration and stops monitoring.\n",
    "        \"\"\"\n",
    "        # --- Add Guard Clause ---\n",
    "        if not self.is_monitoring:\n",
    "            messagebox.showwarning(\"Info\", \"No person is currently registered.\")\n",
    "            return\n",
    "            \n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None # Clear name\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "\n",
    "        # Update GUI\n",
    "        # if self.is_running: # Only enable if camera is on\n",
    "        #     self.register_button.config(state=tk.NORMAL)\n",
    "        # self.clear_button.config(state=tk.DISABLED)\n",
    "        self.status_text.set(\"Monitoring stopped. Ready to register a new person.\")\n",
    "\n",
    "\n",
    "    def process_frame_logic(self, frame):\n",
    "        \"\"\"\n",
    "        Detects faces and applies monitoring logic based on registration status.\n",
    "        This function replaces the old 'detect_sleeping'.\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(50, 50) # Don't detect tiny faces\n",
    "        )\n",
    "\n",
    "        # If not monitoring, just show all faces found\n",
    "        if not self.is_monitoring:\n",
    "            current_status = \"Ready to register.\"\n",
    "            if self.is_running: # Check if camera is on\n",
    "                current_status = \"Click 'Register & Monitor' to begin.\"\n",
    "            \n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2) # Light blue box\n",
    "            \n",
    "            return frame, current_status\n",
    "\n",
    "        # --- If we ARE monitoring ---\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            # We were monitoring, but now the person is gone\n",
    "            current_status = f\"MONITORING: {self.registered_person_name} lost!\"\n",
    "            self.last_face_center = None # Stop stillness counter\n",
    "            self.stillness_counter = 0\n",
    "            return frame, current_status\n",
    "\n",
    "        # Find the face closest to our last known center\n",
    "        min_dist = float('inf')\n",
    "        tracked_face = None\n",
    "        current_center = None\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            center = (x + w // 2, y + h // 2)\n",
    "            dist = np.linalg.norm(np.array(center) - np.array(self.registered_face_center_guess))\n",
    "            \n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                tracked_face = (x, y, w, h)\n",
    "                current_center = center\n",
    "        \n",
    "        # Check if the closest face is \"reasonably\" close.\n",
    "        # If not, it's probably a different person.\n",
    "        if min_dist > MAX_TRACKING_JUMP_PX:\n",
    "             # The person we found is too far from the last spot.\n",
    "             current_status = f\"MONITORING: {self.registered_person_name} lost! (New face detected)\"\n",
    "             self.last_face_center = None\n",
    "             self.stillness_counter = 0\n",
    "             # Draw a box on the *new* face\n",
    "             (x, y, w, h) = tracked_face\n",
    "             cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 165, 255), 2) # Orange box\n",
    "             return frame, current_status\n",
    "\n",
    "        # --- We have successfully re-acquired the tracked face ---\n",
    "        (x, y, w, h) = tracked_face\n",
    "        \n",
    "        # Update our guess for the next frame\n",
    "        self.registered_face_center_guess = current_center\n",
    "        \n",
    "        current_status = f\"Monitoring: {self.registered_person_name} (Active)\"\n",
    "        alert_triggered = False\n",
    "\n",
    "        # Compare with the last known center\n",
    "        if self.last_face_center is not None:\n",
    "            dist_moved = np.linalg.norm(np.array(current_center) - np.array(self.last_face_center))\n",
    "\n",
    "            if dist_moved < STILLNESS_THRESHOLD_PX:\n",
    "                # Person is still\n",
    "                self.stillness_counter += 1\n",
    "            else:\n",
    "                # Person moved, reset counter\n",
    "                self.stillness_counter = 0\n",
    "        \n",
    "        # Update the last known center for the *next* frame's comparison\n",
    "        self.last_face_center = current_center\n",
    "\n",
    "        # Check if the stillness has crossed the alarm threshold\n",
    "        if self.stillness_counter > SLEEP_ALARM_FRAMES:\n",
    "            current_status = f\"!!! ALERT: {self.registered_person_name} IS SLEEPING !!!\"\n",
    "            alert_triggered = True\n",
    "        \n",
    "        # --- Draw on the frame ---\n",
    "        if alert_triggered:\n",
    "            # Draw a bright red box for alert\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "            cv2.putText(frame, f\"ALERT: {self.registered_person_name}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "        else:\n",
    "            # Draw a standard blue box for the tracked person\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, self.registered_person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f\"Stillness: {self.stillness_counter}/{SLEEP_ALARM_FRAMES}\", (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "        return frame, current_status\n",
    "\n",
    "\n",
    "    def detect_sleeping(self, frame):\n",
    "        \"\"\"\n",
    "        This function is no longer called directly by the video_loop.\n",
    "        Its logic has been moved into process_frame_logic.\n",
    "        \"\"\"\n",
    "        pass # Kept to show what was replaced\n",
    "        \n",
    "\n",
    "    def on_closing(self):\n",
    "        \"\"\"\n",
    "        Handles the window close event.\n",
    "        \"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac789159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog  # Import simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# How many pixels of movement is \"not still\"?\n",
    "# This will need tuning based on your camera resolution and distance.\n",
    "STILLNESS_THRESHOLD_PX = 10\n",
    "\n",
    "# How many consecutive \"still\" frames trigger an alarm?\n",
    "# (e.g., 100 frames at ~20fps is ~5 seconds)\n",
    "SLEEP_ALARM_FRAMES = 100\n",
    "\n",
    "# New constant for tracking\n",
    "# If the closest-found face is further than this from the last frame,\n",
    "# we assume it's a new person or the tracker lost the original.\n",
    "MAX_TRACKING_JUMP_PX = 150\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Sleeping Alert System (Dev Phase)\")\n",
    "        self.window.geometry(\"800x700\")\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- Face Detection ---\n",
    "        # Use the built-in Haar Cascade data from OpenCV\n",
    "        try:\n",
    "            face_cascade_path = os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_alt2.xml') # Using 'alt2' model for better performance\n",
    "            if not os.path.exists(face_cascade_path):\n",
    "                raise IOError(\"Haar cascade file not found.\")\n",
    "            self.face_cascade = cv2.CascadeClassifier(face_cascade_path)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to load Haar Cascade: {e}\\nPlease ensure OpenCV is correctly installed.\")\n",
    "            self.window.destroy()\n",
    "            return\n",
    "\n",
    "        # --- \"Sleeping\" Logic State ---\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "        \n",
    "        # --- New Registration State ---\n",
    "        self.is_monitoring = False  # Are we actively monitoring a registered person?\n",
    "        self.registered_face_center_guess = None # The last known center of the monitored person\n",
    "        self.registered_person_name = None # Store the registered person's name\n",
    "        self.current_frame_for_registration = None # Temp stores frame for registration\n",
    "        self.registration_lock = threading.Lock() # Lock for accessing the frame\n",
    "\n",
    "        # --- GUI Elements ---\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "        # --- GUI Layout Fix ---\n",
    "        # Pack non-expanding widgets (buttons, status) to the BOTTOM first.\n",
    "        # This anchors them to the bottom of the main_frame.\n",
    "        \n",
    "        # Control Buttons\n",
    "        self.button_frame = tk.Frame(self.main_frame)\n",
    "        self.button_frame.pack(side=tk.BOTTOM, pady=10) # Anchor to bottom\n",
    "\n",
    "        # Status Label\n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.main_frame, textvariable=self.status_text, font=(\"Arial\", 14))\n",
    "        self.status_label.pack(side=tk.BOTTOM, pady=5) # Anchor to bottom, above buttons\n",
    "\n",
    "        # Video Display Label\n",
    "        # This is packed LAST, so it expands to fill all remaining space.\n",
    "        self.video_label = tk.Label(self.main_frame, bg=\"black\")\n",
    "        self.video_label.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # --- Original Button/Status placement (removed) ---\n",
    "        # self.video_label.pack(fill=tk.BOTH, expand=True)\n",
    "        # self.status_label.pack(pady=10)\n",
    "        # self.button_frame.pack()\n",
    "        # --- End of Original Placement ---\n",
    "\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # --- New Registration Buttons ---\n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.register_person, font=(\"Arial\", 12), width=18)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear Registration\", command=self.clear_registration, font=(\"Arial\", 12), width=18)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "\n",
    "        # Set up the close protocol\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        \"\"\"\n",
    "        Starts the video capture in a new thread.\n",
    "        \"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)  # Use 0 for laptop webcam\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            \n",
    "            self.is_running = True\n",
    "            \n",
    "            # Start the video processing thread\n",
    "            # daemon=True ensures the thread will close when the main app closes\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            \n",
    "            # self.start_button.config(state=tk.DISABLED)\n",
    "            # self.stop_button.config(state=tk.NORMAL)\n",
    "            # self.register_button.config(state=tk.NORMAL) # Enable registration\n",
    "            self.status_text.set(\"Camera running. Click 'Register & Monitor'.\")\n",
    "        \n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap:\n",
    "                self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        \"\"\"\n",
    "        Signals the video loop to stop.\n",
    "        \"\"\"\n",
    "        # Add guard clause\n",
    "        if not self.is_running:\n",
    "            return\n",
    "            \n",
    "        self.is_running = False\n",
    "        \n",
    "        # The thread will see self.is_running is False and exit\n",
    "        # We wait a moment for the thread to finish\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "            \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "        # self.start_button.config(state=tk.NORMAL)\n",
    "        # self.stop_button.config(state=tk.DISABLED)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        self.video_label.config(image=None) # Clear the image\n",
    "        \n",
    "        # --- Reset all states ---\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None # Clear name\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "        # self.register_button.config(state=tk.DISABLED)\n",
    "        # self.clear_button.config(state=tk.DISABLED)\n",
    "\n",
    "    def video_loop(self):\n",
    "        \"\"\"\n",
    "        The main loop for video processing. Runs in a separate thread.\n",
    "        \"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                # Flip for a \"mirror\" view, which is more intuitive\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Store a copy of the frame for the registration function\n",
    "                with self.registration_lock:\n",
    "                    self.current_frame_for_registration = frame.copy()\n",
    "                \n",
    "                # Process the frame\n",
    "                processed_frame, status = self.process_frame_logic(frame)\n",
    "\n",
    "                # Update the status text\n",
    "                self.status_text.set(status)\n",
    "\n",
    "                # Convert the OpenCV (BGR) frame to a PIL (RGB) image\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                \n",
    "                # Resize image to fit the label (optional, but good for layout)\n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1: # Avoid division by zero on init\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "\n",
    "                # Convert PIL image to Tkinter-compatible image\n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "\n",
    "                # Update the video label in the GUI\n",
    "                # This must be done from the main thread, but tkinter seems to handle this call\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            # Control loop speed slightly\n",
    "            time.sleep(0.01) # ~100fps theoretical max, but processing will slow it down\n",
    "\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    def register_person(self):\n",
    "        \"\"\"\n",
    "        Registers the largest face currently in the frame for monitoring.\n",
    "        \"\"\"\n",
    "        # --- Add Guard Clauses ---\n",
    "        if not self.is_running:\n",
    "            messagebox.showwarning(\"Error\", \"Camera is not running. Please start the camera first.\")\n",
    "            return\n",
    "        \n",
    "        if self.is_monitoring:\n",
    "            messagebox.showwarning(\"Error\", \"A person is already being monitored. Please clear the registration first.\")\n",
    "            return\n",
    "\n",
    "        with self.registration_lock:\n",
    "            if self.current_frame_for_registration is None:\n",
    "                messagebox.showwarning(\"Registration Error\", \"Camera not ready. Please try again.\")\n",
    "                return\n",
    "            \n",
    "            # Use the stored frame to find a face\n",
    "            gray = cv2.cvtColor(self.current_frame_for_registration, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.1,\n",
    "                minNeighbors=4,  # Tuned parameter (was 5)\n",
    "                minSize=(40, 40) # Tuned parameter (was 50, 50)\n",
    "            )\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            messagebox.showwarning(\"Registration Error\", \"No person detected in the frame. Please face the camera and try again.\")\n",
    "            return\n",
    "\n",
    "        # Register the largest face\n",
    "        # We sort by area to be sure we get the main person\n",
    "        faces_by_area = sorted(faces, key=lambda f: f[2] * f[3], reverse=True)\n",
    "        (x, y, w, h) = faces_by_area[0]\n",
    "\n",
    "        center_x = x + w // 2\n",
    "        center_y = y + h // 2\n",
    "        \n",
    "        # --- NEW: Ask for the person's name ---\n",
    "        name = simpledialog.askstring(\"Register Person\", \"Enter the person's name:\", parent=self.window)\n",
    "        \n",
    "        if not name:\n",
    "            messagebox.showwarning(\"Registration Cancelled\", \"Registration was cancelled (no name provided).\")\n",
    "            return\n",
    "        \n",
    "        # --- Lock in the registration ---\n",
    "        self.is_monitoring = True\n",
    "        self.registered_person_name = name\n",
    "        self.registered_face_center_guess = (center_x, center_y)\n",
    "        self.last_face_center = (center_x, center_y) # Start stillness check from this point\n",
    "        self.stillness_counter = 0\n",
    "\n",
    "        # Update GUI\n",
    "        # self.register_button.config(state=tk.DISABLED)\n",
    "        # self.clear_button.config(state=tk.NORMAL)\n",
    "        self.status_text.set(f\"Person registered: {self.registered_person_name}. Actively monitoring.\")\n",
    "        messagebox.showinfo(\"Registration Complete\", f\"{self.registered_person_name} registered. Monitoring has started.\")\n",
    "\n",
    "    def clear_registration(self):\n",
    "        \"\"\"\n",
    "        Clears the current registration and stops monitoring.\n",
    "        \"\"\"\n",
    "        # --- Add Guard Clause ---\n",
    "        if not self.is_monitoring:\n",
    "            messagebox.showwarning(\"Info\", \"No person is currently registered.\")\n",
    "            return\n",
    "            \n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None # Clear name\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "\n",
    "        # Update GUI\n",
    "        # if self.is_running: # Only enable if camera is on\n",
    "        #     self.register_button.config(state=tk.NORMAL)\n",
    "        # self.clear_button.config(state=tk.DISABLED)\n",
    "        self.status_text.set(\"Monitoring stopped. Ready to register a new person.\")\n",
    "\n",
    "\n",
    "    def process_frame_logic(self, frame):\n",
    "        \"\"\"\n",
    "        Detects faces and applies monitoring logic based on registration status.\n",
    "        This function replaces the old 'detect_sleeping'.\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=4,  # Tuned parameter (was 5)\n",
    "            minSize=(40, 40) # Tuned parameter (was 50, 50)\n",
    "        )\n",
    "\n",
    "        # If not monitoring, just show all faces found\n",
    "        if not self.is_monitoring:\n",
    "            current_status = \"Ready to register.\"\n",
    "            if self.is_running: # Check if camera is on\n",
    "                current_status = \"Click 'Register & Monitor' to begin.\"\n",
    "            \n",
    "            for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2) # Light blue box\n",
    "            \n",
    "            return frame, current_status\n",
    "\n",
    "        # --- If we ARE monitoring ---\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            # We were monitoring, but now the person is gone\n",
    "            current_status = f\"MONITORING: {self.registered_person_name} lost!\"\n",
    "            self.last_face_center = None # Stop stillness counter\n",
    "            self.stillness_counter = 0\n",
    "            return frame, current_status\n",
    "\n",
    "        # Find the face closest to our last known center\n",
    "        min_dist = float('inf')\n",
    "        tracked_face = None\n",
    "        current_center = None\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            center = (x + w // 2, y + h // 2)\n",
    "            dist = np.linalg.norm(np.array(center) - np.array(self.registered_face_center_guess))\n",
    "            \n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                tracked_face = (x, y, w, h)\n",
    "                current_center = center\n",
    "        \n",
    "        # Check if the closest face is \"reasonably\" close.\n",
    "        # If not, it's probably a different person.\n",
    "        if min_dist > MAX_TRACKING_JUMP_PX:\n",
    "             # The person we found is too far from the last spot.\n",
    "             current_status = f\"MONITORING: {self.registered_person_name} lost! (New face detected)\"\n",
    "             self.last_face_center = None\n",
    "             self.stillness_counter = 0\n",
    "             # Draw a box on the *new* face\n",
    "             (x, y, w, h) = tracked_face\n",
    "             cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 165, 255), 2) # Orange box\n",
    "             return frame, current_status\n",
    "\n",
    "        # --- We have successfully re-acquired the tracked face ---\n",
    "        (x, y, w, h) = tracked_face\n",
    "        \n",
    "        # Update our guess for the next frame\n",
    "        self.registered_face_center_guess = current_center\n",
    "        \n",
    "        current_status = f\"Monitoring: {self.registered_person_name} (Active)\"\n",
    "        alert_triggered = False\n",
    "\n",
    "        # Compare with the last known center\n",
    "        if self.last_face_center is not None:\n",
    "            dist_moved = np.linalg.norm(np.array(current_center) - np.array(self.last_face_center))\n",
    "\n",
    "            if dist_moved < STILLNESS_THRESHOLD_PX:\n",
    "                # Person is still\n",
    "                self.stillness_counter += 1\n",
    "            else:\n",
    "                # Person moved, reset counter\n",
    "                self.stillness_counter = 0\n",
    "        \n",
    "        # Update the last known center for the *next* frame's comparison\n",
    "        self.last_face_center = current_center\n",
    "\n",
    "        # Check if the stillness has crossed the alarm threshold\n",
    "        if self.stillness_counter > SLEEP_ALARM_FRAMES:\n",
    "            current_status = f\"!!! ALERT: {self.registered_person_name} IS SLEEPING !!!\"\n",
    "            alert_triggered = True\n",
    "        \n",
    "        # --- Draw on the frame ---\n",
    "        if alert_triggered:\n",
    "            # Draw a bright red box for alert\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "            cv2.putText(frame, f\"ALERT: {self.registered_person_name}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "        else:\n",
    "            # Draw a standard blue box for the tracked person\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, self.registered_person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f\"Stillness: {self.stillness_counter}/{SLEEP_ALARM_FRAMES}\", (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "        return frame, current_status\n",
    "\n",
    "\n",
    "    def detect_sleeping(self, frame):\n",
    "        \"\"\"\n",
    "        This function is no longer called directly by the video_loop.\n",
    "        Its logic has been moved into process_frame_logic.\n",
    "        \"\"\"\n",
    "        pass # Kept to show what was replaced\n",
    "        \n",
    "\n",
    "    def on_closing(self):\n",
    "        \"\"\"\n",
    "        Handles the window close event.\n",
    "        \"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp  # --- Import MediaPipe ---\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# How many pixels of movement is \"not still\"?\n",
    "# This will need tuning based on your camera resolution and distance.\n",
    "STILLNESS_THRESHOLD_PX = 10\n",
    "\n",
    "# How many consecutive \"still\" frames trigger an alarm?\n",
    "# (e.g., 100 frames at ~20fps is ~5 seconds)\n",
    "SLEEP_ALARM_FRAMES = 100\n",
    "\n",
    "# New constant for tracking\n",
    "# If the closest-found face is further than this from the last frame,\n",
    "# we assume it's a new person or the tracker lost the original.\n",
    "MAX_TRACKING_JUMP_PX = 150\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Sleeping Alert System (Dev Phase)\")\n",
    "        self.window.geometry(\"800x700\")\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- Face Detection (REPLACED) ---\n",
    "        # We now use MediaPipe Face Mesh, which is far more accurate.\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=5,  # Detect up to 5 faces\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # --- \"Sleeping\" Logic State ---\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "        \n",
    "        # --- New Registration State ---\n",
    "        self.is_monitoring = False  # Are we actively monitoring a registered person?\n",
    "        self.registered_face_center_guess = None # The last known center of the monitored person\n",
    "        self.registered_person_name = None # Store the registered person's name\n",
    "        self.current_frame_for_registration = None # Temp stores frame for registration\n",
    "        self.registration_lock = threading.Lock() # Lock for accessing the frame\n",
    "\n",
    "        # --- GUI Elements ---\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "        # --- GUI Layout Fix ---\n",
    "        # Pack non-expanding widgets (buttons, status) to the BOTTOM first.\n",
    "        # This anchors them to the bottom of the main_frame.\n",
    "        \n",
    "        # Control Buttons\n",
    "        self.button_frame = tk.Frame(self.main_frame)\n",
    "        self.button_frame.pack(side=tk.BOTTOM, pady=10) # Anchor to bottom\n",
    "\n",
    "        # Status Label\n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.main_frame, textvariable=self.status_text, font=(\"Arial\", 14))\n",
    "        self.status_label.pack(side=tk.BOTTOM, pady=5) # Anchor to bottom, above buttons\n",
    "\n",
    "        # Video Display Label\n",
    "        # This is packed LAST, so it expands to fill all remaining space.\n",
    "        self.video_label = tk.Label(self.main_frame, bg=\"black\")\n",
    "        self.video_label.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # --- New Registration Buttons ---\n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.register_person, font=(\"Arial\", 12), width=18)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear Registration\", command=self.clear_registration, font=(\"Arial\", 12), width=18)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "\n",
    "        # Set up the close protocol\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        \"\"\"\n",
    "        Starts the video capture in a new thread.\n",
    "        \"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)  # Use 0 for laptop webcam\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            \n",
    "            self.is_running = True\n",
    "            \n",
    "            # Start the video processing thread\n",
    "            # daemon=True ensures the thread will close when the main app closes\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            \n",
    "            self.status_text.set(\"Camera running. Click 'Register & Monitor'.\")\n",
    "        \n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap:\n",
    "                self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        \"\"\"\n",
    "        Signals the video loop to stop.\n",
    "        \"\"\"\n",
    "        # Add guard clause\n",
    "        if not self.is_running:\n",
    "            return\n",
    "            \n",
    "        self.is_running = False\n",
    "        \n",
    "        # The thread will see self.is_running is False and exit\n",
    "        # We wait a moment for the thread to finish\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "            \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        self.video_label.config(image=None) # Clear the image\n",
    "        \n",
    "        # --- Reset all states ---\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None # Clear name\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "\n",
    "    def video_loop(self):\n",
    "        \"\"\"\n",
    "        The main loop for video processing. Runs in a separate thread.\n",
    "        \"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                # Flip for a \"mirror\" view, which is more intuitive\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Store a copy of the frame for the registration function\n",
    "                with self.registration_lock:\n",
    "                    self.current_frame_for_registration = frame.copy()\n",
    "                \n",
    "                # --- FIX: All logic below was moved inside the try block ---\n",
    "\n",
    "                # MediaPipe needs RGB, but OpenCV provides BGR.\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Process the frame\n",
    "                processed_frame, status = self.process_frame_logic(frame, rgb_frame) # Pass rgb_frame\n",
    "\n",
    "                # Update the status text\n",
    "                self.status_text.set(status)\n",
    "\n",
    "                # Convert the OpenCV (BGR) frame to a PIL (RGB) image\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                \n",
    "                # Resize image to fit the label (optional, but good for layout)\n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1: # Avoid division by zero on init\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "\n",
    "                # Convert PIL image to Tkinter-compatible image\n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "\n",
    "                # Update the video label in the GUI\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            # Control loop speed slightly\n",
    "            time.sleep(0.01) # ~100fps theoretical max, but processing will slow it down\n",
    "\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    # --- Helper function to get face data from MediaPipe results ---\n",
    "    # --- FIX: This function was completely rebuilt from the corrupted version ---\n",
    "    def get_faces_from_results(self, frame, results):\n",
    "        \"\"\"\n",
    "        Extracts bounding boxes and nose-tip centers from MediaPipe results.\n",
    "        \"\"\"\n",
    "        faces_data = []\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # --- 1. Get Bounding Box ---\n",
    "                x_min, y_min = w, h\n",
    "                x_max, y_max = 0, 0\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    if x < x_min: x_min = x\n",
    "                    if y < y_min: y_min = y\n",
    "                    if x > x_max: x_max = x\n",
    "                    if y > y_max: y_max = y\n",
    "                \n",
    "                # Add a little padding\n",
    "                x_min = max(0, x_min - 10)\n",
    "                y_min = max(0, y_min - 10)\n",
    "                x_max = min(w, x_max + 10)\n",
    "                y_max = min(h, y_max + 10)\n",
    "\n",
    "                box = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "\n",
    "                # --- 2. Get Nose Tip (Landmark 1) for precise tracking ---\n",
    "                # This is our new \"center\" for movement tracking\n",
    "                nose_tip = face_landmarks.landmark[1]\n",
    "                center = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "                \n",
    "                faces_data.append({'box': box, 'center': center})\n",
    "        \n",
    "        return faces_data\n",
    "\n",
    "    def register_person(self):\n",
    "        \"\"\"\n",
    "        Registers the largest face currently in the frame for monitoring.\n",
    "        \"\"\"\n",
    "        # --- FIX: Removed corrupted code lines from here ---\n",
    "\n",
    "        with self.registration_lock:\n",
    "            if self.current_frame_for_registration is None:\n",
    "                messagebox.showwarning(\"Registration Error\", \"Camera not ready. Please try again.\")\n",
    "                return\n",
    "            \n",
    "            # Use the stored frame to find a face\n",
    "            # MediaPipe needs RGB\n",
    "            rgb_frame_reg = cv2.cvtColor(self.current_frame_for_registration, cv2.COLOR_BGR2RGB)\n",
    "            results = self.face_mesh.process(rgb_frame_reg)\n",
    "            faces = self.get_faces_from_results(self.current_frame_for_registration, results) # Call self.get_faces...\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            messagebox.showwarning(\"Registration Error\", \"No person detected in the frame. Please face the camera and try again.\")\n",
    "            return\n",
    "\n",
    "        # Register the largest face\n",
    "        # faces list is now a list of dicts: [{'box': (x,y,w,h), 'center': (cx,cy)}, ...]\n",
    "        faces_by_area = sorted(faces, key=lambda f: f['box'][2] * f['box'][3], reverse=True)\n",
    "        largest_face = faces_by_area[0]\n",
    "\n",
    "        (x, y, w, h) = largest_face['box']\n",
    "        center_x, center_y = largest_face['center']\n",
    "        \n",
    "        # --- NEW: Ask for the person's name ---\n",
    "        name = simpledialog.askstring(\"Register Person\", \"Enter the person's name:\", parent=self.window)\n",
    "        \n",
    "        if not name:\n",
    "            messagebox.showwarning(\"Registration Cancelled\", \"Registration was cancelled (no name provided).\")\n",
    "            return\n",
    "        \n",
    "        # --- Lock in the registration ---\n",
    "        self.is_monitoring = True\n",
    "        self.registered_person_name = name\n",
    "        self.registered_face_center_guess = (center_x, center_y)\n",
    "        self.last_face_center = (center_x, center_y) # Start stillness check from this point\n",
    "        self.stillness_counter = 0\n",
    "\n",
    "        # Update GUI\n",
    "        self.status_text.set(f\"Person registered: {self.registered_person_name}. Actively monitoring.\")\n",
    "        messagebox.showinfo(\"Registration Complete\", f\"{self.registered_person_name} registered. Monitoring has started.\")\n",
    "\n",
    "    def clear_registration(self):\n",
    "        \"\"\"\n",
    "        Clears the current registration and stops monitoring.\n",
    "        \"\"\"\n",
    "        # --- Add Guard Clause ---\n",
    "        if not self.is_monitoring:\n",
    "            messagebox.showwarning(\"Info\", \"No person is currently registered.\")\n",
    "            return\n",
    "            \n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None # Clear name\n",
    "        self.last_face_center = None\n",
    "        self.stillness_counter = 0\n",
    "\n",
    "        self.status_text.set(\"Monitoring stopped. Ready to register a new person.\")\n",
    "\n",
    "\n",
    "    def process_frame_logic(self, frame, rgb_frame):\n",
    "        \"\"\"\n",
    "        Detects faces and applies monitoring logic based on registration status.\n",
    "        This function replaces the old 'detect_sleeping'.\n",
    "        'frame' is BGR (for drawing), 'rgb_frame' is for MediaPipe.\n",
    "        \"\"\"\n",
    "        # Process the frame with MediaPipe\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        \n",
    "        # Get face data\n",
    "        faces = self.get_faces_from_results(frame, results) # Call self.get_faces...\n",
    "\n",
    "        # If not monitoring, just show all faces found\n",
    "        if not self.is_monitoring:\n",
    "            current_status = \"Ready to register.\"\n",
    "            if self.is_running: # Check if camera is on\n",
    "                current_status = \"Click 'Register & Monitor' to begin.\"\n",
    "            \n",
    "            for face_data in faces:\n",
    "                (x, y, w, h) = face_data['box']\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2) # Light blue box\n",
    "            \n",
    "            return frame, current_status\n",
    "\n",
    "        # --- If we ARE monitoring ---\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            # We were monitoring, but now the person is gone\n",
    "            current_status = f\"MONITORING: {self.registered_person_name} lost!\"\n",
    "            self.last_face_center = None # Stop stillness counter\n",
    "            self.stillness_counter = 0\n",
    "            return frame, current_status\n",
    "\n",
    "        # Find the face closest to our last known center\n",
    "        min_dist = float('inf')\n",
    "        tracked_face_data = None\n",
    "        current_center = None # This will be the nose tip\n",
    "\n",
    "        for face_data in faces:\n",
    "            center = face_data['center']\n",
    "            dist = np.linalg.norm(np.array(center) - np.array(self.registered_face_center_guess))\n",
    "            \n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                tracked_face_data = face_data\n",
    "                current_center = center\n",
    "        \n",
    "        # Check if the closest face is \"reasonably\" close.\n",
    "        # If not, it's probably a different person.\n",
    "        if min_dist > MAX_TRACKING_JUMP_PX:\n",
    "             # The person we found is too far from the last spot.\n",
    "             current_status = f\"MONITORING: {self.registered_person_name} lost! (New face detected)\"\n",
    "             self.last_face_center = None\n",
    "             self.stillness_counter = 0\n",
    "             # Draw a box on the *new* face\n",
    "             (x, y, w, h) = tracked_face_data['box']\n",
    "             cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 165, 255), 2) # Orange box\n",
    "             return frame, current_status\n",
    "\n",
    "        # --- We have successfully re-acquired the tracked face ---\n",
    "        (x, y, w, h) = tracked_face_data['box']\n",
    "        \n",
    "        # Update our guess for the next frame\n",
    "        self.registered_face_center_guess = current_center\n",
    "        \n",
    "        current_status = f\"Monitoring: {self.registered_person_name} (Active)\"\n",
    "        alert_triggered = False\n",
    "\n",
    "        # Compare with the last known center\n",
    "        if self.last_face_center is not None:\n",
    "            dist_moved = np.linalg.norm(np.array(current_center) - np.array(self.last_face_center))\n",
    "\n",
    "            if dist_moved < STILLNESS_THRESHOLD_PX:\n",
    "                # Person is still\n",
    "                self.stillness_counter += 1\n",
    "            else:\n",
    "                # Person moved, reset counter\n",
    "                self.stillness_counter = 0\n",
    "        \n",
    "        # Update the last known center for the *next* frame's comparison\n",
    "        self.last_face_center = current_center\n",
    "\n",
    "        # Check if the stillness has crossed the alarm threshold\n",
    "        if self.stillness_counter > SLEEP_ALARM_FRAMES:\n",
    "            current_status = f\"!!! ALERT: {self.registered_person_name} IS SLEEPING !!!\"\n",
    "            alert_triggered = True\n",
    "        \n",
    "        # --- Draw on the frame ---\n",
    "        if alert_triggered:\n",
    "            # Draw a bright red box for alert\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "            cv2.putText(frame, f\"ALERT: {self.registered_person_name}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "        else:\n",
    "            # Draw a standard blue box for the tracked person\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, self.registered_person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f\"Stillness: {self.stillness_counter}/{SLEEP_ALARM_FRAMES}\", (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "        return frame, current_status\n",
    "\n",
    "\n",
    "    def detect_sleeping(self, frame):\n",
    "        \"\"\"\n",
    "        This function is no longer called directly by the video_loop.\n",
    "        Its logic has been moved into process_frame_logic.\n",
    "        \"\"\"\n",
    "        pass # Kept to show what was replaced\n",
    "        \n",
    "\n",
    "    def on_closing(self):\n",
    "        \"\"\"\n",
    "        Handles the window close event.\n",
    "        \"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204b4db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in video loop: main thread is not in main loop\n",
      "Video loop stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Variable.__del__ at 0x00000238D8AD58A0>\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Application\\Python3114\\Lib\\tkinter\\__init__.py\", line 410, in __del__\n",
      "    if self._tk.getboolean(self._tk.call(\"info\", \"exists\", self._name)):\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: main thread is not in main loop\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp  # --- Import MediaPipe ---\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# --- New Eye Aspect Ratio (EAR) Constants ---\n",
    "# Threshold for \"closed\" eyes. You will need to tune this value.\n",
    "# Start with 0.20. Lower it if it triggers when blinking.\n",
    "EYE_AR_THRESHOLD = 0.20\n",
    "\n",
    "# How many consecutive frames of \"closed\" eyes trigger an alarm?\n",
    "# User requested 10 seconds. Assuming ~20fps, 10 * 20 = 200 frames.\n",
    "EYE_AR_CONSEC_FRAMES = 80\n",
    "\n",
    "\n",
    "# New constant for tracking\n",
    "# If the closest-found face is further than this from the last frame,\n",
    "# we assume it's a new person or the tracker lost the original.\n",
    "MAX_TRACKING_JUMP_PX = 150\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Sleeping Alert System (Dev Phase)\")\n",
    "        self.window.geometry(\"800x700\")\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- Face Detection (REPLACED) ---\n",
    "        # We now use MediaPipe Face Mesh, which is far more accurate.\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=5,  # Detect up to 5 faces\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # --- \"Sleeping\" Logic State ---\n",
    "        self.eye_closed_counter = 0 # Replaces stillness_counter\n",
    "        \n",
    "        # --- New Registration State ---\n",
    "        self.is_monitoring = False  # Are we actively monitoring a registered person?\n",
    "        self.registered_face_center_guess = None # The last known center of the monitored person\n",
    "        self.registered_person_name = None # Store the registered person's name\n",
    "        self.current_frame_for_registration = None # Temp stores frame for registration\n",
    "        self.registration_lock = threading.Lock() # Lock for accessing the frame\n",
    "\n",
    "        # --- GUI Elements ---\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "        # --- GUI Layout Fix ---\n",
    "        # Pack non-expanding widgets (buttons, status) to the BOTTOM first.\n",
    "        # This anchors them to the bottom of the main_frame.\n",
    "        \n",
    "        # Control Buttons\n",
    "        self.button_frame = tk.Frame(self.main_frame)\n",
    "        self.button_frame.pack(side=tk.BOTTOM, pady=10) # Anchor to bottom\n",
    "\n",
    "        # Status Label\n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.main_frame, textvariable=self.status_text, font=(\"Arial\", 14))\n",
    "        self.status_label.pack(side=tk.BOTTOM, pady=5) # Anchor to bottom, above buttons\n",
    "\n",
    "        # Video Display Label\n",
    "        # This is packed LAST, so it expands to fill all remaining space.\n",
    "        self.video_label = tk.Label(self.main_frame, bg=\"black\")\n",
    "        self.video_label.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # --- New Registration Buttons ---\n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.register_person, font=(\"Arial\", 12), width=18)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear Registration\", command=self.clear_registration, font=(\"Arial\", 12), width=18)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "\n",
    "        # Set up the close protocol\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        \"\"\"\n",
    "        Starts the video capture in a new thread.\n",
    "        \"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)  # Use 0 for laptop webcam\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            \n",
    "            self.is_running = True\n",
    "            \n",
    "            # Start the video processing thread\n",
    "            # daemon=True ensures the thread will close when the main app closes\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            \n",
    "            self.status_text.set(\"Camera running. Click 'Register & Monitor'.\")\n",
    "        \n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap:\n",
    "                self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        \"\"\"\n",
    "        Signals the video loop to stop.\n",
    "        \"\"\"\n",
    "        # Add guard clause\n",
    "        if not self.is_running:\n",
    "            return\n",
    "            \n",
    "        self.is_running = False\n",
    "        \n",
    "        # The thread will see self.is_running is False and exit\n",
    "        # We wait a moment for the thread to finish\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "            \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        self.video_label.config(image=None) # Clear the image\n",
    "        \n",
    "        # --- Reset all states ---\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None # Clear name\n",
    "        self.eye_closed_counter = 0\n",
    "\n",
    "    def video_loop(self):\n",
    "        \"\"\"\n",
    "        The main loop for video processing. Runs in a separate thread.\n",
    "        \"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                # Flip for a \"mirror\" view, which is more intuitive\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Store a copy of the frame for the registration function\n",
    "                with self.registration_lock:\n",
    "                    self.current_frame_for_registration = frame.copy()\n",
    "                \n",
    "                # MediaPipe needs RGB, but OpenCV provides BGR.\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Process the frame\n",
    "                processed_frame, status = self.process_frame_logic(frame, rgb_frame) # Pass rgb_frame\n",
    "\n",
    "                # Update the status text\n",
    "                self.status_text.set(status)\n",
    "\n",
    "                # Convert the OpenCV (BGR) frame to a PIL (RGB) image\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                \n",
    "                # Resize image to fit the label (optional, but good for layout)\n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1: # Avoid division by zero on init\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "\n",
    "                # Convert PIL image to Tkinter-compatible image\n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "\n",
    "                # Update the video label in the GUI\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            # Control loop speed slightly\n",
    "            time.sleep(0.01) # ~100fps theoretical max, but processing will slow it down\n",
    "\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    # --- NEW HELPER: Calculate Eye Aspect Ratio (EAR) ---\n",
    "    def get_ear(self, eye_points, w, h):\n",
    "        \"\"\"\n",
    "        Calculates the Eye Aspect Ratio (EAR) given 6 landmark points.\n",
    "        The points should be in the order: [P1, P4, P2, P6, P3, P5]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert landmarks to (x, y) tuples\n",
    "            p1 = (int(eye_points[0].x * w), int(eye_points[0].y * h))\n",
    "            p4 = (int(eye_points[1].x * w), int(eye_points[1].y * h))\n",
    "            p2 = (int(eye_points[2].x * w), int(eye_points[2].y * h))\n",
    "            p6 = (int(eye_points[3].x * w), int(eye_points[3].y * h))\n",
    "            p3 = (int(eye_points[4].x * w), int(eye_points[4].y * h))\n",
    "            p5 = (int(eye_points[5].x * w), int(eye_points[5].y * h))\n",
    "\n",
    "            # Helper to calculate euclidean distance\n",
    "            def get_dist(p_a, p_b):\n",
    "                return np.linalg.norm(np.array(p_a) - np.array(p_b))\n",
    "\n",
    "            # Vertical distances\n",
    "            v_dist_1 = get_dist(p2, p6)\n",
    "            v_dist_2 = get_dist(p3, p5)\n",
    "\n",
    "            # Horizontal distance\n",
    "            h_dist = get_dist(p1, p4)\n",
    "\n",
    "            # Avoid division by zero\n",
    "            if h_dist == 0:\n",
    "                return 0.3 # Return an \"open\" value\n",
    "\n",
    "            # Calculate EAR\n",
    "            ear = (v_dist_1 + v_dist_2) / (2.0 * h_dist)\n",
    "            return ear\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating EAR: {e}\")\n",
    "            return 0.3 # Default to \"open\"\n",
    "\n",
    "    # --- Helper function to get face data from MediaPipe results ---\n",
    "    def get_faces_from_results(self, frame, results):\n",
    "        \"\"\"\n",
    "        Extracts bounding boxes and nose-tip centers from MediaPipe results.\n",
    "        \"\"\"\n",
    "        faces_data = []\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # --- 1. Get Bounding Box ---\n",
    "                x_min, y_min = w, h\n",
    "                x_max, y_max = 0, 0\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    if x < x_min: x_min = x\n",
    "                    if y < y_min: y_min = y\n",
    "                    if x > x_max: x_max = x\n",
    "                    if y > y_max: y_max = y\n",
    "                \n",
    "                # Add a little padding\n",
    "                x_min = max(0, x_min - 10)\n",
    "                y_min = max(0, y_min - 10)\n",
    "                x_max = min(w, x_max + 10)\n",
    "                y_max = min(h, y_max + 10)\n",
    "\n",
    "                box = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "\n",
    "                # --- 2. Get Nose Tip (Landmark 1) for precise tracking ---\n",
    "                # This is our new \"center\" for movement tracking\n",
    "                nose_tip = face_landmarks.landmark[1]\n",
    "                center = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "                \n",
    "                # --- 3. Calculate Eye Aspect Ratio (EAR) ---\n",
    "                # These are the specific landmark indices for the EAR calculation\n",
    "                RIGHT_EYE_EAR_POINTS_INDICES = [33, 133, 159, 145, 158, 153] # P1, P4, P2, P6, P3, P5\n",
    "                LEFT_EYE_EAR_POINTS_INDICES = [362, 263, 386, 374, 385, 380] # P1, P4, P2, P6, P3, P5\n",
    "\n",
    "                landmarks = face_landmarks.landmark\n",
    "                \n",
    "                right_eye_points = [landmarks[i] for i in RIGHT_EYE_EAR_POINTS_INDICES]\n",
    "                left_eye_points = [landmarks[i] for i in LEFT_EYE_EAR_POINTS_INDICES]\n",
    "\n",
    "                right_ear = self.get_ear(right_eye_points, w, h)\n",
    "                left_ear = self.get_ear(left_eye_points, w, h)\n",
    "                \n",
    "                avg_ear = (left_ear + right_ear) / 2.0\n",
    "                \n",
    "                faces_data.append({'box': box, 'center': center, 'avg_ear': avg_ear})\n",
    "        \n",
    "        return faces_data\n",
    "\n",
    "    def register_person(self):\n",
    "        \"\"\"\n",
    "        Registers the largest face currently in the frame for monitoring.\n",
    "        \"\"\"\n",
    "        with self.registration_lock:\n",
    "            if self.current_frame_for_registration is None:\n",
    "                messagebox.showwarning(\"Registration Error\", \"Camera not ready. Please try again.\")\n",
    "                return\n",
    "            \n",
    "            # Use the stored frame to find a face\n",
    "            # MediaPipe needs RGB\n",
    "            rgb_frame_reg = cv2.cvtColor(self.current_frame_for_registration, cv2.COLOR_BGR2RGB)\n",
    "            results = self.face_mesh.process(rgb_frame_reg)\n",
    "            faces = self.get_faces_from_results(self.current_frame_for_registration, results) # Call self.get_faces...\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            messagebox.showwarning(\"Registration Error\", \"No person detected in the frame. Please face the camera and try again.\")\n",
    "            return\n",
    "\n",
    "        # Register the largest face\n",
    "        # faces list is now a list of dicts: [{'box': (x,y,w,h), 'center': (cx,cy)}, ...]\n",
    "        faces_by_area = sorted(faces, key=lambda f: f['box'][2] * f['box'][3], reverse=True)\n",
    "        largest_face = faces_by_area[0]\n",
    "\n",
    "        (x, y, w, h) = largest_face['box']\n",
    "        center_x, center_y = largest_face['center']\n",
    "        \n",
    "        # --- NEW: Ask for the person's name ---\n",
    "        name = simpledialog.askstring(\"Register Person\", \"Enter the person's name:\", parent=self.window)\n",
    "        \n",
    "        if not name:\n",
    "            messagebox.showwarning(\"Registration Cancelled\", \"Registration was cancelled (no name provided).\")\n",
    "            return\n",
    "        \n",
    "        # --- Lock in the registration ---\n",
    "        self.is_monitoring = True\n",
    "        self.registered_person_name = name\n",
    "        self.registered_face_center_guess = (center_x, center_y)\n",
    "        self.eye_closed_counter = 0\n",
    "\n",
    "        # Update GUI\n",
    "        self.status_text.set(f\"Person registered: {self.registered_person_name}. Actively monitoring.\")\n",
    "        messagebox.showinfo(\"Registration Complete\", f\"{self.registered_person_name} registered. Monitoring has started.\")\n",
    "\n",
    "    def clear_registration(self):\n",
    "        \"\"\"\n",
    "        Clears the current registration and stops monitoring.\n",
    "        \"\"\"\n",
    "        # --- Add Guard Clause ---\n",
    "        if not self.is_monitoring:\n",
    "            messagebox.showwarning(\"Info\", \"No person is currently registered.\")\n",
    "            return\n",
    "            \n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None # Clear name\n",
    "        self.eye_closed_counter = 0\n",
    "\n",
    "        self.status_text.set(\"Monitoring stopped. Ready to register a new person.\")\n",
    "\n",
    "\n",
    "    def process_frame_logic(self, frame, rgb_frame):\n",
    "        \"\"\"\n",
    "        Detects faces and applies monitoring logic based on registration status.\n",
    "        This function replaces the old 'detect_sleeping'.\n",
    "        'frame' is BGR (for drawing), 'rgb_frame' is for MediaPipe.\n",
    "        \"\"\"\n",
    "        # Process the frame with MediaPipe\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        \n",
    "        # Get face data\n",
    "        faces = self.get_faces_from_results(frame, results) # Call self.get_faces...\n",
    "\n",
    "        # If not monitoring, just show all faces found\n",
    "        if not self.is_monitoring:\n",
    "            current_status = \"Ready to register.\"\n",
    "            if self.is_running: # Check if camera is on\n",
    "                current_status = \"Click 'Register & Monitor' to begin.\"\n",
    "            \n",
    "            for face_data in faces:\n",
    "                (x, y, w, h) = face_data['box']\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2) # Light blue box\n",
    "            \n",
    "            return frame, current_status\n",
    "\n",
    "        # --- If we ARE monitoring ---\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            # We were monitoring, but now the person is gone\n",
    "            current_status = f\"MONITORING: {self.registered_person_name} lost!\"\n",
    "            self.eye_closed_counter = 0\n",
    "            return frame, current_status\n",
    "\n",
    "        # Find the face closest to our last known center\n",
    "        min_dist = float('inf')\n",
    "        tracked_face_data = None\n",
    "        current_center = None # This will be the nose tip\n",
    "\n",
    "        for face_data in faces:\n",
    "            center = face_data['center']\n",
    "            dist = np.linalg.norm(np.array(center) - np.array(self.registered_face_center_guess))\n",
    "            \n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                tracked_face_data = face_data\n",
    "                current_center = center\n",
    "        \n",
    "        # Check if the closest face is \"reasonably\" close.\n",
    "        # If not, it's probably a different person.\n",
    "        if min_dist > MAX_TRACKING_JUMP_PX:\n",
    "             # The person we found is too far from the last spot.\n",
    "             current_status = f\"MONITORING: {self.registered_person_name} lost! (New face detected)\"\n",
    "             self.eye_closed_counter = 0\n",
    "             # Draw a box on the *new* face\n",
    "             (x, y, w, h) = tracked_face_data['box']\n",
    "             cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 165, 255), 2) # Orange box\n",
    "             return frame, current_status\n",
    "\n",
    "        # --- We have successfully re-acquired the tracked face ---\n",
    "        (x, y, w, h) = tracked_face_data['box']\n",
    "        \n",
    "        # Update our guess for the next frame\n",
    "        self.registered_face_center_guess = current_center\n",
    "        \n",
    "        current_status = f\"Monitoring: {self.registered_person_name} (Active)\"\n",
    "        alert_triggered = False\n",
    "        \n",
    "        # --- Get the EAR value for the tracked face ---\n",
    "        avg_ear = tracked_face_data['avg_ear']\n",
    "\n",
    "        # --- NEW: Eye Aspect Ratio (EAR) Logic ---\n",
    "        if avg_ear < EYE_AR_THRESHOLD:\n",
    "            # Eyes are closed\n",
    "            self.eye_closed_counter += 1\n",
    "        else:\n",
    "            # Eyes are open, reset counter\n",
    "            self.eye_closed_counter = 0\n",
    "        \n",
    "        # Check if the \"eyes closed\" counter has crossed the alarm threshold\n",
    "        if self.eye_closed_counter > EYE_AR_CONSEC_FRAMES:\n",
    "            current_status = f\"!!! ALERT: {self.registered_person_name} EYES CLOSED !!!\"\n",
    "            alert_triggered = True\n",
    "        else:\n",
    "            # Update status text if not alerting\n",
    "            current_status = f\"Monitoring: {self.registered_person_name} (Eyes Open)\"\n",
    "        \n",
    "        # --- Draw on the frame ---\n",
    "        if alert_triggered:\n",
    "            # Draw a bright red box for alert\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "            cv2.putText(frame, f\"ALERT: {self.registered_person_name} (EYES CLOSED)\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        else:\n",
    "            # Draw a standard blue box for the tracked person\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, self.registered_person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "            \n",
    "            # --- MODIFIED: Show eye counter instead of stillness ---\n",
    "            cv2.putText(frame, f\"Eye Closed Count: {self.eye_closed_counter}/{EYE_AR_CONSEC_FRAMES}\", (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "            \n",
    "            # --- ADDED: Show live EAR value for tuning ---\n",
    "            cv2.putText(frame, f\"EAR: {avg_ear:.2f}\", (x, y + h + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        return frame, current_status\n",
    "\n",
    "\n",
    "    def detect_sleeping(self, frame):\n",
    "        \"\"\"\n",
    "        This function is no longer called directly by the video_loop.\n",
    "        Its logic has been moved into process_frame_logic.\n",
    "        \"\"\"\n",
    "        pass # Kept to show what was replaced\n",
    "        \n",
    "\n",
    "    def on_closing(self):\n",
    "        \"\"\"\n",
    "        Handles the window close event.\n",
    "        \"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74802ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp  # --- Import MediaPipe ---\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# --- New Dynamic Calibration Constants ---\n",
    "# We no longer use a fixed EAR threshold.\n",
    "# Instead, we set a personal threshold as a percentage of the user's open-eye value.\n",
    "# e.g., if open EAR is 0.30, threshold will be 0.30 * 0.75 = 0.225\n",
    "THRESHOLD_CALIBRATION_FACTOR = 0.75\n",
    "\n",
    "# Sanity check: if the user's \"open\" EAR is below this,\n",
    "# they probably tried to register with their eyes closed.\n",
    "MIN_OPEN_EYE_EAR = 0.20\n",
    "\n",
    "# How many consecutive frames of \"closed\" eyes trigger an alarm?\n",
    "# User requested 10 seconds. Assuming ~20fps, 10 * 20 = 200 frames.\n",
    "EYE_AR_CONSEC_FRAMES = 200\n",
    "\n",
    "# Tracking constant: max distance a face can \"jump\" between frames.\n",
    "MAX_TRACKING_JUMP_PX = 150\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Sleeping Alert System (Calibrated)\")\n",
    "        self.window.geometry(\"800x700\")\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- MediaPipe Face Mesh Initialization ---\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=5,  # Detect up to 5 faces\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # --- Logic & Registration State ---\n",
    "        self.eye_closed_counter = 0\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None\n",
    "        self.current_frame_for_registration = None\n",
    "        self.registration_lock = threading.Lock()\n",
    "\n",
    "        # --- New Calibration State Variables ---\n",
    "        # These are set during registration and are unique to the user.\n",
    "        self.calibrated_open_ear = 0.0\n",
    "        self.calibrated_threshold = 0.0\n",
    "\n",
    "        # --- GUI Elements ---\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "        # --- GUI Layout Fix: Anchor controls to BOTTOM ---\n",
    "        \n",
    "        self.button_frame = tk.Frame(self.main_frame)\n",
    "        self.button_frame.pack(side=tk.BOTTOM, pady=10)\n",
    "\n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.main_frame, textvariable=self.status_text, font=(\"Arial\", 14))\n",
    "        self.status_label.pack(side=tk.BOTTOM, pady=5)\n",
    "\n",
    "        # Video label expands to fill the remaining space\n",
    "        self.video_label = tk.Label(self.main_frame, bg=\"black\")\n",
    "        self.video_label.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # --- Buttons ---\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.register_person, font=(\"Arial\", 12), width=18)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear Registration\", command=self.clear_registration, font=(\"Arial\", 12), width=18)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        \"\"\"\n",
    "        Starts the video capture in a new thread.\n",
    "        \"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            \n",
    "            self.is_running = True\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            self.status_text.set(\"Camera running. Click 'Register & Monitor'.\")\n",
    "        \n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap:\n",
    "                self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        \"\"\"\n",
    "        Signals the video loop to stop and resets all states.\n",
    "        \"\"\"\n",
    "        if not self.is_running:\n",
    "            return\n",
    "            \n",
    "        self.is_running = False\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "            \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "        self.video_label.config(image=None)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        \n",
    "        # Reset all logic states\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None\n",
    "        self.eye_closed_counter = 0\n",
    "        self.calibrated_open_ear = 0.0\n",
    "        self.calibrated_threshold = 0.0\n",
    "\n",
    "    def video_loop(self):\n",
    "        \"\"\"\n",
    "        The main loop for video processing. Runs in a separate thread.\n",
    "        \"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                with self.registration_lock:\n",
    "                    self.current_frame_for_registration = frame.copy()\n",
    "                \n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Process the frame and get status\n",
    "                processed_frame, status = self.process_frame_logic(frame, rgb_frame)\n",
    "\n",
    "                self.status_text.set(status)\n",
    "\n",
    "                # Convert for Tkinter display\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                \n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1:\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "\n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            time.sleep(0.01)\n",
    "\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    def get_ear(self, eye_points, w, h):\n",
    "        \"\"\"\n",
    "        Calculates the Eye Aspect Ratio (EAR) given 6 landmark points.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert landmarks to (x, y) tuples\n",
    "            p1 = (int(eye_points[0].x * w), int(eye_points[0].y * h))\n",
    "            p4 = (int(eye_points[1].x * w), int(eye_points[1].y * h))\n",
    "            p2 = (int(eye_points[2].x * w), int(eye_points[2].y * h))\n",
    "            p6 = (int(eye_points[3].x * w), int(eye_points[3].y * h))\n",
    "            p3 = (int(eye_points[4].x * w), int(eye_points[4].y * h))\n",
    "            p5 = (int(eye_points[5].x * w), int(eye_points[5].y * h))\n",
    "\n",
    "            def get_dist(p_a, p_b):\n",
    "                return np.linalg.norm(np.array(p_a) - np.array(p_b))\n",
    "\n",
    "            v_dist_1 = get_dist(p2, p6)\n",
    "            v_dist_2 = get_dist(p3, p5)\n",
    "            h_dist = get_dist(p1, p4)\n",
    "\n",
    "            if h_dist == 0:\n",
    "                return 0.3 # Default \"open\"\n",
    "\n",
    "            ear = (v_dist_1 + v_dist_2) / (2.0 * h_dist)\n",
    "            return ear\n",
    "        except Exception:\n",
    "            return 0.3 # Default \"open\"\n",
    "\n",
    "    def get_faces_from_results(self, frame, results):\n",
    "        \"\"\"\n",
    "        Extracts face data (box, center, EAR) from MediaPipe results.\n",
    "        \"\"\"\n",
    "        faces_data = []\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # 1. Get Bounding Box\n",
    "                x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    if x < x_min: x_min = x\n",
    "                    if y < y_min: y_min = y\n",
    "                    if x > x_max: x_max = x\n",
    "                    if y > y_max: y_max = y\n",
    "                \n",
    "                padding = 10\n",
    "                x_min = max(0, x_min - padding)\n",
    "                y_min = max(0, y_min - padding)\n",
    "                x_max = min(w, x_max + padding)\n",
    "                y_max = min(h, y_max + padding)\n",
    "                box = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "\n",
    "                # 2. Get Nose Tip (Landmark 1) for tracking\n",
    "                nose_tip = face_landmarks.landmark[1]\n",
    "                center = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "                \n",
    "                # 3. Calculate Eye Aspect Ratio (EAR)\n",
    "                RIGHT_EYE_EAR_POINTS_INDICES = [33, 133, 159, 145, 158, 153]\n",
    "                LEFT_EYE_EAR_POINTS_INDICES = [362, 263, 386, 374, 385, 380]\n",
    "\n",
    "                landmarks = face_landmarks.landmark\n",
    "                right_eye_points = [landmarks[i] for i in RIGHT_EYE_EAR_POINTS_INDICES]\n",
    "                left_eye_points = [landmarks[i] for i in LEFT_EYE_EAR_POINTS_INDICES]\n",
    "\n",
    "                right_ear = self.get_ear(right_eye_points, w, h)\n",
    "                left_ear = self.get_ear(left_eye_points, w, h)\n",
    "                avg_ear = (left_ear + right_ear) / 2.0\n",
    "                \n",
    "                faces_data.append({'box': box, 'center': center, 'avg_ear': avg_ear})\n",
    "        \n",
    "        return faces_data\n",
    "\n",
    "    def register_person(self):\n",
    "        \"\"\"\n",
    "        Registers a person and dynamically calibrates the EAR threshold.\n",
    "        \"\"\"\n",
    "        with self.registration_lock:\n",
    "            if self.current_frame_for_registration is None:\n",
    "                messagebox.showwarning(\"Registration Error\", \"Camera not ready. Please try again.\")\n",
    "                return\n",
    "            \n",
    "            rgb_frame_reg = cv2.cvtColor(self.current_frame_for_registration, cv2.COLOR_BGR2RGB)\n",
    "            results = self.face_mesh.process(rgb_frame_reg)\n",
    "            faces = self.get_faces_from_results(self.current_frame_for_registration, results)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            messagebox.showwarning(\"Registration Error\", \"No person detected. Please face the camera and try again.\")\n",
    "            return\n",
    "\n",
    "        faces_by_area = sorted(faces, key=lambda f: f['box'][2] * f['box'][3], reverse=True)\n",
    "        largest_face = faces_by_area[0]\n",
    "\n",
    "        # --- DYNAMIC CALIBRATION ---\n",
    "        current_ear = largest_face['avg_ear']\n",
    "        \n",
    "        if current_ear < MIN_OPEN_EYE_EAR:\n",
    "            messagebox.showwarning(\"Registration Error\", f\"Eyes seem to be closed (EAR: {current_ear:.2f}).\\nPlease open your eyes and try again.\")\n",
    "            return\n",
    "\n",
    "        self.calibrated_open_ear = current_ear\n",
    "        self.calibrated_threshold = current_ear * THRESHOLD_CALIBRATION_FACTOR\n",
    "        \n",
    "        name = simpledialog.askstring(\"Register Person\", \"Enter the person's name:\", parent=self.window)\n",
    "        if not name:\n",
    "            messagebox.showwarning(\"Registration Cancelled\", \"Registration was cancelled.\")\n",
    "            return\n",
    "        \n",
    "        # --- Lock in the registration ---\n",
    "        (x, y, w, h) = largest_face['box']\n",
    "        self.is_monitoring = True\n",
    "        self.registered_person_name = name\n",
    "        self.registered_face_center_guess = largest_face['center']\n",
    "        self.eye_closed_counter = 0\n",
    "\n",
    "        status = f\"Calibrated for {name}. Open EAR: {self.calibrated_open_ear:.2f}, Threshold: {self.calibrated_threshold:.2f}\"\n",
    "        self.status_text.set(status)\n",
    "        messagebox.showinfo(\"Registration Complete\", status)\n",
    "\n",
    "    def clear_registration(self):\n",
    "        \"\"\"\n",
    "        Clears the current registration and stops monitoring.\n",
    "        \"\"\"\n",
    "        if not self.is_monitoring:\n",
    "            messagebox.showwarning(\"Info\", \"No person is currently registered.\")\n",
    "            return\n",
    "        \n",
    "        # Reset all logic states\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None\n",
    "        self.eye_closed_counter = 0\n",
    "        self.calibrated_open_ear = 0.0\n",
    "        self.calibrated_threshold = 0.0\n",
    "\n",
    "        self.status_text.set(\"Monitoring stopped. Ready to register a new person.\")\n",
    "\n",
    "\n",
    "    def process_frame_logic(self, frame, rgb_frame):\n",
    "        \"\"\"\n",
    "        Main logic: finds faces, tracks registered person, checks EAR.\n",
    "        \"\"\"\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        faces = self.get_faces_from_results(frame, results)\n",
    "\n",
    "        # If not monitoring, just show all faces\n",
    "        if not self.is_monitoring:\n",
    "            current_status = \"Click 'Register & Monitor' to begin.\"\n",
    "            for face_data in faces:\n",
    "                (x, y, w, h) = face_data['box']\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2)\n",
    "            return frame, current_status\n",
    "\n",
    "        # --- If we ARE monitoring ---\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            current_status = f\"MONITORING: {self.registered_person_name} lost!\"\n",
    "            self.eye_closed_counter = 0\n",
    "            return frame, current_status\n",
    "\n",
    "        # Find the face closest to our last known center\n",
    "        min_dist = float('inf')\n",
    "        tracked_face_data = None\n",
    "        current_center = None\n",
    "\n",
    "        for face_data in faces:\n",
    "            center = face_data['center']\n",
    "            dist = np.linalg.norm(np.array(center) - np.array(self.registered_face_center_guess))\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                tracked_face_data = face_data\n",
    "                current_center = center\n",
    "        \n",
    "        if min_dist > MAX_TRACKING_JUMP_PX:\n",
    "             current_status = f\"MONITORING: {self.registered_person_name} lost! (New face detected)\"\n",
    "             self.eye_closed_counter = 0\n",
    "             (x, y, w, h) = tracked_face_data['box']\n",
    "             cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 165, 255), 2)\n",
    "             return frame, current_status\n",
    "\n",
    "        # --- We have re-acquired the tracked face ---\n",
    "        (x, y, w, h) = tracked_face_data['box']\n",
    "        self.registered_face_center_guess = current_center\n",
    "        avg_ear = tracked_face_data['avg_ear']\n",
    "        alert_triggered = False\n",
    "        \n",
    "        # --- CALIBRATED Eye Aspect Ratio (EAR) Logic ---\n",
    "        if avg_ear < self.calibrated_threshold:\n",
    "            self.eye_closed_counter += 1\n",
    "        else:\n",
    "            self.eye_closed_counter = 0\n",
    "        \n",
    "        if self.eye_closed_counter > EYE_AR_CONSEC_FRAMES:\n",
    "            current_status = f\"!!! ALERT: {self.registered_person_name} EYES CLOSED !!!\"\n",
    "            alert_triggered = True\n",
    "        else:\n",
    "            current_status = f\"Monitoring: {self.registered_person_name} (Eyes Open)\"\n",
    "        \n",
    "        # --- Draw on the frame ---\n",
    "        if alert_triggered:\n",
    "            color = (0, 0, 255) # Red for Alert\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 3)\n",
    "            cv2.putText(frame, f\"ALERT: {self.registered_person_name} (EYES CLOSED)\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        else:\n",
    "            color = (255, 0, 0) # Blue for Tracking\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(frame, self.registered_person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # Show the live data for tuning\n",
    "            ear_text = f\"EAR: {avg_ear:.2f} (Thresh: {self.calibrated_threshold:.2f})\"\n",
    "            count_text = f\"Eye Closed Count: {self.eye_closed_counter}/{EYE_AR_CONSEC_FRAMES}\"\n",
    "            cv2.putText(frame, ear_text, (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, count_text, (x, y + h + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        return frame, current_status\n",
    "\n",
    "\n",
    "    def on_closing(self):\n",
    "        \"\"\"\n",
    "        Handles the window close event.\n",
    "        \"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751bd453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp  # --- Import MediaPipe ---\n",
    "import beepy as beep   # --- NEW: For sound alerts ---\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# --- New Dynamic Calibration Constants ---\n",
    "# We no longer use a fixed EAR threshold.\n",
    "# Instead, we set a personal threshold as a percentage of the user's open-eye value.\n",
    "# e.g., if open EAR is 0.30, threshold will be 0.30 * 0.75 = 0.225\n",
    "THRESHOLD_CALIBRATION_FACTOR = 0.75\n",
    "\n",
    "# Sanity check: if the user's \"open\" EAR is below this,\n",
    "# they probably tried to register with their eyes closed.\n",
    "MIN_OPEN_EYE_EAR = 0.20\n",
    "\n",
    "# How many consecutive frames of \"closed\" eyes trigger an alarm?\n",
    "# User requested 10 seconds. Assuming ~20fps, 10 * 20 = 200 frames.\n",
    "EYE_AR_CONSEC_FRAMES = 200\n",
    "\n",
    "# --- NEW: Tracking \"Grace Period\" ---\n",
    "# How many frames to keep searching for a person before declaring them \"lost\".\n",
    "LOST_GRACE_FRAMES = 50 # ~2.5 seconds at 20fps\n",
    "\n",
    "# Tracking constant: max distance a face can \"jump\" between frames.\n",
    "MAX_TRACKING_JUMP_PX = 150\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Sleeping Alert System (Calibrated + Sound)\")\n",
    "        self.window.geometry(\"800x700\")\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- MediaPipe Face Mesh Initialization ---\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=5,  # Detect up to 5 faces\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # --- Logic & Registration State ---\n",
    "        self.eye_closed_counter = 0\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None\n",
    "        self.current_frame_for_registration = None\n",
    "        self.registration_lock = threading.Lock()\n",
    "\n",
    "        # --- New Calibration State Variables ---\n",
    "        self.calibrated_open_ear = 0.0\n",
    "        self.calibrated_threshold = 0.0\n",
    "        \n",
    "        # --- NEW: State variables for improved tracking and sound ---\n",
    "        self.lost_tracker_counter = 0   # For grace period\n",
    "        self.alert_sound_playing = False # To prevent continuous beeping\n",
    "\n",
    "        # --- GUI Elements ---\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "        # --- GUI Layout Fix: Anchor controls to BOTTOM ---\n",
    "        \n",
    "        self.button_frame = tk.Frame(self.main_frame)\n",
    "        self.button_frame.pack(side=tk.BOTTOM, pady=10)\n",
    "\n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.main_frame, textvariable=self.status_text, font=(\"Arial\", 14))\n",
    "        self.status_label.pack(side=tk.BOTTOM, pady=5)\n",
    "\n",
    "        # Video label expands to fill the remaining space\n",
    "        self.video_label = tk.Label(self.main_frame, bg=\"black\")\n",
    "        self.video_label.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # --- Buttons ---\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.register_person, font=(\"Arial\", 12), width=18)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear Registration\", command=self.clear_registration, font=(\"Arial\", 12), width=18)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        \"\"\"\n",
    "        Starts the video capture in a new thread.\n",
    "        \"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            \n",
    "            self.is_running = True\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            self.status_text.set(\"Camera running. Click 'Register & Monitor'.\")\n",
    "        \n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap:\n",
    "                self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        \"\"\"\n",
    "        Signals the video loop to stop and resets all states.\n",
    "        \"\"\"\n",
    "        if not self.is_running:\n",
    "            return\n",
    "            \n",
    "        self.is_running = False\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "            \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "        self.video_label.config(image=None)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        \n",
    "        # Reset all logic states\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None\n",
    "        self.eye_closed_counter = 0\n",
    "        self.calibrated_open_ear = 0.0\n",
    "        self.calibrated_threshold = 0.0\n",
    "        self.lost_tracker_counter = 0\n",
    "        self.alert_sound_playing = False\n",
    "\n",
    "    def video_loop(self):\n",
    "        \"\"\"\n",
    "        The main loop for video processing. Runs in a separate thread.\n",
    "        \"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                with self.registration_lock:\n",
    "                    self.current_frame_for_registration = frame.copy()\n",
    "                \n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Process the frame and get status\n",
    "                processed_frame, status = self.process_frame_logic(frame, rgb_frame)\n",
    "\n",
    "                self.status_text.set(status)\n",
    "\n",
    "                # Convert for Tkinter display\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                \n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1:\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "\n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            time.sleep(0.01)\n",
    "\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    def get_ear(self, eye_points, w, h):\n",
    "        \"\"\"\n",
    "        Calculates the Eye Aspect Ratio (EAR) given 6 landmark points.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert landmarks to (x, y) tuples\n",
    "            p1 = (int(eye_points[0].x * w), int(eye_points[0].y * h))\n",
    "            p4 = (int(eye_points[1].x * w), int(eye_points[1].y * h))\n",
    "            p2 = (int(eye_points[2].x * w), int(eye_points[2].y * h))\n",
    "            p6 = (int(eye_points[3].x * w), int(eye_points[3].y * h))\n",
    "            p3 = (int(eye_points[4].x * w), int(eye_points[4].y * h))\n",
    "            p5 = (int(eye_points[5].x * w), int(eye_points[5].y * h))\n",
    "\n",
    "            def get_dist(p_a, p_b):\n",
    "                return np.linalg.norm(np.array(p_a) - np.array(p_b))\n",
    "\n",
    "            v_dist_1 = get_dist(p2, p6)\n",
    "            v_dist_2 = get_dist(p3, p5)\n",
    "            h_dist = get_dist(p1, p4)\n",
    "\n",
    "            if h_dist == 0:\n",
    "                return 0.3 # Default \"open\"\n",
    "\n",
    "            ear = (v_dist_1 + v_dist_2) / (2.0 * h_dist)\n",
    "            return ear\n",
    "        except Exception:\n",
    "            return 0.3 # Default \"open\"\n",
    "\n",
    "    def get_faces_from_results(self, frame, results):\n",
    "        \"\"\"\n",
    "        Extracts face data (box, center, EAR) from MediaPipe results.\n",
    "        \"\"\"\n",
    "        faces_data = []\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # 1. Get Bounding Box\n",
    "                x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    if x < x_min: x_min = x\n",
    "                    if y < y_min: y_min = y\n",
    "                    if x > x_max: x_max = x\n",
    "                    if y > y_max: y_max = y\n",
    "                \n",
    "                padding = 10\n",
    "                x_min = max(0, x_min - padding)\n",
    "                y_min = max(0, y_min - padding)\n",
    "                x_max = min(w, x_max + padding)\n",
    "                y_max = min(h, y_max + padding)\n",
    "                box = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "\n",
    "                # 2. Get Nose Tip (Landmark 1) for tracking\n",
    "                nose_tip = face_landmarks.landmark[1]\n",
    "                center = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "                \n",
    "                # 3. Calculate Eye Aspect Ratio (EAR)\n",
    "                RIGHT_EYE_EAR_POINTS_INDICES = [33, 133, 159, 145, 158, 153]\n",
    "                LEFT_EYE_EAR_POINTS_INDICES = [362, 263, 386, 374, 385, 380]\n",
    "\n",
    "                landmarks = face_landmarks.landmark\n",
    "                right_eye_points = [landmarks[i] for i in RIGHT_EYE_EAR_POINTS_INDICES]\n",
    "                left_eye_points = [landmarks[i] for i in LEFT_EYE_EAR_POINTS_INDICES]\n",
    "\n",
    "                right_ear = self.get_ear(right_eye_points, w, h)\n",
    "                left_ear = self.get_ear(left_eye_points, w, h)\n",
    "                avg_ear = (left_ear + right_ear) / 2.0\n",
    "                \n",
    "                faces_data.append({'box': box, 'center': center, 'avg_ear': avg_ear})\n",
    "        \n",
    "        return faces_data\n",
    "\n",
    "    def register_person(self):\n",
    "        \"\"\"\n",
    "        Registers a person and dynamically calibrates the EAR threshold.\n",
    "        \"\"\"\n",
    "        with self.registration_lock:\n",
    "            if self.current_frame_for_registration is None:\n",
    "                messagebox.showwarning(\"Registration Error\", \"Camera not ready. Please try again.\")\n",
    "                return\n",
    "            \n",
    "            rgb_frame_reg = cv2.cvtColor(self.current_frame_for_registration, cv2.COLOR_BGR2RGB)\n",
    "            results = self.face_mesh.process(rgb_frame_reg)\n",
    "            faces = self.get_faces_from_results(self.current_frame_for_registration, results)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            messagebox.showwarning(\"Registration Error\", \"No person detected. Please face the camera and try again.\")\n",
    "            return\n",
    "\n",
    "        faces_by_area = sorted(faces, key=lambda f: f['box'][2] * f['box'][3], reverse=True)\n",
    "        largest_face = faces_by_area[0]\n",
    "\n",
    "        # --- DYNAMIC CALIBRATION ---\n",
    "        current_ear = largest_face['avg_ear']\n",
    "        \n",
    "        if current_ear < MIN_OPEN_EYE_EAR:\n",
    "            messagebox.showwarning(\"Registration Error\", f\"Eyes seem to be closed (EAR: {current_ear:.2f}).\\nPlease open your eyes and try again.\")\n",
    "            return\n",
    "\n",
    "        self.calibrated_open_ear = current_ear\n",
    "        self.calibrated_threshold = current_ear * THRESHOLD_CALIBRATION_FACTOR\n",
    "        \n",
    "        name = simpledialog.askstring(\"Register Person\", \"Enter the person's name:\", parent=self.window)\n",
    "        if not name:\n",
    "            messagebox.showwarning(\"Registration Cancelled\", \"Registration was cancelled.\")\n",
    "            return\n",
    "        \n",
    "        # --- Lock in the registration ---\n",
    "        (x, y, w, h) = largest_face['box']\n",
    "        self.is_monitoring = True\n",
    "        self.registered_person_name = name\n",
    "        self.registered_face_center_guess = largest_face['center']\n",
    "        self.eye_closed_counter = 0\n",
    "        self.lost_tracker_counter = 0 # Reset grace period counter\n",
    "\n",
    "        status = f\"Calibrated for {name}. Open EAR: {self.calibrated_open_ear:.2f}, Threshold: {self.calibrated_threshold:.2f}\"\n",
    "        self.status_text.set(status)\n",
    "        messagebox.showinfo(\"Registration Complete\", status)\n",
    "\n",
    "    def clear_registration(self):\n",
    "        \"\"\"\n",
    "        Clears the current registration and stops monitoring.\n",
    "        \"\"\"\n",
    "        if not self.is_monitoring:\n",
    "            messagebox.showwarning(\"Info\", \"No person is currently registered.\")\n",
    "            return\n",
    "        \n",
    "        # Reset all logic states\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None\n",
    "        self.eye_closed_counter = 0\n",
    "        self.calibrated_open_ear = 0.0\n",
    "        self.calibrated_threshold = 0.0\n",
    "        self.lost_tracker_counter = 0\n",
    "        self.alert_sound_playing = False\n",
    "\n",
    "        self.status_text.set(\"Monitoring stopped. Ready to register a new person.\")\n",
    "\n",
    "\n",
    "    def process_frame_logic(self, frame, rgb_frame):\n",
    "        \"\"\"\n",
    "        Main logic: finds faces, tracks registered person, checks EAR.\n",
    "        \"\"\"\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        faces = self.get_faces_from_results(frame, results)\n",
    "\n",
    "        # If not monitoring, just show all faces\n",
    "        if not self.is_monitoring:\n",
    "            current_status = \"Click 'Register & Monitor' to begin.\"\n",
    "            for face_data in faces:\n",
    "                (x, y, w, h) = face_data['box']\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2)\n",
    "            return frame, current_status\n",
    "\n",
    "        # --- If we ARE monitoring ---\n",
    "        \n",
    "        # --- NEW TRACKING LOGIC ---\n",
    "        tracked_face_data = None\n",
    "        current_center = None\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            # Find the face closest to our last known center\n",
    "            min_dist = float('inf')\n",
    "            for face_data in faces:\n",
    "                center = face_data['center']\n",
    "                dist = np.linalg.norm(np.array(center) - np.array(self.registered_face_center_guess))\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    tracked_face_data = face_data\n",
    "                    current_center = center\n",
    "            \n",
    "            if min_dist > MAX_TRACKING_JUMP_PX:\n",
    "                # The closest face is too far, it's probably someone else\n",
    "                tracked_face_data = None\n",
    "        \n",
    "        if tracked_face_data is None:\n",
    "            # --- We did NOT find our person this frame ---\n",
    "            self.lost_tracker_counter += 1\n",
    "            \n",
    "            if self.lost_tracker_counter > LOST_GRACE_FRAMES:\n",
    "                # We've lost them for too long\n",
    "                current_status = f\"MONITORING: {self.registered_person_name} lost!\"\n",
    "                self.eye_closed_counter = 0\n",
    "                self.alert_sound_playing = False\n",
    "            else:\n",
    "                # We are in the grace period, keep searching\n",
    "                current_status = f\"MONITORING: Searching for {self.registered_person_name}...\"\n",
    "            \n",
    "            return frame, current_status\n",
    "        \n",
    "        # --- We DID find our person (tracked_face_data is not None) ---\n",
    "        self.lost_tracker_counter = 0 # We found them, reset grace period\n",
    "        self.registered_face_center_guess = current_center\n",
    "        (x, y, w, h) = tracked_face_data['box']\n",
    "        avg_ear = tracked_face_data['avg_ear']\n",
    "        alert_triggered = False\n",
    "        \n",
    "        # --- CALIBRATED Eye Aspect Ratio (EAR) Logic ---\n",
    "        if avg_ear < self.calibrated_threshold:\n",
    "            self.eye_closed_counter += 1\n",
    "        else:\n",
    "            self.eye_closed_counter = 0\n",
    "            self.alert_sound_playing = False # Reset sound flag\n",
    "\n",
    "            # --- NEW: Continuous Re-calibration ---\n",
    "            # Slowly adjust the baseline \"open\" EAR value to account for\n",
    "            # changes in lighting, head angle, glasses, etc.\n",
    "            alpha = 0.01 # Small learning rate\n",
    "            self.calibrated_open_ear = (self.calibrated_open_ear * (1 - alpha)) + (avg_ear * alpha)\n",
    "            self.calibrated_threshold = self.calibrated_open_ear * THRESHOLD_CALIBRATION_FACTOR\n",
    "        \n",
    "        if self.eye_closed_counter > EYE_AR_CONSEC_FRAMES:\n",
    "            current_status = f\"!!! ALERT: {self.registered_person_name} EYES CLOSED !!!\"\n",
    "            alert_triggered = True\n",
    "        else:\n",
    "            # Don't override the status if we're alerting\n",
    "            current_status = f\"Monitoring: {self.registered_person_name} (Eyes Open)\"\n",
    "        \n",
    "        # --- Draw on the frame ---\n",
    "        if alert_triggered:\n",
    "            color = (0, 0, 255) # Red for Alert\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 3)\n",
    "            cv2.putText(frame, f\"ALERT: {self.registered_person_name} (EYES CLOSED)\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # --- NEW: Play Sound Alert ---\n",
    "            if not self.alert_sound_playing:\n",
    "                # Start playing the sound in a non-blocking thread\n",
    "                # This prevents the video from freezing\n",
    "                threading.Thread(target=lambda: beep.beep(sound=3), daemon=True).start()\n",
    "                self.alert_sound_playing = True # Set flag so it only beeps once\n",
    "        else:\n",
    "            color = (255, 0, 0) # Blue for Tracking\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(frame, self.registered_person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # Show the live data for tuning\n",
    "            ear_text = f\"EAR: {avg_ear:.2f} (Thresh: {self.calibrated_threshold:.2f})\"\n",
    "            count_text = f\"Eye Closed Count: {self.eye_closed_counter}/{EYE_AR_CONSEC_FRAMES}\"\n",
    "            cv2.putText(frame, ear_text, (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, count_text, (x, y + h + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        return frame, current_status\n",
    "\n",
    "\n",
    "    def on_closing(self):\n",
    "        \"\"\"\n",
    "        Handles the window close event.\n",
    "        \"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except ImportError:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(\"Could not import 'beepy'.\")\n",
    "        print(\"Please install it by running: pip install beepy\")\n",
    "        print(\"---------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8735ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp  # --- Import MediaPipe ---\n",
    "from playsound import playsound # --- NEW: Replaced beepy with playsound ---\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# --- NEW: Path to the alert sound ---\n",
    "# NOTE: Change this to the full path of your .wav file\n",
    "# e.g., \"D:\\\\GIT_HUB\\\\12_Final_Projects_of_all\\\\03_deep_learning\\\\beep-01a.wav\"\n",
    "# Make sure to use double backslashes (\\\\) on Windows!\n",
    "ALERT_SOUND_FILE_PATH = \"D:\\\\GIT_HUB\\\\12_Final_Projects_of_all\\\\03_deep_learning\\\\beep-01a.wav\"\n",
    "\n",
    "\n",
    "# --- New Dynamic Calibration Constants ---\n",
    "# We no longer use a fixed EAR threshold.\n",
    "# Instead, we set a personal threshold as a percentage of the user's open-eye value.\n",
    "# e.g., if open EAR is 0.30, threshold will be 0.30 * 0.75 = 0.225\n",
    "THRESHOLD_CALIBRATION_FACTOR = 0.75\n",
    "\n",
    "# Sanity check: if the user's \"open\" EAR is below this,\n",
    "# they probably tried to register with their eyes closed.\n",
    "MIN_OPEN_EYE_EAR = 0.20\n",
    "\n",
    "# How many consecutive frames of \"closed\" eyes trigger an alarm?\n",
    "# User requested 10 seconds. Assuming ~20fps, 10 * 20 = 200 frames.\n",
    "EYE_AR_CONSEC_FRAMES = 200\n",
    "\n",
    "# --- NEW: Tracking \"Grace Period\" ---\n",
    "# How many frames to keep searching for a person before declaring them \"lost\".\n",
    "LOST_GRACE_FRAMES = 50 # ~2.5 seconds at 20fps\n",
    "\n",
    "# Tracking constant: max distance a face can \"jump\" between frames.\n",
    "MAX_TRACKING_JUMP_PX = 150\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Sleeping Alert System (Calibrated + Sound File)\")\n",
    "        self.window.geometry(\"800x700\")\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- MediaPipe Face Mesh Initialization ---\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=5,  # Detect up to 5 faces\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # --- Logic & Registration State ---\n",
    "        self.eye_closed_counter = 0\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None\n",
    "        self.current_frame_for_registration = None\n",
    "        self.registration_lock = threading.Lock()\n",
    "\n",
    "        # --- New Calibration State Variables ---\n",
    "        self.calibrated_open_ear = 0.0\n",
    "        self.calibrated_threshold = 0.0\n",
    "        \n",
    "        # --- NEW: State variables for improved tracking and sound ---\n",
    "        self.lost_tracker_counter = 0   # For grace period\n",
    "        self.alert_sound_playing = False # To prevent continuous beeping\n",
    "\n",
    "        # --- GUI Elements ---\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "        # --- GUI Layout Fix: Anchor controls to BOTTOM ---\n",
    "        \n",
    "        self.button_frame = tk.Frame(self.main_frame)\n",
    "        self.button_frame.pack(side=tk.BOTTOM, pady=10)\n",
    "\n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.main_frame, textvariable=self.status_text, font=(\"Arial\", 14))\n",
    "        self.status_label.pack(side=tk.BOTTOM, pady=5)\n",
    "\n",
    "        # Video label expands to fill the remaining space\n",
    "        self.video_label = tk.Label(self.main_frame, bg=\"black\")\n",
    "        self.video_label.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # --- Buttons ---\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.register_person, font=(\"Arial\", 12), width=18)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear Registration\", command=self.clear_registration, font=(\"Arial\", 12), width=18)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        \"\"\"\n",
    "        Starts the video capture in a new thread.\n",
    "        \"\"\"\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            \n",
    "            self.is_running = True\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            self.status_text.set(\"Camera running. Click 'Register & Monitor'.\")\n",
    "        \n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap:\n",
    "                self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        \"\"\"\n",
    "        Signals the video loop to stop and resets all states.\n",
    "        \"\"\"\n",
    "        if not self.is_running:\n",
    "            return\n",
    "            \n",
    "        self.is_running = False\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "            \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "\n",
    "        self.video_label.config(image=None)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        \n",
    "        # Reset all logic states\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None\n",
    "        self.eye_closed_counter = 0\n",
    "        self.calibrated_open_ear = 0.0\n",
    "        self.calibrated_threshold = 0.0\n",
    "        self.lost_tracker_counter = 0\n",
    "        self.alert_sound_playing = False\n",
    "\n",
    "    def video_loop(self):\n",
    "        \"\"\"\n",
    "        The main loop for video processing. Runs in a separate thread.\n",
    "        \"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                with self.registration_lock:\n",
    "                    self.current_frame_for_registration = frame.copy()\n",
    "                \n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Process the frame and get status\n",
    "                processed_frame, status = self.process_frame_logic(frame, rgb_frame)\n",
    "\n",
    "                self.status_text.set(status)\n",
    "\n",
    "                # Convert for Tkinter display\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                \n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1:\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "\n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            time.sleep(0.01)\n",
    "\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    def get_ear(self, eye_points, w, h):\n",
    "        \"\"\"\n",
    "        Calculates the Eye Aspect Ratio (EAR) given 6 landmark points.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert landmarks to (x, y) tuples\n",
    "            p1 = (int(eye_points[0].x * w), int(eye_points[0].y * h))\n",
    "            p4 = (int(eye_points[1].x * w), int(eye_points[1].y * h))\n",
    "            p2 = (int(eye_points[2].x * w), int(eye_points[2].y * h))\n",
    "            p6 = (int(eye_points[3].x * w), int(eye_points[3].y * h))\n",
    "            p3 = (int(eye_points[4].x * w), int(eye_points[4].y * h))\n",
    "            p5 = (int(eye_points[5].x * w), int(eye_points[5].y * h))\n",
    "\n",
    "            def get_dist(p_a, p_b):\n",
    "                return np.linalg.norm(np.array(p_a) - np.array(p_b))\n",
    "\n",
    "            v_dist_1 = get_dist(p2, p6)\n",
    "            v_dist_2 = get_dist(p3, p5)\n",
    "            h_dist = get_dist(p1, p4)\n",
    "\n",
    "            if h_dist == 0:\n",
    "                return 0.3 # Default \"open\"\n",
    "\n",
    "            ear = (v_dist_1 + v_dist_2) / (2.0 * h_dist)\n",
    "            return ear\n",
    "        except Exception:\n",
    "            return 0.3 # Default \"open\"\n",
    "\n",
    "    def get_faces_from_results(self, frame, results):\n",
    "        \"\"\"\n",
    "        Extracts face data (box, center, EAR) from MediaPipe results.\n",
    "        \"\"\"\n",
    "        faces_data = []\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # 1. Get Bounding Box\n",
    "                x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    if x < x_min: x_min = x\n",
    "                    if y < y_min: y_min = y\n",
    "                    if x > x_max: x_max = x\n",
    "                    if y > y_max: y_max = y\n",
    "                \n",
    "                padding = 10\n",
    "                x_min = max(0, x_min - padding)\n",
    "                y_min = max(0, y_min - padding)\n",
    "                x_max = min(w, x_max + padding)\n",
    "                y_max = min(h, y_max + padding)\n",
    "                box = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "\n",
    "                # 2. Get Nose Tip (Landmark 1) for tracking\n",
    "                nose_tip = face_landmarks.landmark[1]\n",
    "                center = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "                \n",
    "                # 3. Calculate Eye Aspect Ratio (EAR)\n",
    "                RIGHT_EYE_EAR_POINTS_INDICES = [33, 133, 159, 145, 158, 153]\n",
    "                LEFT_EYE_EAR_POINTS_INDICES = [362, 263, 386, 374, 385, 380]\n",
    "\n",
    "                landmarks = face_landmarks.landmark\n",
    "                right_eye_points = [landmarks[i] for i in RIGHT_EYE_EAR_POINTS_INDICES]\n",
    "                left_eye_points = [landmarks[i] for i in LEFT_EYE_EAR_POINTS_INDICES]\n",
    "\n",
    "                right_ear = self.get_ear(right_eye_points, w, h)\n",
    "                left_ear = self.get_ear(left_eye_points, w, h)\n",
    "                avg_ear = (left_ear + right_ear) / 2.0\n",
    "                \n",
    "                faces_data.append({'box': box, 'center': center, 'avg_ear': avg_ear})\n",
    "        \n",
    "        return faces_data\n",
    "\n",
    "    def register_person(self):\n",
    "        \"\"\"\n",
    "        Registers a person and dynamically calibrates the EAR threshold.\n",
    "        \"\"\"\n",
    "        with self.registration_lock:\n",
    "            if self.current_frame_for_registration is None:\n",
    "                messagebox.showwarning(\"Registration Error\", \"Camera not ready. Please try again.\")\n",
    "                return\n",
    "            \n",
    "            rgb_frame_reg = cv2.cvtColor(self.current_frame_for_registration, cv2.COLOR_BGR2RGB)\n",
    "            results = self.face_mesh.process(rgb_frame_reg)\n",
    "            faces = self.get_faces_from_results(self.current_frame_for_registration, results)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            messagebox.showwarning(\"Registration Error\", \"No person detected. Please face the camera and try again.\")\n",
    "            return\n",
    "\n",
    "        faces_by_area = sorted(faces, key=lambda f: f['box'][2] * f['box'][3], reverse=True)\n",
    "        largest_face = faces_by_area[0]\n",
    "\n",
    "        # --- DYNAMIC CALIBRATION ---\n",
    "        current_ear = largest_face['avg_ear']\n",
    "        \n",
    "        if current_ear < MIN_OPEN_EYE_EAR:\n",
    "            messagebox.showwarning(\"Registration Error\", f\"Eyes seem to be closed (EAR: {current_ear:.2f}).\\nPlease open your eyes and try again.\")\n",
    "            return\n",
    "\n",
    "        self.calibrated_open_ear = current_ear\n",
    "        self.calibrated_threshold = current_ear * THRESHOLD_CALIBRATION_FACTOR\n",
    "        \n",
    "        name = simpledialog.askstring(\"Register Person\", \"Enter the person's name:\", parent=self.window)\n",
    "        if not name:\n",
    "            messagebox.showwarning(\"Registration Cancelled\", \"Registration was cancelled.\")\n",
    "            return\n",
    "        \n",
    "        # --- Lock in the registration ---\n",
    "        (x, y, w, h) = largest_face['box']\n",
    "        self.is_monitoring = True\n",
    "        self.registered_person_name = name\n",
    "        self.registered_face_center_guess = largest_face['center']\n",
    "        self.eye_closed_counter = 0\n",
    "        self.lost_tracker_counter = 0 # Reset grace period counter\n",
    "\n",
    "        status = f\"Calibrated for {name}. Open EAR: {self.calibrated_open_ear:.2f}, Threshold: {self.calibrated_threshold:.2f}\"\n",
    "        self.status_text.set(status)\n",
    "        messagebox.showinfo(\"Registration Complete\", status)\n",
    "\n",
    "    def clear_registration(self):\n",
    "        \"\"\"\n",
    "        Clears the current registration and stops monitoring.\n",
    "        \"\"\"\n",
    "        if not self.is_monitoring:\n",
    "            messagebox.showwarning(\"Info\", \"No person is currently registered.\")\n",
    "            return\n",
    "        \n",
    "        # Reset all logic states\n",
    "        self.is_monitoring = False\n",
    "        self.registered_face_center_guess = None\n",
    "        self.registered_person_name = None\n",
    "        self.eye_closed_counter = 0\n",
    "        self.calibrated_open_ear = 0.0\n",
    "        self.calibrated_threshold = 0.0\n",
    "        self.lost_tracker_counter = 0\n",
    "        self.alert_sound_playing = False\n",
    "\n",
    "        self.status_text.set(\"Monitoring stopped. Ready to register a new person.\")\n",
    "\n",
    "    # --- NEW: Helper function to play sound in a thread ---\n",
    "    def play_alert_sound(self):\n",
    "        \"\"\"\n",
    "        Plays the alert sound file.\n",
    "        Includes a try-except block in case the file is missing or invalid.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(ALERT_SOUND_FILE_PATH):\n",
    "            print(f\"Alert Sound Error: File not found at {ALERT_SOUND_FILE_PATH}\")\n",
    "            print(\"!!! ALERT (SOUND FILE MISSING) !!!\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            playsound(ALERT_SOUND_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            print(f\"Error playing sound file {ALERT_SOUND_FILE_PATH}: {e}\")\n",
    "            # Fallback to a simple print if sound fails\n",
    "            print(\"!!! ALERT BEEP !!!\")\n",
    "\n",
    "    def process_frame_logic(self, frame, rgb_frame):\n",
    "        \"\"\"\n",
    "        Main logic: finds faces, tracks registered person, checks EAR.\n",
    "        \"\"\"\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        faces = self.get_faces_from_results(frame, results)\n",
    "\n",
    "        # If not monitoring, just show all faces\n",
    "        if not self.is_monitoring:\n",
    "            current_status = \"Click 'Register & Monitor' to begin.\"\n",
    "            for face_data in faces:\n",
    "                (x, y, w, h) = face_data['box']\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2)\n",
    "            return frame, current_status\n",
    "\n",
    "        # --- If we ARE monitoring ---\n",
    "        \n",
    "        # --- NEW TRACKING LOGIC ---\n",
    "        tracked_face_data = None\n",
    "        current_center = None\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            # Find the face closest to our last known center\n",
    "            min_dist = float('inf')\n",
    "            for face_data in faces:\n",
    "                center = face_data['center']\n",
    "                dist = np.linalg.norm(np.array(center) - np.array(self.registered_face_center_guess))\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    tracked_face_data = face_data\n",
    "                    current_center = center\n",
    "            \n",
    "            if min_dist > MAX_TRACKING_JUMP_PX:\n",
    "                # The closest face is too far, it's probably someone else\n",
    "                tracked_face_data = None\n",
    "        \n",
    "        if tracked_face_data is None:\n",
    "            # --- We did NOT find our person this frame ---\n",
    "            self.lost_tracker_counter += 1\n",
    "            \n",
    "            if self.lost_tracker_counter > LOST_GRACE_FRAMES:\n",
    "                # We've lost them for too long\n",
    "                current_status = f\"MONITORING: {self.registered_person_name} lost!\"\n",
    "                self.eye_closed_counter = 0\n",
    "                self.alert_sound_playing = False\n",
    "            else:\n",
    "                # We are in the grace period, keep searching\n",
    "                current_status = f\"MONITORING: Searching for {self.registered_person_name}...\"\n",
    "            \n",
    "            return frame, current_status\n",
    "        \n",
    "        # --- We DID find our person (tracked_face_data is not None) ---\n",
    "        self.lost_tracker_counter = 0 # We found them, reset grace period\n",
    "        self.registered_face_center_guess = current_center\n",
    "        (x, y, w, h) = tracked_face_data['box']\n",
    "        avg_ear = tracked_face_data['avg_ear']\n",
    "        alert_triggered = False\n",
    "        \n",
    "        # --- CALIBRATED Eye Aspect Ratio (EAR) Logic ---\n",
    "        if avg_ear < self.calibrated_threshold:\n",
    "            self.eye_closed_counter += 1\n",
    "        else:\n",
    "            self.eye_closed_counter = 0\n",
    "            self.alert_sound_playing = False # Reset sound flag\n",
    "\n",
    "            # --- NEW: Continuous Re-calibration ---\n",
    "            # Slowly adjust the baseline \"open\" EAR value to account for\n",
    "            # changes in lighting, head angle, glasses, etc.\n",
    "            alpha = 0.01 # Small learning rate\n",
    "            self.calibrated_open_ear = (self.calibrated_open_ear * (1 - alpha)) + (avg_ear * alpha)\n",
    "            self.calibrated_threshold = self.calibrated_open_ear * THRESHOLD_CALIBRATION_FACTOR\n",
    "        \n",
    "        if self.eye_closed_counter > EYE_AR_CONSEC_FRAMES:\n",
    "            current_status = f\"!!! ALERT: {self.registered_person_name} EYES CLOSED !!!\"\n",
    "            alert_triggered = True\n",
    "        else:\n",
    "            # Don't override the status if we're alerting\n",
    "            current_status = f\"Monitoring: {self.registered_person_name} (Eyes Open)\"\n",
    "        \n",
    "        # --- Draw on the frame ---\n",
    "        if alert_triggered:\n",
    "            color = (0, 0, 255) # Red for Alert\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 3)\n",
    "            cv2.putText(frame, f\"ALERT: {self.registered_person_name} (EYES CLOSED)\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # --- MODIFIED: Play Sound Alert ---\n",
    "            if not self.alert_sound_playing:\n",
    "                # Start playing the sound in a non-blocking thread\n",
    "                # This prevents the video from freezing\n",
    "                threading.Thread(target=self.play_alert_sound, daemon=True).start()\n",
    "                self.alert_sound_playing = True # Set flag so it only beeps once\n",
    "        else:\n",
    "            color = (255, 0, 0) # Blue for Tracking\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(frame, self.registered_person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # Show the live data for tuning\n",
    "            ear_text = f\"EAR: {avg_ear:.2f} (Thresh: {self.calibrated_threshold:.2f})\"\n",
    "            count_text = f\"Eye Closed Count: {self.eye_closed_counter}/{EYE_AR_CONSEC_FRAMES}\"\n",
    "            cv2.putText(frame, ear_text, (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, count_text, (x, y + h + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "        return frame, current_status\n",
    "\n",
    "\n",
    "    def on_closing(self):\n",
    "        \"\"\"\n",
    "        Handles the window close event.\n",
    "        \"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except ImportError:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(\"Could not import 'playsound'.\")\n",
    "        print(\"Please install it by running: pip install playsound\")\n",
    "        print(\"---------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp  # --- Import MediaPipe ---\n",
    "import winsound        # --- NEW: Replaced playsound with winsound (built-in on Windows) ---\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# --- Path to the alert sound ---\n",
    "# Make sure to use double backslashes (\\\\) on Windows!\n",
    "ALERT_SOUND_FILE_PATH = \"D:\\\\GIT_HUB\\\\12_Final_Projects_of_all\\\\03_deep_learning\\\\beep-01a.wav\"\n",
    "\n",
    "# --- Dynamic Calibration Constants ---\n",
    "THRESHOLD_CALIBRATION_FACTOR = 0.75\n",
    "MIN_OPEN_EYE_EAR = 0.20\n",
    "EYE_AR_CONSEC_FRAMES = 200 # ~10 seconds at 20fps\n",
    "\n",
    "# --- Tracking Constants ---\n",
    "LOST_GRACE_FRAMES = 50 # ~2.5 seconds\n",
    "MAX_TRACKING_JUMP_PX = 150 # Max distance a face can move between frames\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Multi-Person Sleeping Alert System\")\n",
    "        self.window.geometry(\"1000x700\") # Made window wider for sidebar\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- MediaPipe Face Mesh Initialization ---\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=10,  # Detect more faces\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # --- NEW: Multi-Person Logic & Registration State ---\n",
    "        # Replaces all single-person variables\n",
    "        self.registered_people = {} # Main dictionary to hold all registered people\n",
    "        self.registration_lock = threading.Lock()\n",
    "        self.current_frame_for_registration = None\n",
    "\n",
    "        # --- GUI Elements ---\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "\n",
    "        # --- Bottom Frame (Buttons & Status) ---\n",
    "        self.bottom_frame = tk.Frame(self.main_frame)\n",
    "        self.bottom_frame.pack(side=tk.BOTTOM, fill=tk.X, pady=5)\n",
    "        \n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.bottom_frame, textvariable=self.status_text, font=(\"Arial\", 14))\n",
    "        self.status_label.pack(side=tk.LEFT, padx=10)\n",
    "        \n",
    "        self.button_frame = tk.Frame(self.bottom_frame)\n",
    "        self.button_frame.pack(side=tk.RIGHT)\n",
    "\n",
    "        # --- Content Frame (Video & Sidebar) ---\n",
    "        self.content_frame = tk.Frame(self.main_frame)\n",
    "        self.content_frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Video label expands to fill the remaining space\n",
    "        self.video_label = tk.Label(self.content_frame, bg=\"black\")\n",
    "        self.video_label.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # --- NEW: Sidebar for Registered People List ---\n",
    "        self.sidebar_frame = tk.Frame(self.content_frame, width=250, bg=\"#f0f0f0\", relief=tk.SUNKEN, borderwidth=2)\n",
    "        self.sidebar_frame.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        self.sidebar_frame.pack_propagate(False) # Prevent frame from shrinking\n",
    "\n",
    "        self.sidebar_title = tk.Label(self.sidebar_frame, text=\"Registered People\", font=(\"Arial\", 16, \"bold\"), bg=\"#f0f0f0\")\n",
    "        self.sidebar_title.pack(pady=10, padx=10, anchor=\"w\")\n",
    "\n",
    "        self.registered_list_var = tk.StringVar()\n",
    "        self.registered_list_display = tk.Label(self.sidebar_frame, textvariable=self.registered_list_var, font=(\"Arial\", 12), bg=\"#f0f0f0\", justify=tk.LEFT, anchor=\"nw\")\n",
    "        self.registered_list_display.pack(pady=5, padx=10, fill=tk.X, anchor=\"nw\")\n",
    "        self.registered_list_var.set(\"None\")\n",
    "\n",
    "        # --- Buttons (in the bottom_frame) ---\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.register_person, font=(\"Arial\", 12), width=18)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        # Renamed \"Clear Registration\" to \"Clear All\"\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear All\", command=self.clear_all_registrations, font=(\"Arial\", 12), width=18)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        if self.is_running:\n",
    "            return\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            self.is_running = True\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            self.status_text.set(\"Camera running. Click 'Register & Monitor'.\")\n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap: self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        if not self.is_running:\n",
    "            return\n",
    "        self.is_running = False\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "        self.video_label.config(image=None)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        self.clear_all_registrations() # Clear people when camera stops\n",
    "\n",
    "    def video_loop(self):\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                with self.registration_lock:\n",
    "                    self.current_frame_for_registration = frame.copy()\n",
    "                \n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                processed_frame, status, list_text = self.process_frame_logic(frame, rgb_frame)\n",
    "                self.status_text.set(status)\n",
    "                self.registered_list_var.set(list_text if list_text else \"None\")\n",
    "\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1:\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            time.sleep(0.01)\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    def get_ear(self, eye_points, w, h):\n",
    "        try:\n",
    "            p1 = (int(eye_points[0].x * w), int(eye_points[0].y * h))\n",
    "            p4 = (int(eye_points[1].x * w), int(eye_points[1].y * h))\n",
    "            p2 = (int(eye_points[2].x * w), int(eye_points[2].y * h))\n",
    "            p6 = (int(eye_points[3].x * w), int(eye_points[3].y * h))\n",
    "            p3 = (int(eye_points[4].x * w), int(eye_points[4].y * h))\n",
    "            p5 = (int(eye_points[5].x * w), int(eye_points[5].y * h))\n",
    "            def get_dist(p_a, p_b):\n",
    "                return np.linalg.norm(np.array(p_a) - np.array(p_b))\n",
    "            v_dist_1 = get_dist(p2, p6)\n",
    "            v_dist_2 = get_dist(p3, p5)\n",
    "            h_dist = get_dist(p1, p4)\n",
    "            if h_dist == 0: return 0.3\n",
    "            ear = (v_dist_1 + v_dist_2) / (2.0 * h_dist)\n",
    "            return ear\n",
    "        except Exception:\n",
    "            return 0.3\n",
    "\n",
    "    def get_faces_from_results(self, frame, results):\n",
    "        faces_data = []\n",
    "        h, w, _ = frame.shape\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    if x < x_min: x_min = x\n",
    "                    if y < y_min: y_min = y\n",
    "                    if x > x_max: x_max = x\n",
    "                    if y > y_max: y_max = y\n",
    "                padding = 10\n",
    "                x_min = max(0, x_min - padding)\n",
    "                y_min = max(0, y_min - padding)\n",
    "                x_max = min(w, x_max + padding)\n",
    "                y_max = min(h, y_max + padding)\n",
    "                box = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "                nose_tip = face_landmarks.landmark[1]\n",
    "                center = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "                RIGHT_EYE_EAR_POINTS_INDICES = [33, 133, 159, 145, 158, 153]\n",
    "                LEFT_EYE_EAR_POINTS_INDICES = [362, 263, 386, 374, 385, 380]\n",
    "                landmarks = face_landmarks.landmark\n",
    "                right_eye_points = [landmarks[i] for i in RIGHT_EYE_EAR_POINTS_INDICES]\n",
    "                left_eye_points = [landmarks[i] for i in LEFT_EYE_EAR_POINTS_INDICES]\n",
    "                right_ear = self.get_ear(right_eye_points, w, h)\n",
    "                left_ear = self.get_ear(left_eye_points, w, h)\n",
    "                avg_ear = (left_ear + right_ear) / 2.0\n",
    "                faces_data.append({'box': box, 'center': center, 'avg_ear': avg_ear})\n",
    "        return faces_data\n",
    "\n",
    "    def register_person(self):\n",
    "        with self.registration_lock:\n",
    "            if self.current_frame_for_registration is None:\n",
    "                messagebox.showwarning(\"Registration Error\", \"Camera not ready. Please try again.\")\n",
    "                return\n",
    "            rgb_frame_reg = cv2.cvtColor(self.current_frame_for_registration, cv2.COLOR_BGR2RGB)\n",
    "            results = self.face_mesh.process(rgb_frame_reg)\n",
    "            faces = self.get_faces_from_results(self.current_frame_for_registration, results)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            messagebox.showwarning(\"Registration Error\", \"No person detected. Please face the camera and try again.\")\n",
    "            return\n",
    "\n",
    "        faces_by_area = sorted(faces, key=lambda f: f['box'][2] * f['box'][3], reverse=True)\n",
    "        largest_face = faces_by_area[0]\n",
    "        current_ear = largest_face['avg_ear']\n",
    "        \n",
    "        if current_ear < MIN_OPEN_EYE_EAR:\n",
    "            messagebox.showwarning(\"Registration Error\", f\"Eyes seem to be closed (EAR: {current_ear:.2f}).\\nPlease open your eyes and try again.\")\n",
    "            return\n",
    "\n",
    "        calibrated_threshold = current_ear * THRESHOLD_CALIBRATION_FACTOR\n",
    "        name = simpledialog.askstring(\"Register Person\", \"Enter a unique name:\", parent=self.window)\n",
    "        \n",
    "        if not name:\n",
    "            messagebox.showwarning(\"Registration Cancelled\", \"Registration was cancelled.\")\n",
    "            return\n",
    "            \n",
    "        if name in self.registered_people:\n",
    "            messagebox.showwarning(\"Registration Error\", f\"The name '{name}' is already registered. Please use a unique name.\")\n",
    "            return\n",
    "        \n",
    "        # --- Add new person to the dictionary ---\n",
    "        self.registered_people[name] = {\n",
    "            'center': largest_face['center'],\n",
    "            'open_ear': current_ear,\n",
    "            'threshold': calibrated_threshold,\n",
    "            'eye_counter': 0,\n",
    "            'lost_counter': 0,\n",
    "            'alert_sounding': False,\n",
    "            'status': 'Calibrated' # For the sidebar list\n",
    "        }\n",
    "\n",
    "        status = f\"Calibrated for {name}. Open EAR: {current_ear:.2f}, Threshold: {calibrated_threshold:.2f}\"\n",
    "        self.status_text.set(f\"Registered {name}. Monitoring {len(self.registered_people)} person(s).\")\n",
    "        messagebox.showinfo(\"Registration Complete\", status)\n",
    "\n",
    "    def clear_all_registrations(self):\n",
    "        if not self.registered_people:\n",
    "            # No need to show a warning if it's already empty\n",
    "            return\n",
    "        \n",
    "        self.registered_people = {}\n",
    "        self.status_text.set(\"All registrations cleared. Ready to register.\")\n",
    "        self.registered_list_var.set(\"None\")\n",
    "\n",
    "    # --- Replaced play_alert_sound with winsound ---\n",
    "    def play_alert_sound(self):\n",
    "        if not os.path.exists(ALERT_SOUND_FILE_PATH):\n",
    "            print(f\"Alert Sound Error: File not found at {ALERT_SOUND_FILE_PATH}\")\n",
    "            print(\"!!! ALERT (SOUND FILE MISSING) !!!\")\n",
    "            return\n",
    "        try:\n",
    "            # Play sound asynchronously to not block the main thread\n",
    "            winsound.PlaySound(ALERT_SOUND_FILE_PATH, winsound.SND_FILENAME | winsound.SND_ASYNC)\n",
    "        except Exception as e:\n",
    "            print(f\"Error playing sound file {ALERT_SOUND_FILE_PATH}: {e}\")\n",
    "            print(\"!!! ALERT BEEP !!!\")\n",
    "\n",
    "    # --- NEW: Helper function to draw on face and check logic ---\n",
    "    def process_person(self, frame, person_name, person_data, face_data):\n",
    "        \"\"\"\n",
    "        Processes a single person, either found (face_data not None) or lost.\n",
    "        Updates their state and draws on the frame.\n",
    "        \"\"\"\n",
    "        alert_triggered = False\n",
    "        \n",
    "        if face_data is None:\n",
    "            # --- Person is LOST ---\n",
    "            person_data['lost_counter'] += 1\n",
    "            if person_data['lost_counter'] > LOST_GRACE_FRAMES:\n",
    "                person_data['status'] = 'Lost'\n",
    "                person_data['eye_counter'] = 0\n",
    "                person_data['alert_sounding'] = False\n",
    "            else:\n",
    "                person_data['status'] = 'Searching...'\n",
    "            return # Stop processing this person\n",
    "        \n",
    "        # --- Person is FOUND ---\n",
    "        person_data['lost_counter'] = 0\n",
    "        person_data['center'] = face_data['center']\n",
    "        avg_ear = face_data['avg_ear']\n",
    "        \n",
    "        if avg_ear < person_data['threshold']:\n",
    "            person_data['eye_counter'] += 1\n",
    "        else:\n",
    "            person_data['eye_counter'] = 0\n",
    "            person_data['alert_sounding'] = False\n",
    "            # Continuous Re-calibration\n",
    "            alpha = 0.01\n",
    "            person_data['open_ear'] = (person_data['open_ear'] * (1 - alpha)) + (avg_ear * alpha)\n",
    "            person_data['threshold'] = person_data['open_ear'] * THRESHOLD_CALIBRATION_FACTOR\n",
    "        \n",
    "        if person_data['eye_counter'] > EYE_AR_CONSEC_FRAMES:\n",
    "            person_data['status'] = '!!! ALERT !!!'\n",
    "            alert_triggered = True\n",
    "        else:\n",
    "            person_data['status'] = 'Tracking'\n",
    "        \n",
    "        # --- Draw on Frame ---\n",
    "        (x, y, w, h) = face_data['box']\n",
    "        \n",
    "        if alert_triggered:\n",
    "            color = (0, 0, 255) # Red\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 3)\n",
    "            cv2.putText(frame, f\"ALERT: {person_name} (EYES CLOSED)\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            if not person_data['alert_sounding']:\n",
    "                threading.Thread(target=self.play_alert_sound, daemon=True).start()\n",
    "                person_data['alert_sounding'] = True\n",
    "        else:\n",
    "            color = (255, 0, 0) # Blue\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(frame, person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            ear_text = f\"EAR: {avg_ear:.2f} (Th: {person_data['threshold']:.2f})\"\n",
    "            count_text = f\"Count: {person_data['eye_counter']}/{EYE_AR_CONSEC_FRAMES}\"\n",
    "            cv2.putText(frame, ear_text, (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, count_text, (x, y + h + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "\n",
    "    def process_frame_logic(self, frame, rgb_frame):\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        faces = self.get_faces_from_results(frame, results)\n",
    "\n",
    "        if not self.registered_people:\n",
    "            # Not monitoring anyone, just draw all faces\n",
    "            for face_data in faces:\n",
    "                (x, y, w, h) = face_data['box']\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2)\n",
    "            return frame, \"Click 'Register & Monitor' to begin.\", \"\"\n",
    "\n",
    "        # --- Multi-Person Tracking Logic ---\n",
    "        unmatched_face_indices = set(range(len(faces)))\n",
    "        matches = {} # Stores {person_name: face_index}\n",
    "        \n",
    "        # --- Greedily assign best match for each person ---\n",
    "        for name, person_data in self.registered_people.items():\n",
    "            best_dist = float('inf')\n",
    "            best_face_idx = -1\n",
    "            for i in unmatched_face_indices: # Only check against unused faces\n",
    "                dist = np.linalg.norm(np.array(faces[i]['center']) - np.array(person_data['center']))\n",
    "                if dist < best_dist and dist < MAX_TRACKING_JUMP_PX:\n",
    "                    best_dist = dist\n",
    "                    best_face_idx = i\n",
    "            \n",
    "            if best_face_idx != -1:\n",
    "                matches[name] = best_face_idx\n",
    "                unmatched_face_indices.remove(best_face_idx) # This face is now taken\n",
    "\n",
    "        list_display_text = []\n",
    "        # --- Process all people (matched or lost) ---\n",
    "        for name, person_data in self.registered_people.items():\n",
    "            face_data = None\n",
    "            if name in matches:\n",
    "                face_data = faces[matches[name]] # Get the matched face data\n",
    "            \n",
    "            # This function updates the person's state and draws on the frame\n",
    "            self.process_person(frame, name, person_data, face_data)\n",
    "            \n",
    "            list_display_text.append(f\"{name}: {person_data['status']}\")\n",
    "\n",
    "        # --- Draw remaining unmatched (unregistered) faces ---\n",
    "        for i in unmatched_face_indices:\n",
    "            (x, y, w, h) = faces[i]['box']\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2)\n",
    "\n",
    "        status = f\"Monitoring {len(self.registered_people)} person(s).\"\n",
    "        return frame, status, \"\\n\".join(list_display_text)\n",
    "\n",
    "    def on_closing(self):\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except ImportError:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(\"Could not import one or more required libraries.\")\n",
    "        print(\"Please ensure you have the following installed:\")\n",
    "        print(\"pip install opencv-python pillow numpy mediapipe\")\n",
    "        print(\"---------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2757fa13",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp  # --- Import MediaPipe ---\n",
    "import winsound        # --- Using built-in winsound for Windows ---\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# --- Path to the alert sound ---\n",
    "# Make sure to use double backslashes (\\\\) on Windows!\n",
    "ALERT_SOUND_FILE_PATH = \"D:\\\\GIT_HUB\\\\12_Final_Projects_of_all\\\\03_deep_learning\\\\beep-01a.wav\"\n",
    "\n",
    "# --- NEW: Sound loop configuration ---\n",
    "# Time in seconds between playing the alert sound\n",
    "ALERT_SOUND_INTERVAL = 2.0 # Play sound every 2 seconds\n",
    "\n",
    "# --- Dynamic Calibration Constants ---\n",
    "THRESHOLD_CALIBRATION_FACTOR = 0.75\n",
    "MIN_OPEN_EYE_EAR = 0.20\n",
    "EYE_AR_CONSEC_FRAMES = 200 # ~10 seconds at 20fps\n",
    "\n",
    "# --- Tracking Constants ---\n",
    "LOST_GRACE_FRAMES = 50 # ~2.5 seconds\n",
    "MAX_TRACKING_JUMP_PX = 150 # Max distance a face can move between frames\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Multi-Person Sleeping Alert System (v2.0)\")\n",
    "        self.window.geometry(\"1000x700\") # Made window wider for sidebar\n",
    "        self.window.minsize(800, 600) # Set a minimum size\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- MediaPipe Face Mesh Initialization ---\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=10,  # Detect more faces\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # --- Multi-Person Logic & Registration State ---\n",
    "        self.registered_people = {} # Main dictionary to hold all registered people\n",
    "        self.registration_lock = threading.Lock()\n",
    "        self.current_frame_for_registration = None\n",
    "\n",
    "        # --- GUI REBUILD: Using .grid() for stable layout ---\n",
    "        \n",
    "        # Configure the main window's grid\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        self.main_frame.rowconfigure(0, weight=1) # Content (video/sidebar) expands\n",
    "        self.main_frame.rowconfigure(1, weight=0) # Bottom bar does not expand\n",
    "        self.main_frame.columnconfigure(0, weight=1) # Main column expands\n",
    "        \n",
    "        # --- 1. Content Frame (Video & Sidebar) ---\n",
    "        self.content_frame = tk.Frame(self.main_frame)\n",
    "        self.content_frame.grid(row=0, column=0, sticky=\"nsew\", padx=10, pady=10)\n",
    "        self.content_frame.rowconfigure(0, weight=1)\n",
    "        self.content_frame.columnconfigure(0, weight=1) # Video expands\n",
    "        self.content_frame.columnconfigure(1, weight=0) # Sidebar does not expand\n",
    "        \n",
    "        # Video Label\n",
    "        self.video_label = tk.Label(self.content_frame, bg=\"black\")\n",
    "        self.video_label.grid(row=0, column=0, sticky=\"nsew\")\n",
    "\n",
    "        # Sidebar\n",
    "        self.sidebar_frame = tk.Frame(self.content_frame, width=250, bg=\"#f0f0f0\", relief=tk.SUNKEN, borderwidth=2)\n",
    "        self.sidebar_frame.grid(row=0, column=1, sticky=\"ns\", padx=(10, 0))\n",
    "        self.sidebar_frame.pack_propagate(False)\n",
    "\n",
    "        self.sidebar_title = tk.Label(self.sidebar_frame, text=\"Registered People\", font=(\"Arial\", 16, \"bold\"), bg=\"#f0f0f0\")\n",
    "        self.sidebar_title.pack(pady=10, padx=10, anchor=\"w\")\n",
    "\n",
    "        self.registered_list_var = tk.StringVar()\n",
    "        self.registered_list_display = tk.Label(self.sidebar_frame, textvariable=self.registered_list_var, font=(\"Arial\", 12), bg=\"#f0f0f0\", justify=tk.LEFT, anchor=\"nw\")\n",
    "        self.registered_list_display.pack(pady=5, padx=10, fill=tk.X, anchor=\"nw\")\n",
    "        self.registered_list_var.set(\"None\")\n",
    "\n",
    "        # --- 2. Bottom Frame (Status & Buttons) ---\n",
    "        self.bottom_frame = tk.Frame(self.main_frame, bg=\"#e0e0e0\")\n",
    "        self.bottom_frame.grid(row=1, column=0, sticky=\"ew\", padx=10, pady=(0, 10))\n",
    "        self.bottom_frame.columnconfigure(0, weight=1) # Status label expands\n",
    "        self.bottom_frame.columnconfigure(1, weight=0) # Buttons do not expand\n",
    "\n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.bottom_frame, textvariable=self.status_text, font=(\"Arial\", 14), bg=\"#e0e0e0\", anchor=\"w\")\n",
    "        self.status_label.grid(row=0, column=0, sticky=\"ew\", padx=10, pady=5)\n",
    "        \n",
    "        self.button_frame = tk.Frame(self.bottom_frame, bg=\"#e0e0e0\")\n",
    "        self.button_frame.grid(row=0, column=1, sticky=\"e\", padx=5, pady=5)\n",
    "\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.register_person, font=(\"Arial\", 12), width=18)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear All\", command=self.clear_all_registrations, font=(\"Arial\", 12), width=18)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        if self.is_running:\n",
    "            return\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            self.is_running = True\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            self.status_text.set(\"Camera running. Click 'Register & Monitor'.\")\n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap: self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        if not self.is_running:\n",
    "            return\n",
    "        self.is_running = False\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "        self.video_label.config(image=None)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        self.clear_all_registrations() # Clear people when camera stops\n",
    "\n",
    "    def video_loop(self):\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                with self.registration_lock:\n",
    "                    self.current_frame_for_registration = frame.copy()\n",
    "                \n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                processed_frame, status, list_text = self.process_frame_logic(frame, rgb_frame)\n",
    "                \n",
    "                # Update GUI from main thread\n",
    "                self.status_text.set(status)\n",
    "                self.registered_list_var.set(list_text if list_text else \"None\")\n",
    "\n",
    "                # Resize image\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1:\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "                \n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "                \n",
    "                # Update image\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            time.sleep(0.01) # ~100fps theoretical max, but processing slows it\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    def get_ear(self, eye_points, w, h):\n",
    "        \"\"\"Calculates Eye Aspect Ratio (EAR).\"\"\"\n",
    "        try:\n",
    "            p1 = (int(eye_points[0].x * w), int(eye_points[0].y * h))\n",
    "            p4 = (int(eye_points[1].x * w), int(eye_points[1].y * h))\n",
    "            p2 = (int(eye_points[2].x * w), int(eye_points[2].y * h))\n",
    "            p6 = (int(eye_points[3].x * w), int(eye_points[3].y * h))\n",
    "            p3 = (int(eye_points[4].x * w), int(eye_points[4].y * h))\n",
    "            p5 = (int(eye_points[5].x * w), int(eye_points[5].y * h))\n",
    "            def get_dist(p_a, p_b):\n",
    "                return np.linalg.norm(np.array(p_a) - np.array(p_b))\n",
    "            v_dist_1 = get_dist(p2, p6)\n",
    "            v_dist_2 = get_dist(p3, p5)\n",
    "            h_dist = get_dist(p1, p4)\n",
    "            if h_dist == 0: return 0.3\n",
    "            ear = (v_dist_1 + v_dist_2) / (2.0 * h_dist)\n",
    "            return ear\n",
    "        except Exception:\n",
    "            return 0.3\n",
    "\n",
    "    def get_faces_from_results(self, frame, results):\n",
    "        \"\"\"Extracts all face data (box, center, EAR) from MediaPipe results.\"\"\"\n",
    "        faces_data = []\n",
    "        h, w, _ = frame.shape\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    if x < x_min: x_min = x\n",
    "                    if y < y_min: y_min = y\n",
    "                    if x > x_max: x_max = x\n",
    "                    if y > y_max: y_max = y\n",
    "                padding = 10\n",
    "                x_min = max(0, x_min - padding)\n",
    "                y_min = max(0, y_min - padding)\n",
    "                x_max = min(w, x_max + padding)\n",
    "                y_max = min(h, y_max + padding)\n",
    "                box = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "                nose_tip = face_landmarks.landmark[1]\n",
    "                center = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "                RIGHT_EYE_EAR_POINTS_INDICES = [33, 133, 159, 145, 158, 153]\n",
    "                LEFT_EYE_EAR_POINTS_INDICES = [362, 263, 386, 374, 385, 380]\n",
    "                landmarks = face_landmarks.landmark\n",
    "                right_eye_points = [landmarks[i] for i in RIGHT_EYE_EAR_POINTS_INDICES]\n",
    "                left_eye_points = [landmarks[i] for i in LEFT_EYE_EAR_POINTS_INDICES]\n",
    "                right_ear = self.get_ear(right_eye_points, w, h)\n",
    "                left_ear = self.get_ear(left_eye_points, w, h)\n",
    "                avg_ear = (left_ear + right_ear) / 2.0\n",
    "                faces_data.append({'box': box, 'center': center, 'avg_ear': avg_ear})\n",
    "        return faces_data\n",
    "\n",
    "    def register_person(self):\n",
    "        \"\"\"Registers a new person based on the largest face in the frame.\"\"\"\n",
    "        with self.registration_lock:\n",
    "            if self.current_frame_for_registration is None:\n",
    "                messagebox.showwarning(\"Registration Error\", \"Camera not ready. Please try again.\")\n",
    "                return\n",
    "            rgb_frame_reg = cv2.cvtColor(self.current_frame_for_registration, cv2.COLOR_BGR2RGB)\n",
    "            results = self.face_mesh.process(rgb_frame_reg)\n",
    "            faces = self.get_faces_from_results(self.current_frame_for_registration, results)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            messagebox.showwarning(\"Registration Error\", \"No person detected. Please face the camera and try again.\")\n",
    "            return\n",
    "\n",
    "        faces_by_area = sorted(faces, key=lambda f: f['box'][2] * f['box'][3], reverse=True)\n",
    "        largest_face = faces_by_area[0]\n",
    "        current_ear = largest_face['avg_ear']\n",
    "        \n",
    "        if current_ear < MIN_OPEN_EYE_EAR:\n",
    "            messagebox.showwarning(\"Registration Error\", f\"Eyes seem to be closed (EAR: {current_ear:.2f}).\\nPlease open your eyes and try again.\")\n",
    "            return\n",
    "\n",
    "        calibrated_threshold = current_ear * THRESHOLD_CALIBRATION_FACTOR\n",
    "        name = simpledialog.askstring(\"Register Person\", \"Enter a unique name:\", parent=self.window)\n",
    "        \n",
    "        if not name:\n",
    "            messagebox.showwarning(\"Registration Cancelled\", \"Registration was cancelled.\")\n",
    "            return\n",
    "            \n",
    "        if name in self.registered_people:\n",
    "            messagebox.showwarning(\"Registration Error\", f\"The name '{name}' is already registered. Please use a unique name.\")\n",
    "            return\n",
    "        \n",
    "        # --- Add new person to the dictionary ---\n",
    "        self.registered_people[name] = {\n",
    "            'center': largest_face['center'],\n",
    "            'open_ear': current_ear,\n",
    "            'threshold': calibrated_threshold,\n",
    "            'eye_counter': 0,\n",
    "            'lost_counter': 0,\n",
    "            'status': 'Calibrated', # For the sidebar list\n",
    "            'last_alert_time': 0.0 # NEW: For sound loop\n",
    "        }\n",
    "\n",
    "        status = f\"Calibrated for {name}. Open EAR: {current_ear:.2f}, Threshold: {calibrated_threshold:.2f}\"\n",
    "        self.status_text.set(f\"Registered {name}. Monitoring {len(self.registered_people)} person(s).\")\n",
    "        messagebox.showinfo(\"Registration Complete\", status)\n",
    "\n",
    "    def clear_all_registrations(self):\n",
    "        \"\"\"Clears all registered people from monitoring.\"\"\"\n",
    "        if not self.registered_people:\n",
    "            return\n",
    "        self.registered_people = {}\n",
    "        self.status_text.set(\"All registrations cleared. Ready to register.\")\n",
    "        self.registered_list_var.set(\"None\")\n",
    "\n",
    "    def play_alert_sound(self):\n",
    "        \"\"\"Plays the alert sound asynchronously using winsound.\"\"\"\n",
    "        if not os.path.exists(ALERT_SOUND_FILE_PATH):\n",
    "            print(f\"Alert Sound Error: File not found at {ALERT_SOUND_FILE_PATH}\")\n",
    "            print(\"!!! ALERT (SOUND FILE MISSING) !!!\")\n",
    "            return\n",
    "        try:\n",
    "            winsound.PlaySound(ALERT_SOUND_FILE_PATH, winsound.SND_FILENAME | winsound.SND_ASYNC)\n",
    "        except Exception as e:\n",
    "            print(f\"Error playing sound file {ALERT_SOUND_FILE_PATH}: {e}\")\n",
    "            print(\"!!! ALERT BEEP !!!\")\n",
    "\n",
    "    def process_person(self, frame, person_name, person_data, face_data):\n",
    "        \"\"\"\n",
    "        Processes a single person (found or lost), updates state, and draws on frame.\n",
    "        \"\"\"\n",
    "        alert_triggered = False\n",
    "        \n",
    "        if face_data is None:\n",
    "            # --- Person is LOST ---\n",
    "            person_data['lost_counter'] += 1\n",
    "            if person_data['lost_counter'] > LOST_GRACE_FRAMES:\n",
    "                person_data['status'] = 'Lost'\n",
    "                person_data['eye_counter'] = 0\n",
    "            else:\n",
    "                person_data['status'] = 'Searching...'\n",
    "            return # Stop processing\n",
    "        \n",
    "        # --- Person is FOUND ---\n",
    "        person_data['lost_counter'] = 0\n",
    "        person_data['center'] = face_data['center']\n",
    "        avg_ear = face_data['avg_ear']\n",
    "        \n",
    "        # --- Check Eye Status ---\n",
    "        if avg_ear < person_data['threshold']:\n",
    "            person_data['eye_counter'] += 1\n",
    "        else:\n",
    "            person_data['eye_counter'] = 0\n",
    "            # Continuous Re-calibration\n",
    "            alpha = 0.01\n",
    "            person_data['open_ear'] = (person_data['open_ear'] * (1 - alpha)) + (avg_ear * alpha)\n",
    "            person_data['threshold'] = person_data['open_ear'] * THRESHOLD_CALIBRATION_FACTOR\n",
    "        \n",
    "        # --- Check Alert Status ---\n",
    "        if person_data['eye_counter'] > EYE_AR_CONSEC_FRAMES:\n",
    "            person_data['status'] = '!!! ALERT !!!'\n",
    "            alert_triggered = True\n",
    "        else:\n",
    "            person_data['status'] = 'Tracking'\n",
    "        \n",
    "        # --- Draw on Frame ---\n",
    "        (x, y, w, h) = face_data['box']\n",
    "        \n",
    "        if alert_triggered:\n",
    "            color = (0, 0, 255) # Red\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 3)\n",
    "            cv2.putText(frame, f\"ALERT: {person_name} (EYES CLOSED)\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # --- NEW: LOOPING SOUND LOGIC ---\n",
    "            current_time = time.time()\n",
    "            if current_time - person_data['last_alert_time'] > ALERT_SOUND_INTERVAL:\n",
    "                threading.Thread(target=self.play_alert_sound, daemon=True).start()\n",
    "                person_data['last_alert_time'] = current_time\n",
    "        else:\n",
    "            color = (255, 0, 0) # Blue\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(frame, person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            ear_text = f\"EAR: {avg_ear:.2f} (Th: {person_data['threshold']:.2f})\"\n",
    "            count_text = f\"Count: {person_data['eye_counter']}/{EYE_AR_CONSEC_FRAMES}\"\n",
    "            cv2.putText(frame, ear_text, (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, count_text, (x, y + h + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "\n",
    "    def process_frame_logic(self, frame, rgb_frame):\n",
    "        \"\"\"Main logic loop for processing a frame.\"\"\"\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        faces = self.get_faces_from_results(frame, results)\n",
    "\n",
    "        if not self.registered_people:\n",
    "            # Not monitoring anyone, just draw all faces\n",
    "            for face_data in faces:\n",
    "                (x, y, w, h) = face_data['box']\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2)\n",
    "            return frame, \"Click 'Register & Monitor' to begin.\", \"\"\n",
    "\n",
    "        # --- Multi-Person Tracking Logic ---\n",
    "        unmatched_face_indices = set(range(len(faces)))\n",
    "        matches = {} # Stores {person_name: face_index}\n",
    "        \n",
    "        # Greedily assign best match for each person\n",
    "        for name, person_data in self.registered_people.items():\n",
    "            best_dist = float('inf')\n",
    "            best_face_idx = -1\n",
    "            for i in unmatched_face_indices: # Only check against unused faces\n",
    "                dist = np.linalg.norm(np.array(faces[i]['center']) - np.array(person_data['center']))\n",
    "                if dist < best_dist and dist < MAX_TRACKING_JUMP_PX:\n",
    "                    best_dist = dist\n",
    "                    best_face_idx = i\n",
    "            \n",
    "            if best_face_idx != -1:\n",
    "                matches[name] = best_face_idx\n",
    "                unmatched_face_indices.remove(best_face_idx) # This face is now taken\n",
    "\n",
    "        list_display_text = []\n",
    "        # Process all people (matched or lost)\n",
    "        for name, person_data in self.registered_people.items():\n",
    "            face_data = None\n",
    "            if name in matches:\n",
    "                face_data = faces[matches[name]] # Get the matched face data\n",
    "            \n",
    "            # This function updates the person's state and draws on the frame\n",
    "            self.process_person(frame, name, person_data, face_data)\n",
    "            \n",
    "            list_display_text.append(f\"{name}: {person_data['status']}\")\n",
    "\n",
    "        # Draw remaining unmatched (unregistered) faces\n",
    "        for i in unmatched_face_indices:\n",
    "            (x, y, w, h) = faces[i]['box']\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2)\n",
    "\n",
    "        status = f\"Monitoring {len(self.registered_people)} person(s).\"\n",
    "        return frame, status, \"\\n\".join(list_display_text)\n",
    "\n",
    "    def on_closing(self):\n",
    "        \"\"\"Handles the window close event.\"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except ImportError:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(\"Could not import one or more required libraries.\")\n",
    "        print(\"Please ensure you have the following installed:\")\n",
    "        print(\"pip install opencv-python pillow numpy mediapipe\")\n",
    "        print(\"---------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bbac617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video loop stopped.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp  # --- Import MediaPipe ---\n",
    "import winsound        # --- Using built-in winsound for Windows ---\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# --- Path to the alert sound ---\n",
    "# Make sure to use double backslashes (\\\\) on Windows!\n",
    "ALERT_SOUND_FILE_PATH = \"D:\\\\GIT_HUB\\\\12_Final_Projects_of_all\\\\03_deep_learning\\\\beep-01a.wav\"\n",
    "\n",
    "# --- Sound loop configuration ---\n",
    "# Time in seconds between playing the alert sound\n",
    "ALERT_SOUND_INTERVAL = 2.0 # Play sound every 2 seconds\n",
    "\n",
    "# --- Dynamic Calibration Constants ---\n",
    "THRESHOLD_CALIBRATION_FACTOR = 0.75\n",
    "MIN_OPEN_EYE_EAR = 0.20\n",
    "EYE_AR_CONSEC_FRAMES = 200 # ~10 seconds at 20fps\n",
    "\n",
    "# --- Tracking Constants ---\n",
    "LOST_GRACE_FRAMES = 50 # ~2.5 seconds\n",
    "MAX_TRACKING_JUMP_PX = 150 # Max distance a face can move between frames\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        \"\"\"\n",
    "        Initialize the application.\n",
    "        \"\"\"\n",
    "        self.window = window\n",
    "        self.window.title(\"Multi-Person Sleeping Alert System (v2.1 - Stable Calibration)\")\n",
    "        self.window.geometry(\"1000x700\") # Made window wider for sidebar\n",
    "        self.window.minsize(800, 600) # Set a minimum size\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- MediaPipe Face Mesh Initialization ---\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=10,  # Detect more faces\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # --- Multi-Person Logic & Registration State ---\n",
    "        self.registered_people = {} # Main dictionary to hold all registered people\n",
    "        self.registration_lock = threading.Lock()\n",
    "        self.current_frame_for_registration = None\n",
    "\n",
    "        # --- GUI REBUILD: Using .grid() for stable layout ---\n",
    "        \n",
    "        # Configure the main window's grid\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        self.main_frame.rowconfigure(0, weight=1) # Content (video/sidebar) expands\n",
    "        self.main_frame.rowconfigure(1, weight=0) # Bottom bar does not expand\n",
    "        self.main_frame.columnconfigure(0, weight=1) # Main column expands\n",
    "        \n",
    "        # --- 1. Content Frame (Video & Sidebar) ---\n",
    "        self.content_frame = tk.Frame(self.main_frame)\n",
    "        self.content_frame.grid(row=0, column=0, sticky=\"nsew\", padx=10, pady=10)\n",
    "        self.content_frame.rowconfigure(0, weight=1)\n",
    "        self.content_frame.columnconfigure(0, weight=1) # Video expands\n",
    "        self.content_frame.columnconfigure(1, weight=0) # Sidebar does not expand\n",
    "        \n",
    "        # Video Label\n",
    "        self.video_label = tk.Label(self.content_frame, bg=\"black\")\n",
    "        self.video_label.grid(row=0, column=0, sticky=\"nsew\")\n",
    "\n",
    "        # Sidebar\n",
    "        self.sidebar_frame = tk.Frame(self.content_frame, width=250, bg=\"#f0f0f0\", relief=tk.SUNKEN, borderwidth=2)\n",
    "        self.sidebar_frame.grid(row=0, column=1, sticky=\"ns\", padx=(10, 0))\n",
    "        self.sidebar_frame.pack_propagate(False)\n",
    "\n",
    "        self.sidebar_title = tk.Label(self.sidebar_frame, text=\"Registered People\", font=(\"Arial\", 16, \"bold\"), bg=\"#f0f0f0\")\n",
    "        self.sidebar_title.pack(pady=10, padx=10, anchor=\"w\")\n",
    "\n",
    "        self.registered_list_var = tk.StringVar()\n",
    "        self.registered_list_display = tk.Label(self.sidebar_frame, textvariable=self.registered_list_var, font=(\"Arial\", 12), bg=\"#f0f0f0\", justify=tk.LEFT, anchor=\"nw\")\n",
    "        self.registered_list_display.pack(pady=5, padx=10, fill=tk.X, anchor=\"nw\")\n",
    "        self.registered_list_var.set(\"None\")\n",
    "\n",
    "        # --- 2. Bottom Frame (Status & Buttons) ---\n",
    "        self.bottom_frame = tk.Frame(self.main_frame, bg=\"#e0e0e0\")\n",
    "        self.bottom_frame.grid(row=1, column=0, sticky=\"ew\", padx=10, pady=(0, 10))\n",
    "        self.bottom_frame.columnconfigure(0, weight=1) # Status label expands\n",
    "        self.bottom_frame.columnconfigure(1, weight=0) # Buttons do not expand\n",
    "\n",
    "        self.status_text = tk.StringVar()\n",
    "        self.status_text.set(\"Ready. Press 'Start Camera' to begin.\")\n",
    "        self.status_label = tk.Label(self.bottom_frame, textvariable=self.status_text, font=(\"Arial\", 14), bg=\"#e0e0e0\", anchor=\"w\")\n",
    "        self.status_label.grid(row=0, column=0, sticky=\"ew\", padx=10, pady=5)\n",
    "        \n",
    "        self.button_frame = tk.Frame(self.bottom_frame, bg=\"#e0e0e0\")\n",
    "        self.button_frame.grid(row=0, column=1, sticky=\"e\", padx=5, pady=5)\n",
    "\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, font=(\"Arial\", 12), width=15)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.register_person, font=(\"Arial\", 12), width=18)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear All\", command=self.clear_all_registrations, font=(\"Arial\", 12), width=18)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        if self.is_running:\n",
    "            return\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            if not self.cap.isOpened():\n",
    "                raise IOError(\"Cannot open webcam.\")\n",
    "            self.is_running = True\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            self.status_text.set(\"Camera running. Click 'Register & Monitor'.\")\n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            if self.cap: self.cap.release()\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        if not self.is_running:\n",
    "            return\n",
    "        self.is_running = False\n",
    "        if self.video_thread:\n",
    "            self.video_thread.join(timeout=0.5) \n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "        self.video_label.config(image=None)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        self.clear_all_registrations() # Clear people when camera stops\n",
    "\n",
    "    def video_loop(self):\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.status_text.set(\"Error: Can't read from camera.\")\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                with self.registration_lock:\n",
    "                    self.current_frame_for_registration = frame.copy()\n",
    "                \n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                processed_frame, status, list_text = self.process_frame_logic(frame, rgb_frame)\n",
    "                \n",
    "                # Update GUI from main thread\n",
    "                self.status_text.set(status)\n",
    "                self.registered_list_var.set(list_text if list_text else \"None\")\n",
    "\n",
    "                # Resize image\n",
    "                cv_img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                w, h = self.video_label.winfo_width(), self.video_label.winfo_height()\n",
    "                if w > 1 and h > 1:\n",
    "                     pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "                \n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "                \n",
    "                # Update image\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in video loop: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            time.sleep(0.01) # ~100fps theoretical max, but processing slows it\n",
    "        print(\"Video loop stopped.\")\n",
    "\n",
    "    def get_ear(self, eye_points, w, h):\n",
    "        \"\"\"Calculates Eye Aspect Ratio (EAR).\"\"\"\n",
    "        try:\n",
    "            p1 = (int(eye_points[0].x * w), int(eye_points[0].y * h))\n",
    "            p4 = (int(eye_points[1].x * w), int(eye_points[1].y * h))\n",
    "            p2 = (int(eye_points[2].x * w), int(eye_points[2].y * h))\n",
    "            p6 = (int(eye_points[3].x * w), int(eye_points[3].y * h))\n",
    "            p3 = (int(eye_points[4].x * w), int(eye_points[4].y * h))\n",
    "            p5 = (int(eye_points[5].x * w), int(eye_points[5].y * h))\n",
    "            def get_dist(p_a, p_b):\n",
    "                return np.linalg.norm(np.array(p_a) - np.array(p_b))\n",
    "            v_dist_1 = get_dist(p2, p6)\n",
    "            v_dist_2 = get_dist(p3, p5)\n",
    "            h_dist = get_dist(p1, p4)\n",
    "            if h_dist == 0: return 0.3\n",
    "            ear = (v_dist_1 + v_dist_2) / (2.0 * h_dist)\n",
    "            return ear\n",
    "        except Exception:\n",
    "            return 0.3\n",
    "\n",
    "    def get_faces_from_results(self, frame, results):\n",
    "        \"\"\"Extracts all face data (box, center, EAR) from MediaPipe results.\"\"\"\n",
    "        faces_data = []\n",
    "        h, w, _ = frame.shape\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    if x < x_min: x_min = x\n",
    "                    if y < y_min: y_min = y\n",
    "                    if x > x_max: x_max = x\n",
    "                    if y > y_max: y_max = y\n",
    "                padding = 10\n",
    "                x_min = max(0, x_min - padding)\n",
    "                y_min = max(0, y_min - padding)\n",
    "                x_max = min(w, x_max + padding)\n",
    "                y_max = min(h, y_max + padding)\n",
    "                box = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "                nose_tip = face_landmarks.landmark[1]\n",
    "                center = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "                RIGHT_EYE_EAR_POINTS_INDICES = [33, 133, 159, 145, 158, 153]\n",
    "                LEFT_EYE_EAR_POINTS_INDICES = [362, 263, 386, 374, 385, 380]\n",
    "                landmarks = face_landmarks.landmark\n",
    "                right_eye_points = [landmarks[i] for i in RIGHT_EYE_EAR_POINTS_INDICES]\n",
    "                left_eye_points = [landmarks[i] for i in LEFT_EYE_EAR_POINTS_INDICES]\n",
    "                right_ear = self.get_ear(right_eye_points, w, h)\n",
    "                left_ear = self.get_ear(left_eye_points, w, h)\n",
    "                avg_ear = (left_ear + right_ear) / 2.0\n",
    "                faces_data.append({'box': box, 'center': center, 'avg_ear': avg_ear})\n",
    "        return faces_data\n",
    "\n",
    "    def register_person(self):\n",
    "        \"\"\"Registers a new person based on the largest face in the frame.\"\"\"\n",
    "        with self.registration_lock:\n",
    "            if self.current_frame_for_registration is None:\n",
    "                messagebox.showwarning(\"Registration Error\", \"Camera not ready. Please try again.\")\n",
    "                return\n",
    "            rgb_frame_reg = cv2.cvtColor(self.current_frame_for_registration, cv2.COLOR_BGR2RGB)\n",
    "            results = self.face_mesh.process(rgb_frame_reg)\n",
    "            faces = self.get_faces_from_results(self.current_frame_for_registration, results)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            messagebox.showwarning(\"Registration Error\", \"No person detected. Please face the camera and try again.\")\n",
    "            return\n",
    "\n",
    "        faces_by_area = sorted(faces, key=lambda f: f['box'][2] * f['box'][3], reverse=True)\n",
    "        largest_face = faces_by_area[0]\n",
    "        current_ear = largest_face['avg_ear']\n",
    "        \n",
    "        if current_ear < MIN_OPEN_EYE_EAR:\n",
    "            messagebox.showwarning(\"Registration Error\", f\"Eyes seem to be closed (EAR: {current_ear:.2f}).\\nPlease open your eyes and try again.\")\n",
    "            return\n",
    "\n",
    "        calibrated_threshold = current_ear * THRESHOLD_CALIBRATION_FACTOR\n",
    "        name = simpledialog.askstring(\"Register Person\", \"Enter a unique name:\", parent=self.window)\n",
    "        \n",
    "        if not name:\n",
    "            messagebox.showwarning(\"Registration Cancelled\", \"Registration was cancelled.\")\n",
    "            return\n",
    "            \n",
    "        if name in self.registered_people:\n",
    "            messagebox.showwarning(\"Registration Error\", f\"The name '{name}' is already registered. Please use a unique name.\")\n",
    "            return\n",
    "        \n",
    "        # --- Add new person to the dictionary ---\n",
    "        self.registered_people[name] = {\n",
    "            'center': largest_face['center'],\n",
    "            'open_ear': current_ear,\n",
    "            'threshold': calibrated_threshold,\n",
    "            'eye_counter': 0,\n",
    "            'lost_counter': 0,\n",
    "            'status': 'Calibrated', # For the sidebar list\n",
    "            'last_alert_time': 0.0 # For sound loop\n",
    "        }\n",
    "\n",
    "        status = f\"Calibrated for {name}. Open EAR: {current_ear:.2f}, Threshold: {calibrated_threshold:.2f}\"\n",
    "        self.status_text.set(f\"Registered {name}. Monitoring {len(self.registered_people)} person(s).\")\n",
    "        messagebox.showinfo(\"Registration Complete\", status)\n",
    "\n",
    "    def clear_all_registrations(self):\n",
    "        \"\"\"Clears all registered people from monitoring.\"\"\"\n",
    "        if not self.registered_people:\n",
    "            return\n",
    "        self.registered_people = {}\n",
    "        self.status_text.set(\"All registrations cleared. Ready to register.\")\n",
    "        self.registered_list_var.set(\"None\")\n",
    "\n",
    "    def play_alert_sound(self):\n",
    "        \"\"\"Plays the alert sound asynchronously using winsound.\"\"\"\n",
    "        if not os.path.exists(ALERT_SOUND_FILE_PATH):\n",
    "            print(f\"Alert Sound Error: File not found at {ALERT_SOUND_FILE_PATH}\")\n",
    "            print(\"!!! ALERT (SOUND FILE MISSING) !!!\")\n",
    "            return\n",
    "        try:\n",
    "            winsound.PlaySound(ALERT_SOUND_FILE_PATH, winsound.SND_FILENAME | winsound.SND_ASYNC)\n",
    "        except Exception as e:\n",
    "            print(f\"Error playing sound file {ALERT_SOUND_FILE_PATH}: {e}\")\n",
    "            print(\"!!! ALERT BEEP !!!\")\n",
    "\n",
    "    def process_person(self, frame, person_name, person_data, face_data):\n",
    "        \"\"\"\n",
    "        Processes a single person (found or lost), updates state, and draws on frame.\n",
    "        \"\"\"\n",
    "        alert_triggered = False\n",
    "        \n",
    "        if face_data is None:\n",
    "            # --- Person is LOST ---\n",
    "            person_data['lost_counter'] += 1\n",
    "            if person_data['lost_counter'] > LOST_GRACE_FRAMES:\n",
    "                person_data['status'] = 'Lost'\n",
    "                person_data['eye_counter'] = 0\n",
    "            else:\n",
    "                person_data['status'] = 'Searching...'\n",
    "            return # Stop processing\n",
    "        \n",
    "        # --- Person is FOUND ---\n",
    "        person_data['lost_counter'] = 0\n",
    "        person_data['center'] = face_data['center']\n",
    "        avg_ear = face_data['avg_ear']\n",
    "        \n",
    "        # --- Check Eye Status ---\n",
    "        if avg_ear < person_data['threshold']:\n",
    "            person_data['eye_counter'] += 1\n",
    "        else:\n",
    "            person_data['eye_counter'] = 0\n",
    "            \n",
    "            # --- LOGIC FIX: REMOVED continuous re-calibration ---\n",
    "            # The calibration is now stable and set only at registration.\n",
    "            # This prevents the system from \"learning\" a sleepy state.\n",
    "        \n",
    "        # --- Check Alert Status ---\n",
    "        if person_data['eye_counter'] > EYE_AR_CONSEC_FRAMES:\n",
    "            person_data['status'] = '!!! ALERT !!!'\n",
    "            alert_triggered = True\n",
    "        else:\n",
    "            person_data['status'] = 'Tracking'\n",
    "        \n",
    "        # --- Draw on Frame ---\n",
    "        (x, y, w, h) = face_data['box']\n",
    "        \n",
    "        if alert_triggered:\n",
    "            color = (0, 0, 255) # Red\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 3)\n",
    "            cv2.putText(frame, f\"ALERT: {person_name} (EYES CLOSED)\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # --- LOOPING SOUND LOGIC ---\n",
    "            current_time = time.time()\n",
    "            if current_time - person_data['last_alert_time'] > ALERT_SOUND_INTERVAL:\n",
    "                threading.Thread(target=self.play_alert_sound, daemon=True).start()\n",
    "                person_data['last_alert_time'] = current_time\n",
    "        else:\n",
    "            color = (255, 0, 0) # Blue\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(frame, person_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            ear_text = f\"EAR: {avg_ear:.2f} (Th: {person_data['threshold']:.2f})\"\n",
    "            count_text = f\"Count: {person_data['eye_counter']}/{EYE_AR_CONSEC_FRAMES}\"\n",
    "            cv2.putText(frame, ear_text, (x, y + h + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, count_text, (x, y + h + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "\n",
    "    def process_frame_logic(self, frame, rgb_frame):\n",
    "        \"\"\"Main logic loop for processing a frame.\"\"\"\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        faces = self.get_faces_from_results(frame, results)\n",
    "\n",
    "        if not self.registered_people:\n",
    "            # Not monitoring anyone, just draw all faces\n",
    "            for face_data in faces:\n",
    "                (x, y, w, h) = face_data['box']\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2)\n",
    "            return frame, \"Click 'Register & Monitor' to begin.\", \"\"\n",
    "\n",
    "        # --- Multi-Person Tracking Logic ---\n",
    "        unmatched_face_indices = set(range(len(faces)))\n",
    "        matches = {} # Stores {person_name: face_index}\n",
    "        \n",
    "        # Greedily assign best match for each person\n",
    "        for name, person_data in self.registered_people.items():\n",
    "            best_dist = float('inf')\n",
    "            best_face_idx = -1\n",
    "            for i in unmatched_face_indices: # Only check against unused faces\n",
    "                dist = np.linalg.norm(np.array(faces[i]['center']) - np.array(person_data['center']))\n",
    "                if dist < best_dist and dist < MAX_TRACKING_JUMP_PX:\n",
    "                    best_dist = dist\n",
    "                    best_face_idx = i\n",
    "            \n",
    "            if best_face_idx != -1:\n",
    "                matches[name] = best_face_idx\n",
    "                unmatched_face_indices.remove(best_face_idx) # This face is now taken\n",
    "\n",
    "        list_display_text = []\n",
    "        # Process all people (matched or lost)\n",
    "        for name, person_data in self.registered_people.items():\n",
    "            face_data = None\n",
    "            if name in matches:\n",
    "                face_data = faces[matches[name]] # Get the matched face data\n",
    "            \n",
    "            # This function updates the person's state and draws on the frame\n",
    "            self.process_person(frame, name, person_data, face_data)\n",
    "            \n",
    "            list_display_text.append(f\"{name}: {person_data['status']}\")\n",
    "\n",
    "        # Draw remaining unmatched (unregistered) faces\n",
    "        for i in unmatched_face_indices:\n",
    "            (x, y, w, h) = faces[i]['box']\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 192, 0), 2)\n",
    "\n",
    "        status = f\"Monitoring {len(self.registered_people)} person(s).\"\n",
    "        return frame, status, \"\\n\".join(list_display_text)\n",
    "\n",
    "    def on_closing(self):\n",
    "        \"\"\"Handles the window close event.\"\"\"\n",
    "        if messagebox.askokcancel(\"Quit\", \"Do you want to exit the application?\"):\n",
    "            self.stop_video_stream()\n",
    "            self.window.destroy()\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except ImportError:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(\"Could not import one or more required libraries.\")\n",
    "        print(\"Please ensure you have the following installed:\")\n",
    "        print(\"pip install opencv-python pillow numpy mediapipe\")\n",
    "        print(\"---------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23e0a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Error: window \".!_querystring\" was deleted before its visibility changed\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import logging  # --- NEW: For Logging ---\n",
    "import mediapipe as mp\n",
    "import winsound\n",
    "\n",
    "# --- Logging Setup ---\n",
    "# This creates a file named 'sleeping_system_log.txt' and appends events to it.\n",
    "logging.basicConfig(\n",
    "    filename='sleeping_system_log.txt',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "ALERT_SOUND_FILE_PATH = \"D:\\\\GIT_HUB\\\\12_Final_Projects_of_all\\\\03_deep_learning\\\\beep-01a.wav\"\n",
    "ALERT_SOUND_INTERVAL = 2.0\n",
    "\n",
    "# --- Improved Logic Constants ---\n",
    "# Lowered factor to 0.65 to prevent false positives when eyes are just \"relaxed\"\n",
    "THRESHOLD_CALIBRATION_FACTOR = 0.65 \n",
    "# Hard limit: Threshold will never be set higher than this, even if eyes are huge.\n",
    "MAX_POSSIBLE_THRESHOLD = 0.28 \n",
    "MIN_OPEN_EYE_EAR = 0.18\n",
    "EYE_AR_CONSEC_FRAMES = 150 # Slightly faster reaction (~7-8 seconds)\n",
    "\n",
    "# --- Calibration Constants ---\n",
    "CALIBRATION_FRAMES_REQUIRED = 40 # Number of frames to average for robust calibration\n",
    "\n",
    "# --- Tracking Constants ---\n",
    "LOST_GRACE_FRAMES = 50\n",
    "MAX_TRACKING_JUMP_PX = 150\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        self.window = window\n",
    "        self.window.title(\"Sleeping Alert System v3.0 (Logs + Burst Calibration)\")\n",
    "        self.window.geometry(\"1100x750\")\n",
    "        self.window.minsize(900, 650)\n",
    "\n",
    "        logging.info(\"Application started.\")\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- MediaPipe Setup ---\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=5,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # --- Multi-Person & Logic State ---\n",
    "        self.registered_people = {}\n",
    "        self.registration_lock = threading.Lock()\n",
    "        self.current_frame_for_registration = None\n",
    "        \n",
    "        # --- NEW: Calibration State ---\n",
    "        self.is_calibrating = False\n",
    "        self.calibration_buffer = [] # Stores EAR values during calibration\n",
    "        self.calibration_face_center = None # Tracks who we are calibrating\n",
    "\n",
    "        # --- GUI Setup ---\n",
    "        self._setup_gui()\n",
    "\n",
    "    def _setup_gui(self):\n",
    "        \"\"\"Helper to organize GUI code.\"\"\"\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        self.main_frame.rowconfigure(0, weight=1)\n",
    "        self.main_frame.rowconfigure(1, weight=0)\n",
    "        self.main_frame.columnconfigure(0, weight=1)\n",
    "        \n",
    "        # 1. Content Area\n",
    "        self.content_frame = tk.Frame(self.main_frame)\n",
    "        self.content_frame.grid(row=0, column=0, sticky=\"nsew\", padx=10, pady=10)\n",
    "        self.content_frame.rowconfigure(0, weight=1)\n",
    "        self.content_frame.columnconfigure(0, weight=1)\n",
    "        self.content_frame.columnconfigure(1, weight=0)\n",
    "        \n",
    "        self.video_label = tk.Label(self.content_frame, bg=\"black\")\n",
    "        self.video_label.grid(row=0, column=0, sticky=\"nsew\")\n",
    "\n",
    "        # Sidebar\n",
    "        self.sidebar_frame = tk.Frame(self.content_frame, width=280, bg=\"#f0f0f0\", relief=tk.SUNKEN, borderwidth=2)\n",
    "        self.sidebar_frame.grid(row=0, column=1, sticky=\"ns\", padx=(10, 0))\n",
    "        self.sidebar_frame.pack_propagate(False)\n",
    "\n",
    "        tk.Label(self.sidebar_frame, text=\"Registered People\", font=(\"Arial\", 14, \"bold\"), bg=\"#f0f0f0\").pack(pady=10, anchor=\"w\", padx=10)\n",
    "        \n",
    "        self.registered_list_var = tk.StringVar(value=\"No one registered.\")\n",
    "        self.registered_list_display = tk.Label(self.sidebar_frame, textvariable=self.registered_list_var, font=(\"Arial\", 11), bg=\"#f0f0f0\", justify=tk.LEFT, anchor=\"nw\")\n",
    "        self.registered_list_display.pack(pady=5, padx=10, fill=tk.X, anchor=\"nw\")\n",
    "\n",
    "        # 2. Bottom Control Bar\n",
    "        self.bottom_frame = tk.Frame(self.main_frame, bg=\"#e0e0e0\")\n",
    "        self.bottom_frame.grid(row=1, column=0, sticky=\"ew\", padx=10, pady=(0, 10))\n",
    "        self.bottom_frame.columnconfigure(0, weight=1)\n",
    "\n",
    "        self.status_text = tk.StringVar(value=\"Ready. Press 'Start Camera'.\")\n",
    "        self.status_label = tk.Label(self.bottom_frame, textvariable=self.status_text, font=(\"Arial\", 12, \"bold\"), bg=\"#e0e0e0\", anchor=\"w\")\n",
    "        self.status_label.grid(row=0, column=0, sticky=\"ew\", padx=10, pady=10)\n",
    "        \n",
    "        self.button_frame = tk.Frame(self.bottom_frame, bg=\"#e0e0e0\")\n",
    "        self.button_frame.grid(row=0, column=1, sticky=\"e\", padx=5)\n",
    "\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, width=15, bg=\"#dddddd\")\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, width=15, bg=\"#dddddd\", state=tk.DISABLED)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.initiate_calibration, width=20, bg=\"#aaffaa\", state=tk.DISABLED)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear All\", command=self.clear_all_registrations, width=15, bg=\"#ffaaaa\", state=tk.DISABLED)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        if self.is_running: return\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            if not self.cap.isOpened(): raise IOError(\"Cannot open webcam.\")\n",
    "            \n",
    "            self.is_running = True\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            \n",
    "            self.start_button.config(state=tk.DISABLED)\n",
    "            self.stop_button.config(state=tk.NORMAL)\n",
    "            self.register_button.config(state=tk.NORMAL)\n",
    "            self.clear_button.config(state=tk.NORMAL)\n",
    "            self.status_text.set(\"Camera running. Align face and click 'Register'.\")\n",
    "            logging.info(\"Camera started.\")\n",
    "            \n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            logging.error(f\"Webcam failed to start: {e}\")\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        if not self.is_running: return\n",
    "        self.is_running = False\n",
    "        if self.video_thread: self.video_thread.join(timeout=0.5) \n",
    "        if self.cap: self.cap.release()\n",
    "        \n",
    "        self.video_label.config(image=None)\n",
    "        self.start_button.config(state=tk.NORMAL)\n",
    "        self.stop_button.config(state=tk.DISABLED)\n",
    "        self.register_button.config(state=tk.DISABLED)\n",
    "        self.clear_button.config(state=tk.DISABLED)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        \n",
    "        self.clear_all_registrations()\n",
    "        logging.info(\"Camera stopped.\")\n",
    "\n",
    "    def video_loop(self):\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    time.sleep(0.1)\n",
    "                    continue\n",
    "\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Handle Calibration Logic inside the loop for accuracy\n",
    "                if self.is_calibrating:\n",
    "                    frame = self._handle_calibration_step(frame)\n",
    "                else:\n",
    "                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frame, status_msg, list_msg = self.process_monitoring_logic(frame, rgb_frame)\n",
    "                    \n",
    "                    # Update UI safely\n",
    "                    self.status_text.set(status_msg)\n",
    "                    self.registered_list_var.set(list_msg)\n",
    "\n",
    "                # Display Frame\n",
    "                cv_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                \n",
    "                # Responsive resize\n",
    "                w = self.video_label.winfo_width()\n",
    "                h = self.video_label.winfo_height()\n",
    "                if w > 10 and h > 10:\n",
    "                    pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "                \n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Loop Error: {e}\")\n",
    "                logging.error(f\"Video loop error: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            time.sleep(0.01)\n",
    "\n",
    "    # --- Core Logic Methods ---\n",
    "\n",
    "    def get_ear(self, eye_points, w, h):\n",
    "        \"\"\"Calculates Eye Aspect Ratio.\"\"\"\n",
    "        try:\n",
    "            # Standard Euclidean Distance\n",
    "            def dist(p1, p2):\n",
    "                return np.linalg.norm(np.array([p1.x*w, p1.y*h]) - np.array([p2.x*w, p2.y*h]))\n",
    "\n",
    "            # Vertical distances\n",
    "            v1 = dist(eye_points[1], eye_points[5])\n",
    "            v2 = dist(eye_points[2], eye_points[4])\n",
    "            # Horizontal distance\n",
    "            h_dist = dist(eye_points[0], eye_points[3])\n",
    "\n",
    "            if h_dist == 0: return 0.0\n",
    "            return (v1 + v2) / (2.0 * h_dist)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def get_face_data(self, frame, results):\n",
    "        \"\"\"Parses MediaPipe results into usable data (box, EAR, center).\"\"\"\n",
    "        faces = []\n",
    "        h, w, _ = frame.shape\n",
    "        if not results.multi_face_landmarks: return faces\n",
    "\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            # Bounding Box\n",
    "            xs = [l.x for l in landmarks.landmark]\n",
    "            ys = [l.y for l in landmarks.landmark]\n",
    "            x_min, x_max = int(min(xs)*w), int(max(xs)*w)\n",
    "            y_min, y_max = int(min(ys)*h), int(max(ys)*h)\n",
    "            \n",
    "            # Expand box slightly\n",
    "            x_min, y_min = max(0, x_min-10), max(0, y_min-10)\n",
    "            x_max, y_max = min(w, x_max+10), min(h, y_max+10)\n",
    "            \n",
    "            # Nose Tip (Landmark 1)\n",
    "            nose = landmarks.landmark[1]\n",
    "            center = (int(nose.x*w), int(nose.y*h))\n",
    "\n",
    "            # EAR Calculation\n",
    "            # Indices: Left [33, 160, 158, 133, 153, 144], Right [362, 385, 387, 263, 373, 380]\n",
    "            # Using the indices from previous iteration that were verified:\n",
    "            LEFT_IDXS = [33, 159, 158, 133, 153, 145] # P1, P2, P3, P4, P5, P6\n",
    "            RIGHT_IDXS = [362, 386, 385, 263, 380, 374]\n",
    "\n",
    "            left_pts = [landmarks.landmark[i] for i in LEFT_IDXS]\n",
    "            right_pts = [landmarks.landmark[i] for i in RIGHT_IDXS]\n",
    "\n",
    "            ear_left = self.get_ear(left_pts, w, h)\n",
    "            ear_right = self.get_ear(right_pts, w, h)\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            faces.append({\n",
    "                'box': (x_min, y_min, x_max-x_min, y_max-y_min),\n",
    "                'center': center,\n",
    "                'ear': avg_ear\n",
    "            })\n",
    "        return faces\n",
    "\n",
    "    # --- NEW: Robust Calibration Workflow ---\n",
    "\n",
    "    def initiate_calibration(self):\n",
    "        \"\"\"Starts the burst calibration process.\"\"\"\n",
    "        if self.is_calibrating: return\n",
    "        self.is_calibrating = True\n",
    "        self.calibration_buffer = []\n",
    "        self.status_text.set(\"CALIBRATING... KEEP EYES OPEN AND STILL!\")\n",
    "        # Disable buttons\n",
    "        self.register_button.config(state=tk.DISABLED)\n",
    "        logging.info(\"Calibration initiated.\")\n",
    "\n",
    "    def _handle_calibration_step(self, frame):\n",
    "        \"\"\"Accumulates frames to calculate average EAR.\"\"\"\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        faces = self.get_face_data(frame, results)\n",
    "        \n",
    "        h, w, _ = frame.shape\n",
    "        \n",
    "        if not faces:\n",
    "            cv2.putText(frame, \"NO FACE DETECTED\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            return frame\n",
    "\n",
    "        # Find largest face\n",
    "        target = sorted(faces, key=lambda f: f['box'][2]*f['box'][3], reverse=True)[0]\n",
    "        \n",
    "        # Draw Loading Bar\n",
    "        progress = len(self.calibration_buffer) / CALIBRATION_FRAMES_REQUIRED\n",
    "        bar_w = int(w * 0.6)\n",
    "        cv2.rectangle(frame, (int(w*0.2), h-100), (int(w*0.2) + bar_w, h-70), (255, 255, 255), 2)\n",
    "        cv2.rectangle(frame, (int(w*0.2), h-100), (int(w*0.2) + int(bar_w*progress), h-70), (0, 255, 0), -1)\n",
    "        cv2.putText(frame, \"CALIBRATING...\", (int(w*0.2), h-110), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        self.calibration_buffer.append(target['ear'])\n",
    "\n",
    "        if len(self.calibration_buffer) >= CALIBRATION_FRAMES_REQUIRED:\n",
    "            self._finalize_calibration(target['center'])\n",
    "            self.is_calibrating = False\n",
    "            self.register_button.config(state=tk.NORMAL)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def _finalize_calibration(self, face_center):\n",
    "        \"\"\"Calculates the final threshold from the buffer.\"\"\"\n",
    "        avg_ear = sum(self.calibration_buffer) / len(self.calibration_buffer)\n",
    "        \n",
    "        # Logic Check\n",
    "        if avg_ear < MIN_OPEN_EYE_EAR:\n",
    "            messagebox.showwarning(\"Failed\", f\"Eyes detected as closed (EAR: {avg_ear:.2f}). Retry.\")\n",
    "            logging.warning(f\"Calibration failed: EAR too low ({avg_ear:.2f})\")\n",
    "            return\n",
    "\n",
    "        # Calculate robust threshold\n",
    "        threshold = min(avg_ear * THRESHOLD_CALIBRATION_FACTOR, MAX_POSSIBLE_THRESHOLD)\n",
    "        \n",
    "        # Get Name (must be done in main thread, but simpledialog works here usually)\n",
    "        name = simpledialog.askstring(\"Registration\", f\"Calibration Done (EAR: {avg_ear:.2f}).\\nName:\")\n",
    "        \n",
    "        if name and name not in self.registered_people:\n",
    "            self.registered_people[name] = {\n",
    "                'center': face_center,\n",
    "                'threshold': threshold,\n",
    "                'open_ear_baseline': avg_ear,\n",
    "                'closed_frames': 0,\n",
    "                'status': 'Active',\n",
    "                'last_sound_time': 0\n",
    "            }\n",
    "            logging.info(f\"Person registered: {name} with threshold {threshold:.3f}\")\n",
    "        else:\n",
    "            logging.warning(\"Registration cancelled or duplicate name.\")\n",
    "\n",
    "    # --- Monitoring Logic ---\n",
    "\n",
    "    def process_monitoring_logic(self, frame, rgb_frame):\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        faces = self.get_face_data(frame, results)\n",
    "        \n",
    "        if not self.registered_people:\n",
    "            # Just draw boxes if no one registered\n",
    "            for f in faces:\n",
    "                x, y, w, h = f['box']\n",
    "                cv2.rectangle(frame, (x,y), (x+w, y+h), (100, 100, 100), 2)\n",
    "            return frame, \"Ready to Register.\", \"No one registered.\"\n",
    "\n",
    "        # Matching Logic (Simple Nearest Neighbor)\n",
    "        active_faces = {i: f for i, f in enumerate(faces)}\n",
    "        matched_names = []\n",
    "        list_display = []\n",
    "\n",
    "        for name, p_data in self.registered_people.items():\n",
    "            best_dist = float('inf')\n",
    "            best_idx = -1\n",
    "            \n",
    "            # Find closest face\n",
    "            for idx, face in active_faces.items():\n",
    "                dist = np.linalg.norm(np.array(face['center']) - np.array(p_data['center']))\n",
    "                if dist < best_dist:\n",
    "                    best_dist = dist\n",
    "                    best_idx = idx\n",
    "            \n",
    "            # Check if match is valid (tracking continuity)\n",
    "            if best_idx != -1 and best_dist < MAX_TRACKING_JUMP_PX:\n",
    "                # Found the person\n",
    "                face = active_faces[best_idx]\n",
    "                del active_faces[best_idx] # Remove from pool\n",
    "                \n",
    "                # Update position for next frame\n",
    "                p_data['center'] = face['center']\n",
    "                \n",
    "                # --- SLEEP DETECTION LOGIC ---\n",
    "                ear = face['ear']\n",
    "                is_eyes_closed = ear < p_data['threshold']\n",
    "                \n",
    "                if is_eyes_closed:\n",
    "                    p_data['closed_frames'] += 1\n",
    "                else:\n",
    "                    p_data['closed_frames'] = 0 # Reset immediately on open eyes\n",
    "                \n",
    "                # Trigger Alert\n",
    "                if p_data['closed_frames'] > EYE_AR_CONSEC_FRAMES:\n",
    "                    p_data['status'] = \"!!! SLEEPING !!!\"\n",
    "                    self._trigger_alert(name, p_data)\n",
    "                    color = (0, 0, 255)\n",
    "                    cv2.putText(frame, \"WAKE UP!\", (face['box'][0], face['box'][1]-20), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n",
    "                else:\n",
    "                    p_data['status'] = \"Active\"\n",
    "                    color = (0, 255, 0)\n",
    "\n",
    "                # Draw Info\n",
    "                x, y, w, h = face['box']\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                cv2.putText(frame, f\"{name} (EAR: {ear:.2f})\", (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                \n",
    "            else:\n",
    "                # Person lost\n",
    "                p_data['status'] = \"Lost\"\n",
    "                p_data['closed_frames'] = 0\n",
    "\n",
    "            list_display.append(f\"{name}: {p_data['status']}\")\n",
    "\n",
    "        status_msg = f\"Monitoring {len(self.registered_people)} people.\"\n",
    "        return frame, status_msg, \"\\n\".join(list_display)\n",
    "\n",
    "    def _trigger_alert(self, name, p_data):\n",
    "        \"\"\"Plays sound loop and logs alert.\"\"\"\n",
    "        now = time.time()\n",
    "        if now - p_data['last_sound_time'] > ALERT_SOUND_INTERVAL:\n",
    "            logging.warning(f\"Alert triggered for {name}!\")\n",
    "            threading.Thread(target=self._play_sound, daemon=True).start()\n",
    "            p_data['last_sound_time'] = now\n",
    "\n",
    "    def _play_sound(self):\n",
    "        if os.path.exists(ALERT_SOUND_FILE_PATH):\n",
    "            try:\n",
    "                winsound.PlaySound(ALERT_SOUND_FILE_PATH, winsound.SND_FILENAME | winsound.SND_ASYNC)\n",
    "            except:\n",
    "                pass # Fail silently in thread\n",
    "\n",
    "    def clear_all_registrations(self):\n",
    "        self.registered_people = {}\n",
    "        logging.info(\"All registrations cleared.\")\n",
    "        self.status_text.set(\"Registrations Cleared.\")\n",
    "\n",
    "    def on_closing(self):\n",
    "        if messagebox.askokcancel(\"Quit\", \"Exit Application?\"):\n",
    "            self.stop_video_stream()\n",
    "            logging.info(\"Application closed.\")\n",
    "            self.window.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Critical crash: {e}\")\n",
    "        print(f\"Crash: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95cbda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Error: dictionary changed size during iteration\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox, simpledialog\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import mediapipe as mp\n",
    "import winsound\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(\n",
    "    filename='sleeping_system_log.txt',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "ALERT_SOUND_FILE_PATH = \"D:\\\\GIT_HUB\\\\12_Final_Projects_of_all\\\\03_deep_learning\\\\beep-01a.wav\"\n",
    "ALERT_SOUND_INTERVAL = 2.0\n",
    "\n",
    "# --- Improved Logic Constants ---\n",
    "THRESHOLD_CALIBRATION_FACTOR = 0.65 \n",
    "MAX_POSSIBLE_THRESHOLD = 0.28 \n",
    "MIN_OPEN_EYE_EAR = 0.18\n",
    "EYE_AR_CONSEC_FRAMES = 150 \n",
    "\n",
    "# --- Calibration Constants ---\n",
    "CALIBRATION_FRAMES_REQUIRED = 40 \n",
    "\n",
    "# --- Tracking Constants ---\n",
    "LOST_GRACE_FRAMES = 50\n",
    "MAX_TRACKING_JUMP_PX = 150\n",
    "\n",
    "class SleepingAlertApp:\n",
    "    def __init__(self, window):\n",
    "        self.window = window\n",
    "        self.window.title(\"Sleeping Alert System v3.1 (Thread Safe)\")\n",
    "        self.window.geometry(\"1100x750\")\n",
    "        self.window.minsize(900, 650)\n",
    "\n",
    "        logging.info(\"Application started.\")\n",
    "\n",
    "        # --- State Variables ---\n",
    "        self.cap = None\n",
    "        self.video_thread = None\n",
    "        self.is_running = False\n",
    "\n",
    "        # --- MediaPipe Setup ---\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=5,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # --- Multi-Person & Logic State ---\n",
    "        self.registered_people = {}\n",
    "        self.registration_lock = threading.Lock()\n",
    "        self.current_frame_for_registration = None\n",
    "        \n",
    "        # --- NEW: Calibration State ---\n",
    "        self.is_calibrating = False\n",
    "        self.calibration_buffer = [] \n",
    "        \n",
    "        # --- GUI Setup ---\n",
    "        self._setup_gui()\n",
    "\n",
    "    def _setup_gui(self):\n",
    "        \"\"\"Helper to organize GUI code.\"\"\"\n",
    "        self.main_frame = tk.Frame(self.window)\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        self.main_frame.rowconfigure(0, weight=1)\n",
    "        self.main_frame.rowconfigure(1, weight=0)\n",
    "        self.main_frame.columnconfigure(0, weight=1)\n",
    "        \n",
    "        # 1. Content Area\n",
    "        self.content_frame = tk.Frame(self.main_frame)\n",
    "        self.content_frame.grid(row=0, column=0, sticky=\"nsew\", padx=10, pady=10)\n",
    "        self.content_frame.rowconfigure(0, weight=1)\n",
    "        self.content_frame.columnconfigure(0, weight=1)\n",
    "        self.content_frame.columnconfigure(1, weight=0)\n",
    "        \n",
    "        self.video_label = tk.Label(self.content_frame, bg=\"black\")\n",
    "        self.video_label.grid(row=0, column=0, sticky=\"nsew\")\n",
    "\n",
    "        # Sidebar\n",
    "        self.sidebar_frame = tk.Frame(self.content_frame, width=280, bg=\"#f0f0f0\", relief=tk.SUNKEN, borderwidth=2)\n",
    "        self.sidebar_frame.grid(row=0, column=1, sticky=\"ns\", padx=(10, 0))\n",
    "        self.sidebar_frame.pack_propagate(False)\n",
    "\n",
    "        tk.Label(self.sidebar_frame, text=\"Registered People\", font=(\"Arial\", 14, \"bold\"), bg=\"#f0f0f0\").pack(pady=10, anchor=\"w\", padx=10)\n",
    "        \n",
    "        self.registered_list_var = tk.StringVar(value=\"No one registered.\")\n",
    "        self.registered_list_display = tk.Label(self.sidebar_frame, textvariable=self.registered_list_var, font=(\"Arial\", 11), bg=\"#f0f0f0\", justify=tk.LEFT, anchor=\"nw\")\n",
    "        self.registered_list_display.pack(pady=5, padx=10, fill=tk.X, anchor=\"nw\")\n",
    "\n",
    "        # 2. Bottom Control Bar\n",
    "        self.bottom_frame = tk.Frame(self.main_frame, bg=\"#e0e0e0\")\n",
    "        self.bottom_frame.grid(row=1, column=0, sticky=\"ew\", padx=10, pady=(0, 10))\n",
    "        self.bottom_frame.columnconfigure(0, weight=1)\n",
    "\n",
    "        self.status_text = tk.StringVar(value=\"Ready. Press 'Start Camera'.\")\n",
    "        self.status_label = tk.Label(self.bottom_frame, textvariable=self.status_text, font=(\"Arial\", 12, \"bold\"), bg=\"#e0e0e0\", anchor=\"w\")\n",
    "        self.status_label.grid(row=0, column=0, sticky=\"ew\", padx=10, pady=10)\n",
    "        \n",
    "        self.button_frame = tk.Frame(self.bottom_frame, bg=\"#e0e0e0\")\n",
    "        self.button_frame.grid(row=0, column=1, sticky=\"e\", padx=5)\n",
    "\n",
    "        self.start_button = tk.Button(self.button_frame, text=\"Start Camera\", command=self.start_video_stream, width=15, bg=\"#dddddd\")\n",
    "        self.start_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.stop_button = tk.Button(self.button_frame, text=\"Stop Camera\", command=self.stop_video_stream, width=15, bg=\"#dddddd\", state=tk.DISABLED)\n",
    "        self.stop_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.register_button = tk.Button(self.button_frame, text=\"Register & Monitor\", command=self.initiate_calibration, width=20, bg=\"#aaffaa\", state=tk.DISABLED)\n",
    "        self.register_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.clear_button = tk.Button(self.button_frame, text=\"Clear All\", command=self.clear_all_registrations, width=15, bg=\"#ffaaaa\", state=tk.DISABLED)\n",
    "        self.clear_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.window.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "\n",
    "    def start_video_stream(self):\n",
    "        if self.is_running: return\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            if not self.cap.isOpened(): raise IOError(\"Cannot open webcam.\")\n",
    "            \n",
    "            self.is_running = True\n",
    "            self.video_thread = threading.Thread(target=self.video_loop, daemon=True)\n",
    "            self.video_thread.start()\n",
    "            \n",
    "            self.start_button.config(state=tk.DISABLED)\n",
    "            self.stop_button.config(state=tk.NORMAL)\n",
    "            self.register_button.config(state=tk.NORMAL)\n",
    "            self.clear_button.config(state=tk.NORMAL)\n",
    "            self.status_text.set(\"Camera running. Align face and click 'Register'.\")\n",
    "            logging.info(\"Camera started.\")\n",
    "            \n",
    "        except IOError as e:\n",
    "            messagebox.showerror(\"Webcam Error\", str(e))\n",
    "            logging.error(f\"Webcam failed to start: {e}\")\n",
    "\n",
    "    def stop_video_stream(self):\n",
    "        if not self.is_running: return\n",
    "        self.is_running = False\n",
    "        if self.video_thread: self.video_thread.join(timeout=0.5) \n",
    "        if self.cap: self.cap.release()\n",
    "        \n",
    "        self.video_label.config(image=None)\n",
    "        self.start_button.config(state=tk.NORMAL)\n",
    "        self.stop_button.config(state=tk.DISABLED)\n",
    "        self.register_button.config(state=tk.DISABLED)\n",
    "        self.clear_button.config(state=tk.DISABLED)\n",
    "        self.status_text.set(\"Camera stopped.\")\n",
    "        \n",
    "        self.clear_all_registrations()\n",
    "        logging.info(\"Camera stopped.\")\n",
    "\n",
    "    def video_loop(self):\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    time.sleep(0.1)\n",
    "                    continue\n",
    "\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Handle Calibration Logic inside the loop for accuracy\n",
    "                if self.is_calibrating:\n",
    "                    frame = self._handle_calibration_step_in_thread(frame)\n",
    "                else:\n",
    "                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frame, status_msg, list_msg = self.process_monitoring_logic(frame, rgb_frame)\n",
    "                    \n",
    "                    # Update UI\n",
    "                    self.status_text.set(status_msg)\n",
    "                    self.registered_list_var.set(list_msg)\n",
    "\n",
    "                # Display Frame\n",
    "                cv_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(cv_img)\n",
    "                \n",
    "                w = self.video_label.winfo_width()\n",
    "                h = self.video_label.winfo_height()\n",
    "                if w > 10 and h > 10:\n",
    "                    pil_img = pil_img.resize((w, h), Image.Resampling.LANCZOS)\n",
    "                \n",
    "                imgtk = ImageTk.PhotoImage(image=pil_img)\n",
    "                self.video_label.imgtk = imgtk\n",
    "                self.video_label.configure(image=imgtk)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Loop Error: {e}\")\n",
    "                logging.error(f\"Video loop error: {e}\")\n",
    "                self.is_running = False\n",
    "            \n",
    "            time.sleep(0.01)\n",
    "\n",
    "    # --- Core Logic Methods ---\n",
    "\n",
    "    def get_ear(self, eye_points, w, h):\n",
    "        \"\"\"Calculates Eye Aspect Ratio.\"\"\"\n",
    "        try:\n",
    "            def dist(p1, p2):\n",
    "                return np.linalg.norm(np.array([p1.x*w, p1.y*h]) - np.array([p2.x*w, p2.y*h]))\n",
    "            v1 = dist(eye_points[1], eye_points[5])\n",
    "            v2 = dist(eye_points[2], eye_points[4])\n",
    "            h_dist = dist(eye_points[0], eye_points[3])\n",
    "            if h_dist == 0: return 0.0\n",
    "            return (v1 + v2) / (2.0 * h_dist)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def get_face_data(self, frame, results):\n",
    "        \"\"\"Parses MediaPipe results into usable data (box, EAR, center).\"\"\"\n",
    "        faces = []\n",
    "        h, w, _ = frame.shape\n",
    "        if not results.multi_face_landmarks: return faces\n",
    "\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            xs = [l.x for l in landmarks.landmark]\n",
    "            ys = [l.y for l in landmarks.landmark]\n",
    "            x_min, x_max = int(min(xs)*w), int(max(xs)*w)\n",
    "            y_min, y_max = int(min(ys)*h), int(max(ys)*h)\n",
    "            \n",
    "            x_min, y_min = max(0, x_min-10), max(0, y_min-10)\n",
    "            x_max, y_max = min(w, x_max+10), min(h, y_max+10)\n",
    "            \n",
    "            nose = landmarks.landmark[1]\n",
    "            center = (int(nose.x*w), int(nose.y*h))\n",
    "\n",
    "            LEFT_IDXS = [33, 159, 158, 133, 153, 145] \n",
    "            RIGHT_IDXS = [362, 386, 385, 263, 380, 374]\n",
    "\n",
    "            left_pts = [landmarks.landmark[i] for i in LEFT_IDXS]\n",
    "            right_pts = [landmarks.landmark[i] for i in RIGHT_IDXS]\n",
    "\n",
    "            ear_left = self.get_ear(left_pts, w, h)\n",
    "            ear_right = self.get_ear(right_pts, w, h)\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            faces.append({\n",
    "                'box': (x_min, y_min, x_max-x_min, y_max-y_min),\n",
    "                'center': center,\n",
    "                'ear': avg_ear\n",
    "            })\n",
    "        return faces\n",
    "\n",
    "    # --- NEW: Thread-Safe Calibration Workflow ---\n",
    "\n",
    "    def initiate_calibration(self):\n",
    "        \"\"\"Starts the burst calibration process.\"\"\"\n",
    "        if self.is_calibrating: return\n",
    "        self.is_calibrating = True\n",
    "        self.calibration_buffer = []\n",
    "        self.status_text.set(\"CALIBRATING... KEEP EYES OPEN AND STILL!\")\n",
    "        # Disable buttons\n",
    "        self.register_button.config(state=tk.DISABLED)\n",
    "        logging.info(\"Calibration initiated.\")\n",
    "\n",
    "    def _handle_calibration_step_in_thread(self, frame):\n",
    "        \"\"\"\n",
    "        Runs in Video Thread. Collects data.\n",
    "        When done, schedules _finalize_calibration_in_main_thread on Main Thread.\n",
    "        \"\"\"\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        faces = self.get_face_data(frame, results)\n",
    "        \n",
    "        h, w, _ = frame.shape\n",
    "        \n",
    "        if not faces:\n",
    "            cv2.putText(frame, \"NO FACE DETECTED\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            return frame\n",
    "\n",
    "        # Find largest face\n",
    "        target = sorted(faces, key=lambda f: f['box'][2]*f['box'][3], reverse=True)[0]\n",
    "        \n",
    "        # Draw Loading Bar\n",
    "        progress = len(self.calibration_buffer) / CALIBRATION_FRAMES_REQUIRED\n",
    "        bar_w = int(w * 0.6)\n",
    "        cv2.rectangle(frame, (int(w*0.2), h-100), (int(w*0.2) + bar_w, h-70), (255, 255, 255), 2)\n",
    "        cv2.rectangle(frame, (int(w*0.2), h-100), (int(w*0.2) + int(bar_w*progress), h-70), (0, 255, 0), -1)\n",
    "        cv2.putText(frame, \"CALIBRATING...\", (int(w*0.2), h-110), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        self.calibration_buffer.append(target['ear'])\n",
    "\n",
    "        if len(self.calibration_buffer) >= CALIBRATION_FRAMES_REQUIRED:\n",
    "            # Calculation done in thread\n",
    "            avg_ear = sum(self.calibration_buffer) / len(self.calibration_buffer)\n",
    "            target_center = target['center']\n",
    "            \n",
    "            # CRITICAL: Stop calibrating logic in this thread\n",
    "            self.is_calibrating = False\n",
    "            \n",
    "            # CRITICAL: Schedule GUI dialog on MAIN THREAD. Do not call simpledialog here.\n",
    "            self.window.after(0, self._finalize_calibration_in_main_thread, target_center, avg_ear)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def _finalize_calibration_in_main_thread(self, face_center, avg_ear):\n",
    "        \"\"\"\n",
    "        Runs on Main Thread. Opens dialog and updates variables.\n",
    "        \"\"\"\n",
    "        # Logic Check\n",
    "        if avg_ear < MIN_OPEN_EYE_EAR:\n",
    "            messagebox.showwarning(\"Failed\", f\"Eyes detected as closed (EAR: {avg_ear:.2f}). Retry.\")\n",
    "            logging.warning(f\"Calibration failed: EAR too low ({avg_ear:.2f})\")\n",
    "            self.register_button.config(state=tk.NORMAL)\n",
    "            return\n",
    "\n",
    "        # Calculate robust threshold\n",
    "        threshold = min(avg_ear * THRESHOLD_CALIBRATION_FACTOR, MAX_POSSIBLE_THRESHOLD)\n",
    "        \n",
    "        # Dialog - Safe to call here\n",
    "        name = simpledialog.askstring(\"Registration\", f\"Calibration Done (EAR: {avg_ear:.2f}).\\nName:\")\n",
    "        \n",
    "        if name and name not in self.registered_people:\n",
    "            self.registered_people[name] = {\n",
    "                'center': face_center,\n",
    "                'threshold': threshold,\n",
    "                'open_ear_baseline': avg_ear,\n",
    "                'closed_frames': 0,\n",
    "                'status': 'Active',\n",
    "                'last_sound_time': 0\n",
    "            }\n",
    "            logging.info(f\"Person registered: {name} with threshold {threshold:.3f}\")\n",
    "        else:\n",
    "            logging.warning(\"Registration cancelled or duplicate name.\")\n",
    "            \n",
    "        # Re-enable button\n",
    "        self.register_button.config(state=tk.NORMAL)\n",
    "\n",
    "    # --- Monitoring Logic ---\n",
    "\n",
    "    def process_monitoring_logic(self, frame, rgb_frame):\n",
    "        results = self.face_mesh.process(rgb_frame)\n",
    "        faces = self.get_face_data(frame, results)\n",
    "        \n",
    "        if not self.registered_people:\n",
    "            for f in faces:\n",
    "                x, y, w, h = f['box']\n",
    "                cv2.rectangle(frame, (x,y), (x+w, y+h), (100, 100, 100), 2)\n",
    "            return frame, \"Ready to Register.\", \"No one registered.\"\n",
    "\n",
    "        active_faces = {i: f for i, f in enumerate(faces)}\n",
    "        list_display = []\n",
    "\n",
    "        for name, p_data in self.registered_people.items():\n",
    "            best_dist = float('inf')\n",
    "            best_idx = -1\n",
    "            \n",
    "            for idx, face in active_faces.items():\n",
    "                dist = np.linalg.norm(np.array(face['center']) - np.array(p_data['center']))\n",
    "                if dist < best_dist:\n",
    "                    best_dist = dist\n",
    "                    best_idx = idx\n",
    "            \n",
    "            if best_idx != -1 and best_dist < MAX_TRACKING_JUMP_PX:\n",
    "                face = active_faces[best_idx]\n",
    "                del active_faces[best_idx] \n",
    "                \n",
    "                p_data['center'] = face['center']\n",
    "                \n",
    "                ear = face['ear']\n",
    "                is_eyes_closed = ear < p_data['threshold']\n",
    "                \n",
    "                if is_eyes_closed:\n",
    "                    p_data['closed_frames'] += 1\n",
    "                else:\n",
    "                    p_data['closed_frames'] = 0 \n",
    "                \n",
    "                if p_data['closed_frames'] > EYE_AR_CONSEC_FRAMES:\n",
    "                    p_data['status'] = \"!!! SLEEPING !!!\"\n",
    "                    self._trigger_alert(name, p_data)\n",
    "                    color = (0, 0, 255)\n",
    "                    cv2.putText(frame, \"WAKE UP!\", (face['box'][0], face['box'][1]-20), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n",
    "                else:\n",
    "                    p_data['status'] = \"Active\"\n",
    "                    color = (0, 255, 0)\n",
    "\n",
    "                x, y, w, h = face['box']\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                cv2.putText(frame, f\"{name} (EAR: {ear:.2f})\", (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                \n",
    "            else:\n",
    "                p_data['status'] = \"Lost\"\n",
    "                p_data['closed_frames'] = 0\n",
    "\n",
    "            list_display.append(f\"{name}: {p_data['status']}\")\n",
    "\n",
    "        status_msg = f\"Monitoring {len(self.registered_people)} people.\"\n",
    "        return frame, status_msg, \"\\n\".join(list_display)\n",
    "\n",
    "    def _trigger_alert(self, name, p_data):\n",
    "        now = time.time()\n",
    "        if now - p_data['last_sound_time'] > ALERT_SOUND_INTERVAL:\n",
    "            logging.warning(f\"Alert triggered for {name}!\")\n",
    "            threading.Thread(target=self._play_sound, daemon=True).start()\n",
    "            p_data['last_sound_time'] = now\n",
    "\n",
    "    def _play_sound(self):\n",
    "        if os.path.exists(ALERT_SOUND_FILE_PATH):\n",
    "            try:\n",
    "                winsound.PlaySound(ALERT_SOUND_FILE_PATH, winsound.SND_FILENAME | winsound.SND_ASYNC)\n",
    "            except:\n",
    "                pass \n",
    "\n",
    "    def clear_all_registrations(self):\n",
    "        self.registered_people = {}\n",
    "        logging.info(\"All registrations cleared.\")\n",
    "        self.status_text.set(\"Registrations Cleared.\")\n",
    "        self.registered_list_var.set(\"No one registered.\")\n",
    "\n",
    "    def on_closing(self):\n",
    "        if messagebox.askokcancel(\"Quit\", \"Exit Application?\"):\n",
    "            self.stop_video_stream()\n",
    "            logging.info(\"Application closed.\")\n",
    "            self.window.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        root = tk.Tk()\n",
    "        app = SleepingAlertApp(root)\n",
    "        root.mainloop()\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Critical crash: {e}\")\n",
    "        print(f\"Crash: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de06c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3114",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
